{"metadata":{"kernelspec":{"display_name":"Streamlit Notebook","name":"streamlit"}},"nbformat_minor":5,"nbformat":4,"cells":[{"cell_type":"code","id":"3775908f-ca36-4846-8f38-5adca39217f2","metadata":{"language":"python","name":"libs","collapsed":true,"codeCollapsed":true},"source":"# Import python packages & establish session\nimport pandas as pd\nfrom PyPDF2 import PdfFileReader\nfrom snowflake.snowpark.files import SnowflakeFile\nfrom io import BytesIO\nfrom snowflake.snowpark.types import StringType, StructField, StructType\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\n\nfrom snowflake.snowpark.context import get_active_session\nsession = get_active_session()\n","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"ed0001f3-6291-4a1d-bab2-da39e907cacb","metadata":{"name":"intro","collapsed":false},"source":"RAG Made Easy w/ Snowflake Cortex\n========\n\nCreating an end-to-end Retrieval Augmented Generation process (or RAG) directly in Snowflake.\n1) Extract full text from PDF files using Snowpark.\n2) Chunk those documents using Langchain in Snowpark.\n3) Use Cortex to create embeddings of those chunks.\n4) Use Vector Similarity to show the most similar chunk when prompting an LLM.\n\nRemember to add the PyPDF2 and langchain packages (top right!)"},{"cell_type":"code","id":"9d86cf9c-f0d1-4834-b143-cb39517071c3","metadata":{"language":"sql","name":"show_data","collapsed":true,"codeCollapsed":true},"outputs":[],"source":"-- Optional set up: Place your MD files in a stage for extraction\nls @huberman;","execution_count":null},{"cell_type":"code","id":"f1096fb7-6a33-48fb-8984-772288e650e9","metadata":{"language":"python","name":"pdf_read_func","collapsed":false},"outputs":[],"source":"#Create a Snowpark based function to extract text from PDFs\n\ndef readpdf(file_path):\n    whole_text = \"\"\n    with SnowflakeFile.open(file_path, 'rb') as file:\n        f = BytesIO(file.readall())\n        pdf_reader = PdfFileReader(f)\n        whole_text = \"\"\n        for page in pdf_reader.pages:\n            whole_text += page.extract_text()\n    return whole_text","execution_count":null},{"cell_type":"markdown","id":"c1a5e251-47fa-450a-8667-2c8899796b02","metadata":{"name":"UDF_reg"},"source":"#Register the UDF. \n#Optional : Convert the cell to markdown to prevent rerunning later.\nsession.udf.register(\n    func = readpdf\n  , return_type = StringType()\n  , input_types = [StringType()]\n  , is_permanent = True\n  , name = 'SNOWPARK_PDF'\n  , replace = True\n  , packages=['snowflake-snowpark-python','pypdf2']\n  , stage_location = 'LLM_DEMO.PODCASTS.UDF'\n)"},{"cell_type":"code","id":"6ea74c09-b8d0-440d-84d1-fd1cfaef9ce3","metadata":{"language":"sql","name":"raw_table","collapsed":false},"outputs":[],"source":"CREATE OR REPLACE TABLE RAW_TEXT AS\nSELECT\n    relative_path\n    , file_url\n    , snowpark_pdf(build_scoped_file_url(@huberman, relative_path)) as raw_text\nfrom directory(@huberman);","execution_count":null},{"cell_type":"code","id":"e633b10e-8255-4e67-ad98-0d2704af274a","metadata":{"language":"sql","name":"too_big_sadness","collapsed":false},"outputs":[],"source":"--Optional : This will fail due to tokens exceeding limit, which means we need to chunk!\nSELECT\nSNOWFLAKE.CORTEX.COMPLETE('gemma-7b',CONCAT('summarise the following text',raw_text)) \nFROM\nRAW_TEXT\nLIMIT 1;","execution_count":null},{"cell_type":"markdown","id":"4b06bb86-7434-4b11-aee4-41af529095c7","metadata":{"name":"word_on_chunking","collapsed":false},"source":"A note on chunking\n-----\nChunking is the process of splitting a large body of text into smaller 'chunks' whilst attempting to keep as much relevant information as possible. Make the chunks too small and you run the risk of removing key information that the model requires to answer the question. Too large and it may be harder to retreive the correct body of text from the vector search - or spend tokens excessively.\n\nThere are many strategies towards chunking. Eg - pass the most relevant, top n relevant chunks, or pass the most relevent chunk + the chunk either side of that one. Play around and see what works for your use case!\n"},{"cell_type":"code","id":"5de470cc-255e-42fa-87b8-fcac62040b41","metadata":{"language":"python","name":"chunker_udtf","collapsed":false},"outputs":[],"source":"#A class for chunking text and returning a table via UDTF\nclass text_chunker:\n\n    def process(self,text):        \n        text_raw=[]\n        text_raw.append(text) \n        \n        text_splitter = RecursiveCharacterTextSplitter(\n            separators = [\"\\n\"], # Define an appropriate separator. New line is good typically!\n            chunk_size = 1000, #Adjust this as you see fit\n            chunk_overlap  = 50, #This let's text have some form of overlap. Useful for keeping chunks contextual\n            length_function = len,\n            add_start_index = True #Optional but useful if you'd like to feed the chunk before/after\n        )\n    \n        chunks = text_splitter.create_documents(text_raw)\n        df = pd.DataFrame(chunks, columns=['chunks','meta'])\n        \n        yield from df.itertuples(index=False, name=None)\n","execution_count":null},{"cell_type":"markdown","id":"c9c88000-ba6b-4fe9-9215-8a2e4720948d","metadata":{"name":"reg_chunk_udtf","collapsed":false},"source":"#Register the UDTF - set the stage location\n\nschema = StructType([\n     StructField(\"chunk\", StringType()),\n    StructField(\"meta\", StringType()),\n ])\n\nsession.udtf.register( \n    handler = text_chunker,\n    output_schema= schema, \n    input_types = [StringType()] , \n    is_permanent = True , \n    name = 'CHUNK_TEXT' , \n    replace = True , \n    packages=['pandas','langchain'], stage_location = 'LLM_DEMO.PODCASTS.UDF' )"},{"cell_type":"code","id":"6ef44130-b787-495d-9720-af0e584decc3","metadata":{"language":"sql","name":"chunked_table","codeCollapsed":false,"collapsed":false},"outputs":[],"source":"--Create the chunked version of the table\nCREATE OR REPLACE TABLE CHUNK_TEXT AS\nSELECT\n        relative_path,\n        func.*\n    FROM raw_text AS raw,\n         TABLE(chunk_text(raw_text)) as func;","execution_count":null},{"cell_type":"code","id":"5ec68f14-db2a-4df2-b9ce-b82e580bf154","metadata":{"language":"sql","name":"vector_store"},"outputs":[],"source":"--Convert your chunks to embeddings\nCREATE OR REPLACE TABLE VECTOR_STORE AS\nSELECT\nRELATIVE_PATH as EPISODE_NAME,\nCHUNK AS CHUNK,\nsnowflake.cortex.embed_text('e5-base-v2', chunk) as chunk_embedding\nFROM CHUNK_TEXT;","execution_count":null},{"cell_type":"code","id":"6e01fac7-205d-4edc-ba52-aad04327abd7","metadata":{"language":"sql","name":"vector_distance","collapsed":false},"outputs":[],"source":"--Vector distance allows use to find the most similar chunk to a question\nSELECT EPISODE_NAME, CHUNK from LLM_DEMO.PODCASTS.VECTOR_STORE\n            ORDER BY VECTOR_L2_DISTANCE(\n            SNOWFLAKE.CORTEX.embed_text('e5-base-v2', \n            'What makes time perceived to be slower?'\n            ), CHUNK_EMBEDDING\n            ) limit 1\n        ;","execution_count":null},{"cell_type":"code","id":"abdd6751-1ff1-4932-9e9a-a42fbfd4afb7","metadata":{"language":"sql","name":"llm_rag","collapsed":false},"outputs":[],"source":"--Pass the chunk we need along with the prompt to get a better structured answer from the LLM\nSELECT snowflake.cortex.complete(\n    'mistral-7b', \n    CONCAT( \n        'Answer the question based on the context. Be concise.','Context: ',\n        (\n            SELECT chunk FROM LLM_DEMO.PODCASTS.VECTOR_STORE \n            ORDER BY vector_l2_distance(\n            snowflake.cortex.embed_text('e5-base-v2', \n            'How should I optimise my caffeine intake?'\n            ), chunk_embedding\n            ) LIMIT 1\n        ),\n        'Question: ', \n        'How should I optimise my caffeine intake?',\n        'Answer: '\n    )\n) as response;","execution_count":null},{"cell_type":"code","id":"ad8b0da2-aed2-4173-a8c6-13633e38f4d0","metadata":{"language":"python","name":"ask_your_data","collapsed":false},"outputs":[],"source":"import streamlit as st # Import python packages\nfrom snowflake.snowpark.context import get_active_session\nsession = get_active_session() # Get the current credentials\n\nst.title(\"Ask Your Data Anything :snowflake:\")\nst.write(\"\"\"Built using end-to-end RAG in Snowflake with Cortex functions.\"\"\")\n\nmodel = st.selectbox('Select your model:',('mistral-7b','llama2-70b-chat','gemma-7b','mixtral-8x7b'))\n\nprompt = st.text_input(\"Enter prompt\", placeholder=\"What makes time perceived to be slower?\", label_visibility=\"collapsed\")\n\nquest_q = f'''\nselect snowflake.cortex.complete(\n    '{model}', \n    concat( \n        'Answer the question based on the context. Be concise.','Context: ',\n        (\n            select chunk from LLM_DEMO.PODCASTS.VECTOR_STORE\n            order by vector_l2_distance(\n            snowflake.cortex.embed_text('e5-base-v2', \n            '{prompt}'\n            ), chunk_embedding\n            ) limit 1\n        ),\n        'Question: ', \n        '{prompt}',\n        'Answer: '\n    )\n) as response;\n'''\n\nif prompt:\n    df_query = session.sql(quest_q).to_pandas()\n    st.write(df_query['RESPONSE'][0])","execution_count":null},{"cell_type":"markdown","id":"aefa9cc2-2a35-4d80-b890-0e93e58746d6","metadata":{"name":"listagg_chunks","collapsed":false},"source":"import streamlit as st # Import python packages\nfrom snowflake.snowpark.context import get_active_session\nsession = get_active_session() # Get the current credentials\n\nst.title(\"Ask Your Data Anything :snowflake:\")\nst.write(\"\"\"Built using end-to-end RAG in Snowflake with Cortex functions.\"\"\")\n\nmodel = st.selectbox('Select your model:',('llama2-70b-chat','llama2-7b-chat'))\n\nprompt = st.text_input(\"Enter prompt\", placeholder=\"What makes time perceived to be slower?\", label_visibility=\"collapsed\")\n\nquest_q = f'''\nselect snowflake.cortex.complete(\n    '{model}', \n    concat( \n        'Answer the question based on the context. Be concise.','Context: ',\n        (\n            with my_chunks(SUM_CHUNK) as (\n            select SUM_CHUNK from LLM_DEMO.PODCASTS.TEST_VECT\n            order by vector_l2_distance(\n            snowflake.cortex.embed_text('e5-base-v2', \n            '{prompt}'\n            ), SUM_EMBEDDING\n            ) limit 3)\n            select listagg(SUM_CHUNK) as SUM_CHUNK from my_chunks\n        ),\n        'Question: ', \n        '{prompt}',\n        'Answer: '\n    )\n) as response;\n'''\n\nif prompt:\n    df_query = session.sql(quest_q).to_pandas()\n    st.write(df_query['RESPONSE'][0])"},{"cell_type":"markdown","id":"a192ee1c-e525-484d-97fe-39cc79b14af7","metadata":{"name":"vector_store_v2","collapsed":false},"source":"--Convert your chunks to embeddings and use cortex to summarise (to pass through more information in a prompt)\nCREATE OR REPLACE TABLE VECTOR_STORE_EXTRA AS\nSELECT\nRELATIVE_PATH as EPISODE_NAME,\nCHUNK AS CHUNK,\nsnowflake.cortex.summarize(chunk) as SUMMARISED_CHUNK,\nsnowflake.cortex.embed_text('e5-base-v2', chunk) as chunk_embedding,\nsnowflake.cortex.embed_text('e5-base-v2', summarised_chunk) as sum_embedding\nFROM CHUNK_TEXT;"}]}