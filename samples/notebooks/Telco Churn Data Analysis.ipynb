{"metadata":{"kernelspec":{"display_name":"Streamlit Notebook","name":"streamlit"}},"nbformat_minor":5,"nbformat":4,"cells":[{"cell_type":"code","id":"3775908f-ca36-4846-8f38-5adca39217f2","metadata":{"language":"python","name":"CELL0","collapsed":false,"codeCollapsed":false},"source":"#  Copyright (c) 2023 Snowflake Computing Inc. All rights reserved.\n\nimport streamlit as st\nst.markdown(\"# ðŸ“ˆ Telco Churn Model\")\nst.write(\"Lets build a Telco Churn Model that will predict probability of churn for a telco company based on several user features.\")\nst.write(\"Note this notebook builds upon the [Snowflake Hex Quickstart](https://quickstarts.snowflake.com/guide/hex-churn-model/index.html). Hence minimal changes have been done to add other capabilities such as scikit-learn pipelines, hyper-parameter tuning, model deployment etc.\"\"\")","execution_count":null,"outputs":[]},{"cell_type":"code","id":"c695373e-ac74-4b62-a1f1-08206cbd5c81","metadata":{"language":"python","name":"CELL2","collapsed":true,"codeCollapsed":false},"source":"st.markdown(\"## Import python packages\")\nimport pandas as pd\nimport numpy as np\nfrom imblearn.over_sampling import SMOTE \n# We can also use Snowpark for our analyses!\nimport altair as alt","execution_count":null,"outputs":[]},{"cell_type":"code","id":"712182bf-38f8-4b54-8511-494f139ad508","metadata":{"language":"python","name":"CELL3"},"outputs":[],"source":"from snowflake.snowpark.context import get_active_session\nsession = get_active_session()\nprint(session)","execution_count":null},{"cell_type":"code","id":"cbfde78a-b5fc-4f7c-a413-fa8c076458f9","metadata":{"language":"sql","name":"CELL19","collapsed":false,"codeCollapsed":false},"source":"select * from SNOWPUBLIC.STREAMLIT.telco_churn_raw_data_DEMO;","execution_count":null,"outputs":[]},{"cell_type":"code","id":"c34ca1e0-434f-40ba-97e5-3210c74b192c","metadata":{"language":"python","name":"CELL35","collapsed":false,"codeCollapsed":false},"outputs":[],"source":"telco_churn_snow_df = cells.CELL19.to_df()\nst.dataframe(telco_churn_snow_df)","execution_count":null},{"cell_type":"code","id":"43edbbcc-3ae9-4f9a-8d8d-9ab4e1d42211","metadata":{"language":"python","name":"CELL1","collapsed":false,"codeCollapsed":false},"source":"st.markdown(\"# Exploratory Data Analysis (EDA) \\n Machine learning models thrive on clean and well-organized data. To ensure our models perform at their best, we'll investigate our dataset to address any missing values and visualize the distributions of each column.\")","execution_count":null,"outputs":[]},{"cell_type":"code","id":"fb5fa2b7-fb41-411f-af93-5b8990a1cd7f","metadata":{"language":"python","name":"CELL30","collapsed":false},"outputs":[],"source":"st.markdown(\"## Basic Summary Statistics\") \nst.dataframe(telco_churn_snow_df.describe())","execution_count":null},{"cell_type":"code","id":"bf8ed78d-c98d-4119-8f20-8924b90c4b67","metadata":{"language":"python","name":"CELL11","collapsed":false,"codeCollapsed":false},"source":"st.markdown(\"## Checking nulls with Pandas\") \nst.write(\"As can be seen, there is no null value in any of the feature columns\")\ntelco_churn_pdf = telco_churn_snow_df.to_pandas()\nst.dataframe(telco_churn_pdf.isnull().sum())","execution_count":null,"outputs":[]},{"cell_type":"code","id":"bf78bb87-047e-4706-9268-0ec01a26bb93","metadata":{"language":"python","name":"CELL10","collapsed":false,"codeCollapsed":false},"source":"st.markdown(\"## Checking the distribution of the features.\") \ncolumns = telco_churn_pdf.columns\nnum_columns_for_display = 2\ncol1, col2 = st.columns(num_columns_for_display)\nindex = 0\nfor col in columns:\n    source = pd.DataFrame(telco_churn_pdf[col])\n    chrt = alt.Chart(source).mark_bar().encode(\n    alt.X(f\"{col}:Q\", bin=True),\n    y='count()',\n    )\n    if index % num_columns_for_display == 0:\n        with col1: \n            st.altair_chart(chrt)\n    elif index % num_columns_for_display == 1:\n        with col2: \n            st.altair_chart(chrt)\n    index = index + 1","execution_count":null,"outputs":[]},{"cell_type":"code","id":"b54387a5-1d61-4c80-a2b4-33a1e7b32c5c","metadata":{"language":"python","name":"CELL29","collapsed":false},"outputs":[],"source":"st.markdown(\"## Big data processing with Snowpark Dataframes\")\nimport time\nstart = time.time()\ntelco_churn_snow_df.group_by('\"Churn\"').count()\nend = time.time()\nst.markdown(f\"Total Time with Snowpark: {end-start}\")\n\nstart = time.time()\ntelco_churn_snow_pdf = telco_churn_snow_df.to_pandas()\nend_mid = time.time()\ntelco_churn_snow_pdf.groupby(\"Churn\").count()\nend = time.time()\nst.markdown(f\"Total Time with Pandas: {end-start}\")\n\nst.markdown(\"Why is Pandas slower than Snowpark?\")\nst.markdown(f\"I/O time to convert to Pandas dataframe: {end_mid-start}\")\nst.markdown(f\"Processing time with Pandas dataframe: {end-end_mid}\")\nst.markdown(f\"I/O account for {(end_mid-start)/(end-start)*100:.2f}% of processing time\")","execution_count":null},{"cell_type":"code","id":"152d1847-a0d9-4f39-bf24-4d340db5b891","metadata":{"language":"python","name":"CELL12","collapsed":false,"codeCollapsed":false},"source":"st.markdown(\"## Understanding Churn Rate - Imbalanced dataset \\n If you want to understand a model, you need to know its weaknesses. When the target variable has one class that is much more frequent than the other, your data is imbalanced. This causes issues when evaluating models since both classes don't get equal attention.\\n In contrast to modeling an imbalanced dataset, a model trained on balanced data sees an equal amount of observations per class. By eliminating the imbalance, we also eliminate the model's potential to achieve high metric scores due to bias towards a majority class. This means that when we evaluate our model, the metrics can capture a better representation of how well the model does at making valuable predictions.\")\nst.dataframe(telco_churn_snow_df.group_by('\"Churn\"').count())","execution_count":null,"outputs":[]},{"cell_type":"code","id":"e4489e32-a3ab-435d-abee-60396748dcec","metadata":{"language":"python","name":"CELL15","collapsed":false,"codeCollapsed":false},"source":"st.markdown(\"# Feature Engineering - include balancing the dataset and standard scaling \\n To prepare our data for our model, we'll need to handle the imbalanced data problem by upsampling our dataset. Once that's taken care of, we'll use scikit-learn to preprocess our data into a format that the model expects. This means scaling our features and splitting our data into training and testing datasets.\")\n# Extract the training features\nfeatures_names = [col for col in telco_churn_pdf.columns if col not in ['Churn']]\nfeatures = telco_churn_pdf[features_names]\n\n# extract the target\ntarget = telco_churn_pdf['Churn']\nst.markdown(\"## Lets balance the dataset.\")\n# upsample the minority class in the dataset\nupsampler = SMOTE(random_state = 111)\nfeatures, target = upsampler.fit_resample(features, target)\nst.dataframe(features.head())\n\nst.markdown(\"## Upsampled data.\")\nupsampled_data = pd.concat([features, target], axis=1)\nupsampled_data.reset_index(inplace=True)\nupsampled_data.rename(columns={'index': 'INDEX'}, inplace=True)\nst.dataframe(upsampled_data.head())","execution_count":null,"outputs":[]},{"cell_type":"code","id":"bc051113-6bef-40a5-908b-38b4de5ff8c4","metadata":{"language":"python","name":"CELL22","collapsed":false},"outputs":[],"source":"upsampled_data = session.create_dataframe(upsampled_data)\n# Get the list of column names from the dataset\nfeature_names_input = [c for c in upsampled_data.columns if c != '\"Churn\"' and c != \"INDEX\"]","execution_count":null},{"cell_type":"code","id":"b33f5b5c-1181-4d68-90d3-342060389fec","metadata":{"language":"python","name":"CELL32","collapsed":false},"outputs":[],"source":"st.dataframe(upsampled_data[feature_names_input].to_pandas())","execution_count":null},{"cell_type":"markdown","id":"c6a09e11-586a-45dd-ba17-aff7d2f08fd4","metadata":{"name":"CELL33","collapsed":false},"source":"We can perform StandardScaler preprocessing via sklearn to process in-memory or Snowpark ML preprocessing for pushdown compute. Note the similarity between the APIs used and output results."},{"cell_type":"code","id":"f9533336-f5d3-461c-aa75-b604e23e44c1","metadata":{"language":"python","name":"CELL31","collapsed":false},"outputs":[],"source":"st.markdown(\"## Sci-kit learn Preprocessing with Pandas DataFrames\")\nimport sklearn.preprocessing as pp_original\n# Initialize a StandardScaler object with input and output column names\nscaler = pp_original.StandardScaler()\nfeatures_pdf = upsampled_data[feature_names_input].to_pandas()\n# Fit the scaler to the dataset\nscaler.fit(features_pdf)\n\n# Transform the dataset using the fitted scaler\nscaled_features = scaler.transform(features_pdf)\nst.dataframe(scaled_features)","execution_count":null},{"cell_type":"code","id":"fa37e57b-9410-47d8-9d88-d3d3438f3469","metadata":{"language":"python","name":"CELL23","collapsed":false},"outputs":[],"source":"st.markdown(\"## Snowpark ML preprocessing with Snowpark DataFrames\")\nimport snowflake.ml.modeling.preprocessing as pp\n# Get the list of column names from the dataset\nfeature_names_input = [c for c in upsampled_data.columns if c != '\"Churn\"' and c != \"INDEX\"]\n\n\n# Initialize a StandardScaler object with input and output column names\nscaler = pp.StandardScaler(\n    input_cols=feature_names_input,\n    output_cols=feature_names_input\n)\n\n# Fit the scaler to the dataset\nscaler.fit(upsampled_data)\n\n# Transform the dataset using the fitted scaler\nscaled_features = scaler.transform(upsampled_data)\nst.dataframe(scaled_features)","execution_count":null},{"cell_type":"code","id":"589337dc-d674-40d6-9b73-a9d35fa28086","metadata":{"language":"python","name":"CELL18","collapsed":false,"codeCollapsed":false},"source":"st.markdown(\"## Let's perform the train test split using 80/20.\")\n\n# Define the target variable (label) column name\nlabel = ['\"Churn\"']\n\n# Define the output column name for the predicted label\noutput_label = ['\"predicted_churn\"']\n\n# Split the scaled_features dataset into training and testing sets with an 80/20 ratio\ntraining, testing = scaled_features.random_split(weights=[0.8, 0.2], seed=111)","execution_count":null,"outputs":[]},{"cell_type":"code","id":"15803558-604a-4312-b838-46c71ec74bf3","metadata":{"language":"python","name":"CELL14","collapsed":false,"codeCollapsed":false},"source":"from snowflake.ml.modeling.ensemble import RandomForestClassifier\n# Training\nst.markdown(\"# Model Training - Random Forest Classifier \\n The mystery model of the day is a [random forest classifier](https://towardsdatascience.com/understanding-random-forest-58381e0602d2). I'll spare you the details on how it works, but in short, it creates an ensemble of smaller models that all make predictions on the same data. Whichever prediction has the most votes is the final prediction that the model goes with.\")\n# Initialize a RandomForestClassifier object with input, label, and output column names\nmodel = RandomForestClassifier(\n    input_cols=feature_names_input,\n    label_cols=label,\n    output_cols=output_label,\n)","execution_count":null,"outputs":[]},{"cell_type":"code","id":"7aba2aca-6227-443e-ae16-0fa69661cca7","metadata":{"language":"python","name":"CELL16","collapsed":false},"outputs":[],"source":"# Train the RandomForestClassifier model using the training set\nmodel.fit(training)","execution_count":null},{"cell_type":"code","id":"9acc6eba-17cf-4d3c-8343-4023612b693f","metadata":{"language":"python","name":"CELL21","collapsed":false},"outputs":[],"source":"# Predict the target variable (churn) for the testing set using the trained model\nresults = model.predict(testing)","execution_count":null},{"cell_type":"code","id":"f6303824-d5b7-457d-9106-70cb821cc4d6","metadata":{"language":"python","name":"CELL24","collapsed":false},"outputs":[],"source":"testing","execution_count":null},{"cell_type":"code","id":"8cc8f6c8-5d53-4608-a56d-e0e959a27bd0","metadata":{"language":"python","name":"CELL17","collapsed":false,"codeCollapsed":false},"source":"st.markdown(\"\"\"# Model Evaluation\n\nModel evaluation is all about checking how well our machine learning model is doing by comparing its predictions to the actual outcomes. In this case, we're going to focus on *recall* instead of *accuracy* as our main evaluation metric. \"\"\")\n\n# return only the predicted churn values\npredictions = results.to_pandas().sort_values(\"INDEX\")[output_label].astype(int).to_numpy().flatten()\nactual = testing.to_pandas().sort_values(\"INDEX\")[['Churn']].to_numpy().flatten()\n","execution_count":null,"outputs":[]},{"cell_type":"code","id":"418c65c8-c6f2-4a63-b8cd-9c139a395a11","metadata":{"language":"python","name":"CELL9","collapsed":false,"codeCollapsed":false},"source":"\n\nst.markdown(\"\"\"# Feature Importance\n\nFeature importance is all about figuring out which input variables are the real MVPs when it comes to making predictions with our machine learning model. We'll find out which features are the most important by looking at how much they contribute to the model's overall performance.\"\"\")\nrf = model.to_sklearn()\nimportances = pd.DataFrame(\n    list(zip(features.columns, rf.feature_importances_)),\n    columns=[\"feature\", \"importance\"],\n)\n\nbar_chart = alt.Chart(importances).mark_bar().encode(\n    x=\"importance:Q\",\n    y=alt.Y(\"feature:N\", sort=\"-x\")\n)\nst.altair_chart(bar_chart, use_container_width=True)","execution_count":null,"outputs":[]},{"cell_type":"code","id":"5d0b210a-71f9-43f3-9047-e1d11afbb98f","metadata":{"language":"python","name":"CELL13","collapsed":false,"codeCollapsed":false},"source":"st.markdown(\"\"\"# Predicting churn for a new user\nUsing our trained random forest model, we can make predictions that tell us whether a new customer will churn or not.\"\"\")\n\naccount_weeks = \"10\"\ndata_usage = \"1.7\"\nmins_per_month = \"82\"\ndaytime_calls = \"67\"\ncustomer_service_calls = \"4\"\nmonthly_charge = \"37\"\nroam_mins = \"0\"\noverage_fee = \"9.5\"\nrenewed_contract = \"true\"\nhas_data_plan = \"true\"\nuser_vector = np.array([\n    account_weeks,\n    1 if renewed_contract else 0,\n    1 if has_data_plan else 0,\n    data_usage,\n    customer_service_calls,\n    mins_per_month,\n    daytime_calls,\n    monthly_charge,\n    overage_fee,\n    roam_mins,\n]).reshape(1,-1)\n\nuser_dataframe = pd.DataFrame(user_vector, columns=[f'\"{_}\"' for _ in features.columns])\n","execution_count":null,"outputs":[]},{"cell_type":"code","id":"2899f3dd-ba96-486d-aae1-ef956dc3a0a6","metadata":{"language":"python","name":"CELL25","collapsed":false},"outputs":[],"source":"st.markdown(\"#### Input dataframe for new user\")\nst.dataframe(user_dataframe)\nuser_vector = scaler.transform(user_dataframe)","execution_count":null},{"cell_type":"code","id":"a3ffec0f-1ad1-44a7-ba16-b7d332d85cf3","metadata":{"language":"python","name":"CELL28","collapsed":false},"outputs":[],"source":"st.dataframe(model.predict(user_vector)[['\"predicted_churn\"']].values)","execution_count":null},{"cell_type":"code","id":"9267afe2-0972-488a-b01a-554e63f5f38a","metadata":{"language":"python","name":"CELL26","collapsed":false},"outputs":[],"source":"st.markdown(\"#### Scaled dataframe for new user\")\nst.dataframe(user_vector)\nst.markdown(\"#### Prediction\")\npredicted_value = model.predict(user_vector)[['\"predicted_churn\"']].values.astype(int).flatten()\nuser_probability = model.predict_proba(user_vector)\nprobability_of_prediction = max(user_probability[user_probability.columns[-2:]].values[0]) * 100\nprediction = 'churn' if predicted_value == 1 else 'not churn'\nprint(prediction)","execution_count":null},{"cell_type":"code","id":"fa7d04cd-eed2-4e42-8ac1-d70db0c53e4b","metadata":{"language":"python","name":"CELL4","collapsed":false},"outputs":[],"source":"st.markdown(\"\"\"# Predicting churn for a new user\nUsing our trained random forest model, we can make predictions that tell us whether a new customer will churn or not.\"\"\")\n\ncol1, col2 = st.columns(2)\nwith col1: \n    account_weeks = st.slider(\"AccountWeeks\", int(features[\"AccountWeeks\"].min()) , int(features[\"AccountWeeks\"].max()))\n    data_usage = st.slider(\"DataUsage\", int(features[\"DataUsage\"].min()) , int(features[\"DataUsage\"].max()))\n    mins_per_month = st.slider(\"DayMins\", int(features[\"DayMins\"].min()) , int(features[\"DayMins\"].max()))\n    daytime_calls = st.slider(\"DayCalls\", int(features[\"DayCalls\"].min()) , int(features[\"DayCalls\"].max()))\n    renewed_contract =  st.selectbox(\"Renewed Contract?\",('true','false'))\nwith col2: \n    monthly_charge = st.slider(\"MonthlyCharge\", int(features[\"MonthlyCharge\"].min()) , int(features[\"MonthlyCharge\"].max()))\n    roam_mins = st.slider(\"RoamMins\", int(features[\"RoamMins\"].min()) , int(features[\"RoamMins\"].max()))\n    customer_service_calls = st.slider(\"CustServCalls\", int(features[\"CustServCalls\"].min()) , int(features[\"CustServCalls\"].max()))\n    overage_fee = st.slider(\"OverageFee\", int(features[\"OverageFee\"].min()) , int(features[\"OverageFee\"].max()))\n    has_data_plan = st.selectbox(\"Has Data Plan?\",('true','false'))\nuser_vector = np.array([\n    account_weeks,\n    1 if renewed_contract else 0,\n    1 if has_data_plan else 0,\n    data_usage,\n    customer_service_calls,\n    mins_per_month,\n    daytime_calls,\n    monthly_charge,\n    overage_fee,\n    roam_mins,\n]).reshape(1,-1)\n\nuser_dataframe = pd.DataFrame(user_vector, columns=[f'\"{_}\"' for _ in features.columns])\nst.markdown(\"#### Input dataframe for new user\")\nst.dataframe(user_dataframe)\nuser_vector = scaler.transform(user_dataframe)\nst.markdown(\"#### Scaled dataframe for new user\")\nst.dataframe(user_vector)\nst.markdown(\"#### Prediction\")\npredicted_value = model.predict(user_vector)[['\"predicted_churn\"']].values.astype(int).flatten()\nuser_probability = model.predict_proba(user_vector)\nprobability_of_prediction = max(user_probability[user_probability.columns[-2:]].values[0]) * 100\nprediction = 'churn' if predicted_value == 1 else 'not churn'\nprint(prediction)\n","execution_count":null},{"cell_type":"code","id":"baed87c3-3bd1-40cb-b8d3-7b4be39702fe","metadata":{"language":"python","name":"CELL27","collapsed":false},"outputs":[],"source":"st.markdown(\"Exporting Model with Timestamp\")\nimport pickle\nimport datetime\nfilename = f'telco-eda-model-{datetime.datetime.now()}.pkl'\n\npickle.dump(model, open(filename,'wb'))\nprint(f\"Saved to {filename}\")","execution_count":null}]}