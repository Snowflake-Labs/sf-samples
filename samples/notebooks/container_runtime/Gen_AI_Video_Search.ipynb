{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f9ab3d0d-8bca-422e-ace6-ae19aded5148",
   "metadata": {
    "collapsed": false,
    "name": "Overview",
    "resultHeight": 306
   },
   "source": [
    "# AI Video Search with Snowflake and Twelve Labs\n",
    "\n",
    "### Overview\n",
    "\n",
    "This guide outlines the process for creating a video search and summarization workflow in Snowflake Notebook on Container Runtime. Videos stored in the cloud storage are processed to generate embeddings using the Twelve Labs API, with parallelization achieved through a Snowpark Python User Defined Table Function (UDTF). These embeddings are stored in a Snowflake table using the VECTOR datatype, enabling efficient similarity searches with VECTOR_COSINE_SIMILARITY. Text queries are converted into embeddings using the same API to find the top N matching video clips. Audio from these clips is extracted using MoviePy and transcribed with Whisper. Finally, Cortex Complete is used to summarize the results, including video details, timestamps, and transcripts.\n",
    "\n",
    "## Step-By-Step Guide\n",
    "\n",
    "For prerequisites and environment setup, please refer to the [QuickStart Guide](https://quickstarts.snowflake.com/guide/ai-video-search-with-snowflake-and-twelveLabs/index.html).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd2eaca2-667e-4118-9604-bb400a08cf6b",
   "metadata": {
    "name": "Info_1",
    "resultHeight": 41
   },
   "source": [
    "Install Python packages and other libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3775908f-ca36-4846-8f38-5adca39217f2",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "Install_Libraries",
    "resultHeight": 26334
   },
   "outputs": [],
   "source": [
    "!pip install twelvelabs\n",
    "!pip install git+https://github.com/openai/whisper.git ffmpeg-python moviepy\n",
    "!DEBIAN_FRONTEND=noninteractive apt-get install -y ffmpeg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95cfbbae-747e-4ef5-977f-7f471f62dedf",
   "metadata": {
    "collapsed": false,
    "name": "Info_2",
    "resultHeight": 41
   },
   "source": [
    "Import installed libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "392068cc-ac1d-4aea-b511-a37eb563534c",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "Import_Libraries",
    "resultHeight": 0
   },
   "outputs": [],
   "source": [
    "from twelvelabs import TwelveLabs\n",
    "from twelvelabs.models.embed import EmbeddingsTask\n",
    "from snowflake.snowpark.context import get_active_session\n",
    "from twelvelabs.models.task import Task\n",
    "import requests\n",
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import snowflake\n",
    "from snowflake import cortex\n",
    "\n",
    "session = get_active_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12506f33-644d-4373-865a-37c3bf19ca09",
   "metadata": {
    "collapsed": false,
    "name": "Info_3",
    "resultHeight": 83
   },
   "source": [
    "This is where we provide a list of publicly accessible URLs of videos.  \n",
    "\n",
    "*NOTE: In this guide, three sample videos have been provided.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca098690-2084-4686-9a83-454fae87cac7",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "python",
    "name": "Videos_List",
    "resultHeight": 0
   },
   "outputs": [],
   "source": [
    "video_urls = ['https://sfquickstarts.s3.us-west-1.amazonaws.com/misc/videos/snowflake_build2024_announcements.mp4',\n",
    "              'http://commondatastorage.googleapis.com/gtv-videos-bucket/sample/Sintel.mp4',\n",
    "              'http://commondatastorage.googleapis.com/gtv-videos-bucket/sample/BigBuckBunny.mp4']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7808153-808d-4069-8b2e-11e4aab1c3f0",
   "metadata": {
    "name": "Info_4",
    "resultHeight": 41
   },
   "source": [
    "Create and register `create_video_embeddings` Snowpark Python User Defined Table Function (UDTF) for creating embeddings for the videos using Twelve Labs.\n",
    "\n",
    "Things to note in the UDTF:\n",
    "\n",
    "* `session.add_import('@\"DASH_DB\".\"DASH_SCHEMA\".\"DASH_PKGS\"/twelvelabs.zip')` adds the **twelvelabs** Python package that will create the video embeddings\n",
    "* `packages=['httpx','pydantic']` adds the additional packages required and also readily available in the Snowflake Anaconda channel\n",
    "* `external_access_integrations=['twelvelabs_access_integration'],` and `secrets={'cred': 'twelve_labs_api'}` adds the external access integration and the twelvelabs API (secret) that will authorize the UDTF to securely access Twelve Labs account to create the video embeddings\n",
    "*  `    output_schema=StructType([\n",
    "        StructField(\"embedding\", VectorType(float,1024)),\n",
    "        StructField(\"start_offset_sec\", FloatType()),\n",
    "        StructField(\"end_offset_sec\", FloatType()),\n",
    "        StructField(\"embedding_scope\", StringType())\n",
    "    ])` defines the schema with columns and their datatypes that will be the output of this UDTF. Notice the **VectorType** of the `embedding` column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31310039-efa6-4b59-abd0-1eb1d28d64fb",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "UDTF_Generate_Video_Embeddings",
    "resultHeight": 0
   },
   "outputs": [],
   "source": [
    "from snowflake.snowpark.functions import udtf, lit, Tuple\n",
    "from snowflake.snowpark.types import FloatType, StringType, StructType, StructField, Iterable, VectorType\n",
    "\n",
    "session.clear_imports()\n",
    "session.add_import('@\"DASH_DB\".\"DASH_SCHEMA\".\"DASH_PKGS\"/twelvelabs.zip')\n",
    "@udtf(name=\"create_video_embeddings\",\n",
    "     packages=['httpx','pydantic'],\n",
    "     external_access_integrations=['twelvelabs_access_integration'],\n",
    "     secrets={'cred': 'twelve_labs_api'},\n",
    "     if_not_exists=True,\n",
    "     is_permanent=True,\n",
    "     stage_location='@DASH_DB.DASH_SCHEMA.DASH_UDFS',\n",
    "     output_schema=StructType([\n",
    "        StructField(\"embedding\", VectorType(float,1024)),\n",
    "        StructField(\"start_offset_sec\", FloatType()),\n",
    "        StructField(\"end_offset_sec\", FloatType()),\n",
    "        StructField(\"embedding_scope\", StringType())\n",
    "    ])\n",
    "    )\n",
    "class create_video_embeddings:\n",
    "    def __init__(self):\n",
    "        from twelvelabs import TwelveLabs\n",
    "        from twelvelabs.models.embed import EmbeddingsTask\n",
    "        import _snowflake\n",
    "        \n",
    "        twelve_labs_api_key = _snowflake.get_generic_secret_string('cred') \n",
    "        twelvelabs_client = TwelveLabs(api_key=twelve_labs_api_key)\n",
    "        self.twelvelabs_client = twelvelabs_client\n",
    "\n",
    "    def process(self, video_url: str) -> Iterable[Tuple[list, float, float, str]]:\n",
    "        # Create an embeddings task\n",
    "        task = self.twelvelabs_client.embed.task.create(\n",
    "            model_name=\"Marengo-retrieval-2.7\",\n",
    "            video_url=video_url\n",
    "        )\n",
    "        \n",
    "        # Wait for the task to complete\n",
    "        status = task.wait_for_done(sleep_interval=60)\n",
    "\n",
    "        # Retrieve and process embeddings\n",
    "        task = task.retrieve()\n",
    "        if task.video_embedding is not None and task.video_embedding.segments is not None:\n",
    "            for segment in task.video_embedding.segments:\n",
    "                yield (\n",
    "                    segment.embeddings_float,  # Embedding (list of floats)\n",
    "                    segment.start_offset_sec,  # Start offset in seconds\n",
    "                    segment.end_offset_sec,    # End offset in seconds\n",
    "                    segment.embedding_scope,   # Embedding scope\n",
    "                )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04401ddd-9cd9-4fc7-b3d9-045f88ecdc6a",
   "metadata": {
    "collapsed": false,
    "name": "Info_5",
    "resultHeight": 67
   },
   "source": [
    "Create a Snowpark DataFrame using the list of videos and for each video call `create_video_embeddings` UDTF to generate embeddings. Note the use of `.over(partition_by=\"url\")`. Then, save those embeddings in a Snowflake table called `video_embeddings`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72585787-a622-4d90-90ea-5d92f53e768b",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "python",
    "name": "Save_Video_Embeddings",
    "resultHeight": 439
   },
   "outputs": [],
   "source": [
    "df = session.create_dataframe(video_urls,schema=['url'])\n",
    "df = df.join_table_function(create_video_embeddings(df['url']).over(partition_by=\"url\"))\n",
    "df.write.mode('overwrite').save_as_table('video_embeddings')\n",
    "df = session.table('video_embeddings')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d6340c5-3440-48a1-ba48-8655bfbf1542",
   "metadata": {
    "collapsed": false,
    "name": "Info_6",
    "resultHeight": 198
   },
   "source": [
    "Download open source `whisper` model and define the following Python functions:\n",
    "\n",
    "* download_video\n",
    "* extract_audio_from_video\n",
    "* transcribe_with_whisper\n",
    "* transcribe_video\n",
    "* transcribe_video_clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "536408a7-fde9-45f8-ba86-b44ace446178",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "PyFn_Transcribe_Videos",
    "resultHeight": 71
   },
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import os\n",
    "from moviepy import VideoFileClip\n",
    "import whisper\n",
    "import warnings\n",
    "import logging\n",
    "\n",
    "# Suppress Whisper warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "# Set MoviePy logger level to ERROR or CRITICAL to suppress INFO logs\n",
    "logging.getLogger(\"moviepy\").setLevel(logging.ERROR)\n",
    "\n",
    "# Load the Whisper model once\n",
    "whisper_model = whisper.load_model(\"base\")  # or any other model you want to use, e.g., 'small', 'medium', 'large'\n",
    "\n",
    "def download_video(video_url, output_video_path, status):\n",
    "    try:\n",
    "        # status.caption(\"Downloading video file...\")\n",
    "        urllib.request.urlretrieve(video_url, output_video_path)\n",
    "        # status.caption(f\"Video downloaded to {output_video_path}.\")\n",
    "        return output_video_path\n",
    "    except Exception as e:\n",
    "        status.caption(f\"An error occurred during video download: {e}\")\n",
    "        return None\n",
    "\n",
    "def extract_audio_from_video(video_path, output_audio_path, status, start_time=None, end_time=None):\n",
    "    try:\n",
    "        # status.caption(\"Extracting audio from video...\")\n",
    "        video_clip = VideoFileClip(video_path)\n",
    "        \n",
    "        # If start and end times are provided, trim the video\n",
    "        if start_time is not None or end_time is not None:\n",
    "            video_clip = video_clip.subclipped(start_time, end_time)\n",
    "        \n",
    "        video_clip.audio.write_audiofile(output_audio_path)\n",
    "        # status.caption(f\"Audio extracted to {output_audio_path}.\")\n",
    "        return output_audio_path\n",
    "    except Exception as e:\n",
    "        status.caption(f\"An error occurred during audio extraction: {e}\")\n",
    "        return None\n",
    "\n",
    "def transcribe_with_whisper(audio_path, status):\n",
    "    try:\n",
    "        # status.caption(\"Transcribing audio with Whisper...\")\n",
    "        result = whisper_model.transcribe(audio_path)\n",
    "        # status.caption(\"Transcription complete.\")\n",
    "        return result[\"text\"]\n",
    "    except Exception as e:\n",
    "        status.caption(f\"An error occurred during transcription: {e}\")\n",
    "        return None\n",
    "\n",
    "def transcribe_video(video_url, status, temp_video_path=\"temp_video.mp4\", temp_audio_path=\"temp_audio.mp3\"):\n",
    "    try:\n",
    "        # Step 1: Download video \n",
    "        video_path = download_video(video_url, temp_video_path, status)\n",
    "        if not video_path:\n",
    "            return None\n",
    "\n",
    "        # Step 2: Extract audio from video\n",
    "        audio_path = extract_audio_from_video(video_path, temp_audio_path, status)\n",
    "        if not audio_path:\n",
    "            return None\n",
    "\n",
    "        # Step 3: Transcribe audio with Whisper\n",
    "        transcription = transcribe_with_whisper(audio_path, status)\n",
    "        return transcription\n",
    "    finally:\n",
    "        # Clean up temporary files\n",
    "        if os.path.exists(temp_video_path):\n",
    "            os.remove(temp_video_path)\n",
    "            # status.caption(\"Temporary video file removed.\")\n",
    "        if os.path.exists(temp_audio_path):\n",
    "            os.remove(temp_audio_path)\n",
    "            # status.caption(\"Temporary audio file removed.\")\n",
    "\n",
    "def transcribe_video_clip(video_url, status, start_time, end_time, temp_video_path=\"temp_video.mp4\", temp_audio_path=\"temp_audio_clip.mp3\"):\n",
    "    try:\n",
    "        # Step 1: Download video \n",
    "        video_path = download_video(video_url, temp_video_path, status)\n",
    "        if not video_path:\n",
    "            return None\n",
    "\n",
    "        # Step 2: Extract audio from the specified clip\n",
    "        audio_path = extract_audio_from_video(video_path, temp_audio_path, status, start_time, end_time)\n",
    "        if not audio_path:\n",
    "            return None\n",
    "\n",
    "        # Step 3: Transcribe the extracted audio clip with Whisper\n",
    "        transcription = transcribe_with_whisper(audio_path, status)\n",
    "        return transcription\n",
    "    finally:\n",
    "        # Clean up temporary files\n",
    "        if os.path.exists(temp_video_path):\n",
    "            os.remove(temp_video_path)\n",
    "            # status.caption(\"Temporary video file removed.\")\n",
    "        if os.path.exists(temp_audio_path):\n",
    "            os.remove(temp_audio_path)\n",
    "            # status.caption(\"Temporary audio file removed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f30f7188-7025-4eb9-a54a-8f7a225f9cca",
   "metadata": {
    "name": "Info_7",
    "resultHeight": 92
   },
   "source": [
    "Define Python function `similarity_scores` that uses Twelve Labs to create embeddings for a given text -- *entered_text* passed in as a parameter. Then, similarity scores are generated using Snowflake function **VECTOR_COSINE_SIMILARITY** between text embeddings and video embeddings stored in `video_embeddings` table. This function returns top *N* records (based on *max_results* passed in as a parameter) with columns VIDEO_URL, START_OFFSET_SEC, END_OFFSET_SEC, and SIMILARITY_SCORE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73487b72-beb7-4542-b29b-e0b3fd51a1ff",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "PyFn_Similarity_Scores",
    "resultHeight": 0
   },
   "outputs": [],
   "source": [
    "# TODO: Replace tlk_XXXXXXXXXXXXXXXXXX with your Twelve Labs API Key\n",
    "TWELVE_LABS_API_KEY =\"tlk_XXXXXXXXXXXXXXXXXX\"\n",
    "# Initialize the Twelve Labs client\n",
    "twelvelabs_client = TwelveLabs(api_key=TWELVE_LABS_API_KEY)\n",
    "\n",
    "def truncate_text(text, max_tokens=77):\n",
    "    # Truncate text to roughly 77 tokens (assuming ~6 chars per token on average)\n",
    "    return text[:max_tokens * 6]  # Adjust based on actual tokenization behavior\n",
    "\n",
    "def similarity_scores(search_text,results_limit=5):\n",
    "    # Twelve Labs Embed API supports text-to-embedding  \n",
    "    truncated_text = truncate_text(search_text, max_tokens=77)\n",
    "    \n",
    "    twelvelabs_response = twelvelabs_client.embed.create(\n",
    "      model_name=\"Marengo-retrieval-2.7\",\n",
    "      text=truncated_text,\n",
    "      text_truncate='start'\n",
    "    )\n",
    "\n",
    "    if twelvelabs_response.text_embedding is not None and twelvelabs_response.text_embedding.segments is not None:\n",
    "        text_query_embeddings = twelvelabs_response.text_embedding.segments[0].embeddings_float\n",
    "        return session.sql(f\"\"\"\n",
    "            SELECT URL as VIDEO_URL,START_OFFSET_SEC,END_OFFSET_SEC,\n",
    "            round(VECTOR_COSINE_SIMILARITY(embedding::VECTOR(FLOAT, 1024),{text_query_embeddings}::VECTOR(FLOAT, 1024)),2) as SIMILARITY_SCORE \n",
    "            from video_embeddings order by similarity_score desc limit {results_limit}\"\"\")\n",
    "    else:\n",
    "        return twelvelabs_response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f9bd901-e022-4086-9de0-b7105018f02c",
   "metadata": {
    "name": "Info_8",
    "resultHeight": 92
   },
   "source": [
    "Streamlit application that takes **Search Text**, **Max Results**, and **Summary LLM** as user input. Then, it first calls `similarity_scores` function to get top *N* video clip records along with their similarity scores. For each clip, it then calls `transcribe_video_clip` function passing in its VIDEO_URL, START_OFFSET_SEC, END_OFFSET_SEC to generate the clip transcription. Finally, it calls `snowflake.cortex.Complete` to summarize the output.\n",
    "\n",
    "For all search results, the app displays the URL of the video, the clip start and end times, similarity score generated by VECTOR_COSINE_SIMILARITY, clip transcript generated by open source whisper model, as well as the summary generated by Snowflake Cortex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac865adc-733a-4f26-9234-06ac336a1f9d",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "python",
    "name": "Search_Clips_Application",
    "resultHeight": 1434
   },
   "outputs": [],
   "source": [
    "st.subheader(\"Search Clips Application\")\n",
    "\n",
    "with st.container():\n",
    "    with st.expander(\"Enter search text and select max results\", expanded=True):\n",
    "        left_col,mid_col,right_col = st.columns(3)\n",
    "        with left_col:\n",
    "            entered_text = st.text_input('Search Text')\n",
    "        with mid_col:\n",
    "            max_results = st.selectbox('Max Results',(1,2,3,4,5))\n",
    "        with right_col:\n",
    "            selected_llm = st.selectbox('Select Summary LLM',('llama3.2-3b','llama3.1-405b','mistral-large2', 'snowflake-arctic',))\n",
    "        \n",
    "with st.container():\n",
    "    _,mid_col1,_ = st.columns([.3,.4,.2])\n",
    "    with mid_col1:\n",
    "        similarity_scores_btn = st.button('Search and Summarize Matching Video Clips',type=\"primary\")\n",
    "\n",
    "with st.container():\n",
    "    if similarity_scores_btn:\n",
    "        if entered_text:\n",
    "            with st.status(\"In progress...\") as status:\n",
    "                df = similarity_scores(entered_text,max_results).to_pandas()\n",
    "                status.subheader(f\"Top {max_results} clip(s) for search query '{entered_text}'\")\n",
    "                for row in df.itertuples():\n",
    "                    transcribed_clip = transcribe_video_clip(row.VIDEO_URL, status, row.START_OFFSET_SEC, row.END_OFFSET_SEC)\n",
    "                    prompt = f\"\"\"\n",
    "                    [INST] Summarize the following and include name of the video as well as start and end clip times in seconds with everything in natural language as opposed to attributes: \n",
    "                    ###\n",
    "                    Video URL: {row.VIDEO_URL},\n",
    "                    Clip Start: {row.START_OFFSET_SEC} | Clip End: {row.END_OFFSET_SEC} | Similarity Score: {row.SIMILARITY_SCORE}\n",
    "                    Clip Transcript: {transcribed_clip}\n",
    "                    ###\n",
    "                    [/INST]\n",
    "                    \"\"\"\n",
    "                    status.write(f\"Video URL: {row.VIDEO_URL}\")\n",
    "                    status.caption(f\"-- Clip Start: {row.START_OFFSET_SEC} | Clip End: {row.END_OFFSET_SEC} | Similarity Score: {row.SIMILARITY_SCORE}\")\n",
    "                    status.caption(f\"-- Clip Transcript: {transcribed_clip}\")\n",
    "                    status.write(f\"Summary: {cortex.Complete(selected_llm,prompt)}\")\n",
    "                    status.divider()\n",
    "                status.update(label=\"Done!\", state=\"complete\", expanded=True)\n",
    "        else:\n",
    "            st.caption(\"User ERROR: Please enter search text!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
