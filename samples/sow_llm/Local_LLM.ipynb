{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create some local directories in /tmp\n",
    "  * as a downloads cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Determine which T5 FLAN models we want to \n",
    "  * serialize\n",
    "  * deserialize\n",
    "  * run inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "fsRoot = '/tmp/base'\n",
    "\n",
    "zipRoot = fsRoot + '/root'\n",
    "modelDest = zipRoot + '/saved' \n",
    "tokenDir = zipRoot + '/token'\n",
    "\n",
    "import pathlib\n",
    "dirs = [ modelDest, tokenDir ]\n",
    "for d in dirs:\n",
    "    pathlib.Path(d).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "modelList = [\"google/flan-t5-base\"] ## Add larger models as required\n",
    "#modelList = [\"google/flan-t5-base\", \"google/flan-t5-large\", \"google/flan-t5-xl\", \"google/flan-t5-xxl\" ]\n",
    "# Model Sizes: 1GB (base), 3GB (large), 11GB (xl), 44GB (xxl)\n",
    "# \"google/flan-t5-base\", \"google/flan-t5-large\", \"google/flan-t5-xl\", \"google/flan-t5-xxl\" \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import T5Tokenizer,T5ForConditionalGeneration, T5Config\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. If you see this, DO NOT PANIC! This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=True`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wrote model to: /tmp/base/root/saved/pydict.pt\n",
      "wrote config to: /tmp/base/root/saved/config.json\n",
      "wrote tokenizer config to: /tmp/base/root/token\n",
      "translate English to French: What time is it??\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Quelle temps est-ce?\n",
      "-----------------------\n",
      "translate English to German: What is your name?\n",
      "Was ist Ihr Name?\n",
      "-----------------------\n",
      "Translate this English sentence to Spanish: Cat loves chicken pizza\n",
      "El rato encanta la pizza de chile\n",
      "-----------------------\n",
      "Alice made her friend, Xin, laugh when she told a funny joke.\\n\\nWho is she referring to?\\n\\n(A)Alice \\n(B)Xin\n",
      "Alice\n",
      "-----------------------\n",
      "Alice made her friend, Xin, laugh when she told a funny joke.\\n\\nWho laughed?\\n\\n(A)Alice \\n(B)Xin\n",
      "Alice\n",
      "-----------------------\n",
      "Answer the following yes/no question by reasoning step by step. Can a dog drive a car?\n",
      "Dogs are not allowed to drive cars. Therefore, the final answer is no.\n",
      "-----------------------\n",
      "Please answer to the following question. Who is going to be the next Ballon d\\\\'or?\n",
      "s<unk> ra sánchez\n",
      "-----------------------\n",
      "Translate to German:  My name is Arthur\n",
      "Ich muß Arthur heißen.\n",
      "-----------------------\n",
      "Q: Can Geoffrey Hinton have a conversation with George Washington? Give the rationale before answering.\n",
      "George Washington was born in 1789. Geoffrey Hinton was born in 1789. So the final answer is no.\n",
      "-----------------------\n",
      "Please answer the following question. What is the boiling point of Nitrogen?\n",
      "212 ° c\n",
      "-----------------------\n",
      "Answer the following yes/no question. Can you write a whole Haiku in a single tweet?\n",
      "no\n",
      "-----------------------\n",
      "Q: ( False or not False or False ) is? A: Let\\\\'s think step by step\n",
      "False or not False or False is a satire of the term False or False. So the answer is False.\n",
      "-----------------------\n",
      "The square root of x is the cube root of y. What is y to the power of 2, if x = 4?\n",
      "x = 4 * 2 = 8 x = 16 y = 16 to the power of 2.\n",
      "-----------------------\n",
      "Premise:  At my age you will probably have learnt one lesson. Hypothesis:  It\\\\'s not certain how many lessons you\\\\'ll learn by your thirties. Does the premise entail the hypothesis?\n",
      "it is not possible to tell\n",
      "-----------------------\n"
     ]
    }
   ],
   "source": [
    "for modelID in modelList:\n",
    "    tokenizer = T5Tokenizer.from_pretrained(modelID, cache_dir=fsRoot)\n",
    "    model = T5ForConditionalGeneration.from_pretrained(modelID, cache_dir=fsRoot)\n",
    "\n",
    "    serialize=True\n",
    "    if (serialize):\n",
    "        tokenizer.save_pretrained(tokenDir) # huggingface API take directory\n",
    "        torch.save(model.state_dict(), modelDest + '/pydict.pt') #torch API takes file\n",
    "        config = T5Config.from_pretrained(modelID)\n",
    "        config.to_json_file(modelDest + '/config.json')\n",
    "        print ('wrote model to:',  modelDest + '/pydict.pt')\n",
    "        print ('wrote config to:',  modelDest + '/config.json')\n",
    "        print ('wrote tokenizer config to:',  tokenDir )\n",
    "\n",
    "    inference = True\n",
    "    if (inference):\n",
    "        infile = './flan_prompt.csv' \n",
    "        df = pd.read_csv(infile)\n",
    "        for p in list(df['prompt'].unique()):\n",
    "            print (p)\n",
    "            input_ids = tokenizer(p, return_tensors=\"pt\").input_ids\n",
    "            outputs = model.generate(input_ids ,max_length=50)\n",
    "            response = tokenizer.decode(outputs[0])\n",
    "            response = response.replace(tokenizer.pad_token, \"\").replace(tokenizer.eos_token, \"\")\n",
    "            print (response.strip())\n",
    "            print ('-----------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Done - ZZZZ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
