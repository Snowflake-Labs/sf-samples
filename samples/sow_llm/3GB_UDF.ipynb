{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "fsRoot = '/tmp/llm'\n",
    "os.environ['HF_HOME'] = fsRoot + '/misc'\n",
    "os.environ['HF_DATASETS_CACHE'] = fsRoot + '/datasets'\n",
    "os.environ['TRANSFORMERS_CACHE'] = fsRoot + '/models'\n",
    "\n",
    "modelID = \"google/flan-t5-large\"\n",
    "\n",
    "zipRoot = fsRoot + '/root'\n",
    "modelDest = zipRoot + '/saved' \n",
    "tokenDir = zipRoot + '/token' \n",
    "\n",
    "import pathlib\n",
    "dirs = [fsRoot + '/misc', fsRoot + '/datasets', fsRoot + '/models', zipRoot,  modelDest, tokenDir ]\n",
    "for d in dirs:\n",
    "    pathlib.Path(d).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snowflake.snowpark.session import Session\n",
    "import snowflake.connector\n",
    "import snowflake.snowpark.functions as F\n",
    "\n",
    "from transformers import T5Tokenizer,T5ForConditionalGeneration, T5Config\n",
    "import torch\n",
    "\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a connection to Snowflake, Snowpark supports the following authentification methods:\n",
    "* Username and password\n",
    "* externalbrowser (Okta, ADFS, or any other SAML 2.0-compliant identity provider (IdP))\n",
    "* oauth\n",
    "* Key pair\n",
    "\n",
    "This example is using a JSON file with the following structure\n",
    "```\n",
    "{\n",
    "    \"account\":\"MY SNOWFLAKE ACCOUNT\",\n",
    "    \"user\": \"MY USER\",\n",
    "    \"password\":\"MY PASSWORD\",\n",
    "    \"role\":\"MY ROLE\",\n",
    "    \"warehouse\":\"MY WH\",\n",
    "    \"database\":\"MY DB\",\n",
    "    \"schema\":\"MY SCHEMA\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../config.json') as f:\n",
    "    data = json.load(f)\n",
    "    USERNAME = data['user']\n",
    "    PASSWORD = data['password']\n",
    "    SF_ACCOUNT = data['account']\n",
    "    SF_ROLE = data['role']\n",
    "    SF_WH = data['warehouse'] # std_warehouse == STD_WH, max_warehouse == MAX_CON=1\n",
    "    SF_DB = data['database']\n",
    "    SF_SCHEMA = data['schema']\n",
    "\n",
    "connection_parameters = {\n",
    "    \"account\": SF_ACCOUNT,\n",
    "    \"user\": USERNAME,\n",
    "    \"password\": PASSWORD,\n",
    "    \"role\": SF_ROLE,\n",
    "    \"warehouse\": SF_WH,\n",
    "    \"database\": SF_DB,\n",
    "    \"schema\": SF_SCHEMA\n",
    "}\n",
    "\n",
    "session = Session.builder.configs(connection_parameters).create()\n",
    "\n",
    "con = snowflake.connector.connect(\n",
    "    user=USERNAME, #You can get it by executing in UI: desc user <username>; \n",
    "    # Or Snowflake APP UI --> \"HOME icon\" | Profile | Username\n",
    "    account=SF_ACCOUNT, #Add all of the account-name between https:// and snowflakecomputing.com\n",
    "    password=PASSWORD,\n",
    "    database=SF_DB,\n",
    "    warehouse=SF_WH,\n",
    "    role=SF_ROLE,\n",
    "    schema=SF_SCHEMA\n",
    ")\n",
    "\n",
    "cur = con.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------\n",
      "|\"CURRENT_ROLE()\"  |\"CURRENT_VERSION()\"  |\n",
      "------------------------------------------\n",
      "|ACCOUNTADMIN      |7.31.0               |\n",
      "------------------------------------------\n",
      "\n",
      "----------------------\n",
      "|\"WAREHOUSETYPE\"     |\n",
      "----------------------\n",
      "|SNOWPARK-OPTIMIZED  |\n",
      "----------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#sqlStatment = 'select current_account(), current_role(), current_user(), current_database(), current_schema(), current_warehouse(), current_version()'\n",
    "sqlStatment = 'select  current_role(),  current_version()'\n",
    "session.sql(sqlStatment).show()\n",
    "sqlStatment = f\"show warehouses like '{SF_WH}'\"\n",
    "session.sql(sqlStatment).collect()\n",
    "sqlStatment = 'SELECT \"type\" as warehouseType FROM table(result_scan(last_query_id()))'\n",
    "session.sql(sqlStatment).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confirm expected Downloads Sizes (e.g. pytorch_model.bin)\n",
    "  * flan-t5-large == 3.1 GB\n",
    "  * size could be limited within Sandbox and you may run out of room in /tmp\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f33e8b8216c418e99b7b5a3127fc0b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a749dfb4e484223977810f32a5124be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/2.20k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf02df0f2f8f479db9f1f51d44ab6374",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/2.54k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. If you see this, DO NOT PANIC! This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=True`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "997d993ad86e4f88b10641c036fe4dad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/662 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb07ca09b2684588af83933e67deec1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/3.13G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bba88576d784f92b1840888166aafb8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)neration_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = T5Tokenizer.from_pretrained(modelID, cache_dir=fsRoot)\n",
    "model = T5ForConditionalGeneration.from_pretrained(modelID, cache_dir=fsRoot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Serialize model and tokenizers\n",
    "  \n",
    "  * Use torch to save model as dictionary\n",
    "  * Save the model config.json\n",
    "  * tokenizer is saved to a direcotry then zipped\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2241ffaa51f84ceb807f24d6862252fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/662 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving Dictionary Model to: /tmp/llm/root/saved\n",
      "Saving Tokenizer to: /tmp/llm/root/token\n"
     ]
    }
   ],
   "source": [
    "torch_dir = False\n",
    "\n",
    "torch.save(model.state_dict(), modelDest + '/pydict.pt')\n",
    "config = T5Config.from_pretrained(modelID)\n",
    "config.to_json_file(modelDest + '/config.json')\n",
    "print ('Saving Dictionary Model to:', modelDest)\n",
    "\n",
    "if (torch_dir):\n",
    "    model.save_pretrained(modelDest)\n",
    "    print ('Saving  Model to:', modelDest)\n",
    "\n",
    "tokenizer.save_pretrained(tokenDir)\n",
    "print ('Saving Tokenizer to:', tokenDir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "\n",
    "def mkZip(inDir, zipName, overwrite=False):\n",
    "    zip_name = zipName + '.zip'\n",
    "    isExists = os.path.exists(zip_name)\n",
    "\n",
    "    if (not isExists) or (overwrite):\n",
    "        with zipfile.ZipFile(zip_name, 'w', zipfile.ZIP_DEFLATED) as zip_ref:\n",
    "            for folder_name, subfolders, filenames in os.walk(inDir):\n",
    "                for filename in filenames:\n",
    "                    file_path = os.path.join(folder_name, filename)\n",
    "                    zip_ref.write(file_path, arcname=os.path.relpath(file_path, zip_name))\n",
    "\n",
    "        zip_ref.close()\n",
    "\n",
    "    file_stats = os.stat(zip_name)\n",
    "    return (zip_name, file_stats.st_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory changed successfully /private/tmp/llm/root\n"
     ]
    }
   ],
   "source": [
    "# Check current working directory.\n",
    "retval = os.getcwd()\n",
    "\n",
    "# Now change the directory\n",
    "os.chdir( zipRoot )\n",
    "\n",
    "# Check current working directory.\n",
    "retval = os.getcwd()\n",
    "\n",
    "print (\"Directory changed successfully %s\" % retval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the tokenizer ZIP\n",
    "  * tokenizer is small\n",
    "  * tokenizer files can vary (may need 'n' with 2 <= n <=6 ?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ztoken.zip : 0.3941621780395508 MB\n",
      "token /tmp/llm/root/ztoken.zip\n"
     ]
    }
   ],
   "source": [
    "inputDir = 'token'\n",
    "zipName = 'ztoken'\n",
    "zipFile = zipName + '.zip'\n",
    "(zip_name, zipSize) = mkZip(inputDir, zipName, overwrite=True)\n",
    "zipPath = zipRoot + '/' + zipFile\n",
    "print (f'{zip_name} : {zipSize / (1024*1024)} MB')\n",
    "print (inputDir, zipPath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Copy the tokenizer ZIP file to a stage\n",
    "  * tokenizer is small\n",
    "  * tokenizer files can vary (may need 'n' with 2 <= n <=6 ?)\n",
    "  * zip file is also useful for file synchronization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(status='Stage area TMP_LLM_3GB_STAGE successfully created.')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session.sql('CREATE OR REPLACE STAGE TMP_LLM_3GB_STAGE').collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PUT file:///tmp/llm/root/ztoken.zip @TMP_LLM_3GB_STAGE AUTO_COMPRESS=FALSE OVERWRITE=TRUE;\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<snowflake.connector.cursor.SnowflakeCursor at 0x7fedf28ddf60>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cmd = \"PUT file://\" + zipPath + \" @TMP_LLM_3GB_STAGE AUTO_COMPRESS=FALSE OVERWRITE=TRUE;\"\n",
    "print (cmd)\n",
    "cur.execute(cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading files from: /tmp/llm/root/saved\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['/tmp/llm/root/saved/config.json', '/tmp/llm/root/saved/pydict.pt']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import glob\n",
    "print (f'Loading files from: {modelDest}')\n",
    "uploadFiles = glob.glob(modelDest+'/*')\n",
    "uploadFiles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload the Large Model files (dictionary) and config file\n",
    "  * Note - the assumption the weights are in a single file is not correct for larger LLMs\n",
    "  * e.g. a larger LLMs weights may be split up into multiple files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PUT file:///tmp/llm/root/saved/config.json @TMP_LLM_3GB_STAGE AUTO_COMPRESS=FALSE OVERWRITE=TRUE;\n",
      "/tmp/llm/root/saved/config.json : 0.0007104873657226562 MB\n",
      "PUT file:///tmp/llm/root/saved/pydict.pt @TMP_LLM_3GB_STAGE AUTO_COMPRESS=FALSE OVERWRITE=TRUE;\n",
      "/tmp/llm/root/saved/pydict.pt : 2987.7108659744263 MB\n"
     ]
    }
   ],
   "source": [
    "for zipPath in uploadFiles:\n",
    "    cmd = \"PUT file://\" + zipPath + \" @TMP_LLM_3GB_STAGE AUTO_COMPRESS=FALSE OVERWRITE=TRUE;\"\n",
    "    print (cmd)\n",
    "    cur.execute(cmd)\n",
    "    file_stats = os.stat(zipPath)\n",
    "    print (f'{zipPath} : {file_stats.st_size / (1024*1024)} MB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confirm files within stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------------------------------------------\n",
      "|\"name\"                         |\"size\"      |\"md5\"                                 |\"last_modified\"               |\n",
      "--------------------------------------------------------------------------------------------------------------------\n",
      "|tmp_llm_3gb_stage/config.json  |752         |de5837bfe22958ed40a3ce95e53cd0fe      |Mon, 4 Sep 2023 17:54:14 GMT  |\n",
      "|tmp_llm_3gb_stage/pydict.pt    |3132841920  |886a400b58e78ccbff3ac6a4ddc30887-374  |Mon, 4 Sep 2023 17:54:45 GMT  |\n",
      "|tmp_llm_3gb_stage/ztoken.zip   |413312      |fba625321f736711054d648817e9fd0f      |Mon, 4 Sep 2023 17:54:14 GMT  |\n",
      "--------------------------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "session.sql(\"ls @TMP_LLM_3GB_STAGE\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<snowflake.connector.cursor.SnowflakeCursor at 0x7fedf28ddf60>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sql_statement = '''CREATE OR REPLACE FUNCTION zip_large_llm_infer(str string)\n",
    "RETURNS STRING\n",
    "LANGUAGE PYTHON\n",
    "RUNTIME_VERSION = 3.8\n",
    "HANDLER = 'large_llm_inference'\n",
    "PACKAGES = ('transformers', 'pytorch', 'sentencepiece')\n",
    "IMPORTS = ('@TMP_LLM_3GB_STAGE/pydict.pt', '@TMP_LLM_3GB_STAGE/config.json', '@TMP_LLM_3GB_STAGE/ztoken.zip')\n",
    "AS\n",
    "$$\n",
    "import fcntl\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "import threading\n",
    "import zipfile\n",
    "from transformers import T5Tokenizer,T5ForConditionalGeneration, T5Config\n",
    "import torch\n",
    "\n",
    "os.environ[\"HF_DATASETS_OFFLINE\"] = \"1\"\n",
    "os.environ[\"TRANSFORMERS_OFFLINE\"] = \"1\" \n",
    "\n",
    "# File lock class for synchronizing write access to /tmp.\n",
    "class FileLock:\n",
    "  def __enter__(self):\n",
    "    self._lock = threading.Lock()\n",
    "    self._lock.acquire()\n",
    "    self._fd = open('/tmp/lockfile.LOCK', 'w+')\n",
    "    fcntl.lockf(self._fd, fcntl.LOCK_EX)\n",
    "\n",
    "  def __exit__(self, type, value, traceback):\n",
    "    self._fd.close()\n",
    "    self._lock.release()\n",
    "\n",
    "# Get the location of the import directory. Snowflake sets the import\n",
    "# directory location so code can retrieve the location via sys._xoptions.\n",
    "IMPORT_DIRECTORY_NAME = \"snowflake_import_directory\"\n",
    "import_dir = sys._xoptions[IMPORT_DIRECTORY_NAME]\n",
    "\n",
    "# Get the path to the ZIP file and set the location to extract to.\n",
    "model_file_path = import_dir + \"pydict.pt\"\n",
    "model_json_file_path = import_dir + \"config.json\"\n",
    "token_zip_file_path = import_dir + \"ztoken.zip\"\n",
    "extracted = '/tmp'\n",
    "modelSrc = extracted + '/token'\n",
    "\n",
    "# make sure the ZIP file is up to date (e.g. OVERWRITE old copy)\n",
    "with FileLock():\n",
    "  if not os.path.isdir(modelSrc):\n",
    "    with zipfile.ZipFile(token_zip_file_path, 'r') as myzip:\n",
    "      myzip.extractall(extracted)\n",
    "\n",
    "# Load the model from the extracted file.\n",
    "tokenizer = T5Tokenizer.from_pretrained(modelSrc)\n",
    "\n",
    "model = T5ForConditionalGeneration.from_pretrained(pretrained_model_name_or_path=None, \\\n",
    "  config=model_json_file_path, \\\n",
    "    state_dict =  torch.load(model_file_path))\n",
    "\n",
    "def large_llm_inference(text):\n",
    "  input_ids = tokenizer(text, return_tensors=\"pt\").input_ids\n",
    "  outputs = model.generate(input_ids ,max_length=50)\n",
    "  response = tokenizer.decode(outputs[0])\n",
    "  response = response.replace(tokenizer.pad_token, \"\").replace(tokenizer.eos_token, \"\")\n",
    "  return (response.strip().upper())\n",
    "  \n",
    "$$;\n",
    "'''\n",
    "cur.execute(sql_statement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------\n",
      "|\"INPUT\"                                         |\"LLM_RESULT\"                |\n",
      "-------------------------------------------------------------------------------\n",
      "|translate English to French: What time is it??  |<UNK> QUELLE HEURE EST-CE?  |\n",
      "-------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlStatment = f\"select 'translate English to French: What time is it??' as input, zip_large_llm_infer( 'translate English to French: What time is it??' ) as llm_result\"\n",
    "session.sql(sqlStatment).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional -- Assumes of you have a table of prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nbr of prompts: 14\n"
     ]
    }
   ],
   "source": [
    "prompt_df = session.table(\"FLAN_PROMPT\")\n",
    "print(f\"Nbr of prompts: {prompt_df.count():,}\")\n",
    "#prompt_df.show(max_width=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|\"PROMPT\"                                                                                                                                                |\"ZIP_LARGE_LLM_INFER(\"\"PROMPT\"\")\"                                                          |\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|translate English to French: What time is it??                                                                                                          |<UNK> QUELLE HEURE EST-CE?                                                                 |\n",
      "|translate English to German: What is your name?                                                                                                         |WAS IST IHR NAMEN?                                                                         |\n",
      "|Translate this English sentence to Spanish: Cat loves chicken pizza                                                                                     |EL GATO ENCANTA PIZZA DE POLLO                                                             |\n",
      "|Alice made her friend, Xin, laugh when she told a funny joke.\\n\\nWho is she referring to?\\n\\n(A)Alice \\n(B)Xin                                          |(A)                                                                                        |\n",
      "|Alice made her friend, Xin, laugh when she told a funny joke.\\n\\nWho laughed?\\n\\n(A)Alice \\n(B)Xin                                                      |(B)                                                                                        |\n",
      "|Answer the following yes/no question by reasoning step by step. Can a dog drive a car?                                                                  |A DOG IS A MAMMAL THAT CANNOT DRIVE A CAR. THEREFORE, THE FINAL ANSWER IS NO.              |\n",
      "|Please answer to the following question. Who is going to be the next Ballon d\\\\'or?                                                                     |NIKOLAIS GERGIEV                                                                           |\n",
      "|Translate to German:  My name is Arthur                                                                                                                 |ICH BIN ARTHUR                                                                             |\n",
      "|Q: Can Geoffrey Hinton have a conversation with George Washington? Give the rationale before answering.                                                 |GEORGE WASHINGTON DIED IN 1789. GEOFFREY HINTON WAS BORN IN 1818. THE ANSWER: NO.          |\n",
      "|Please answer the following question. What is the boiling point of Nitrogen?                                                                            |212 KELVIN                                                                                 |\n",
      "|Answer the following yes/no question. Can you write a whole Haiku in a single tweet?                                                                    |NO                                                                                         |\n",
      "|Q: ( False or not False or False ) is? A: Let\\\\'s think step by step                                                                                    |FALSE OR NOT FALSE OR FALSE IS A VERB. THE VERB IS FALSE OR NOT FALSE. THE ANSWER: FALSE.  |\n",
      "|The square root of x is the cube root of y. What is y to the power of 2, if x = 4?                                                                      |2                                                                                          |\n",
      "|Premise:  At my age you will probably have learnt one lesson. Hypothesis:  It\\\\'s not certain how many lessons you\\\\'ll learn by your thirties. Doe...  |IT IS NOT POSSIBLE TO TELL                                                                 |\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt_df.select(F.col(\"PROMPT\")).select(F.col(\"PROMPT\"), F.call_function(\"zip_large_llm_infer\", F.col(\"PROMPT\"))).show(15, max_width=150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Done - ZZZZ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
