{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "fsRoot = '/tmp/gb11'\n",
    "os.environ['HF_HOME'] = fsRoot + '/misc'\n",
    "os.environ['HF_DATASETS_CACHE'] = fsRoot + '/datasets'\n",
    "os.environ['TRANSFORMERS_CACHE'] = fsRoot + '/models'\n",
    "\n",
    "modelID = \"google/flan-t5-xl\"\n",
    "#modelID = \"google/flan-t5-xxl\"\n",
    "\n",
    "zipRoot = fsRoot + '/root'\n",
    "modelDest = zipRoot + '/saved' \n",
    "tokenDir = zipRoot + '/token' \n",
    "\n",
    "import pathlib\n",
    "dirs = [fsRoot + '/misc', fsRoot + '/datasets', fsRoot + '/models', zipRoot,  modelDest, tokenDir ]\n",
    "for d in dirs:\n",
    "    pathlib.Path(d).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snowflake.snowpark.session import Session\n",
    "import snowflake.connector\n",
    "import snowflake.snowpark.functions as F\n",
    "\n",
    "from transformers import T5Tokenizer,T5ForConditionalGeneration, T5Config\n",
    "import torch\n",
    "\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a connection to Snowflake, Snowpark supports the following authentification methods:\n",
    "* Username and password\n",
    "* externalbrowser (Okta, ADFS, or any other SAML 2.0-compliant identity provider (IdP))\n",
    "* oauth\n",
    "* Key pair\n",
    "\n",
    "This example is using a JSON file with the following structure\n",
    "```\n",
    "{\n",
    "    \"account\":\"MY SNOWFLAKE ACCOUNT\",\n",
    "    \"user\": \"MY USER\",\n",
    "    \"password\":\"MY PASSWORD\",\n",
    "    \"role\":\"MY ROLE\",\n",
    "    \"warehouse\":\"MY WH\",\n",
    "    \"database\":\"MY DB\",\n",
    "    \"schema\":\"MY SCHEMA\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../config.json') as f:\n",
    "#with open('../demo164.json') as f:\n",
    "    data = json.load(f)\n",
    "    USERNAME = data['user']\n",
    "    PASSWORD = data['password']\n",
    "    SF_ACCOUNT = data['account']\n",
    "    SF_ROLE = data['role']\n",
    "    SF_WH = data['warehouse']\n",
    "    SF_DB = data['database']\n",
    "    SF_SCHEMA = data['schema']\n",
    "\n",
    "connection_parameters = {\n",
    "    \"account\": SF_ACCOUNT,\n",
    "    \"user\": USERNAME,\n",
    "    \"password\": PASSWORD,\n",
    "    \"role\": SF_ROLE,\n",
    "    \"warehouse\": SF_WH,\n",
    "    \"database\": SF_DB,\n",
    "    \"schema\": SF_SCHEMA\n",
    "}\n",
    "\n",
    "session = Session.builder.configs(connection_parameters).create()\n",
    "\n",
    "con = snowflake.connector.connect(\n",
    "    user=USERNAME, #You can get it by executing in UI: desc user <username>; \n",
    "    # Or Snowflake APP UI --> \"HOME icon\" | Profile | Username\n",
    "    account=SF_ACCOUNT, #Add all of the account-name between https:// and snowflakecomputing.com\n",
    "    password=PASSWORD,\n",
    "    database=SF_DB,\n",
    "    warehouse=SF_WH,\n",
    "    role=SF_ROLE,\n",
    "    schema=SF_SCHEMA\n",
    ")\n",
    "\n",
    "cur = con.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------\n",
      "|\"CURRENT_ROLE()\"  |\"CURRENT_VERSION()\"  |\n",
      "------------------------------------------\n",
      "|ACCOUNTADMIN      |7.31.0               |\n",
      "------------------------------------------\n",
      "\n",
      "----------------------\n",
      "|\"WAREHOUSETYPE\"     |\n",
      "----------------------\n",
      "|SNOWPARK-OPTIMIZED  |\n",
      "----------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#sqlStatment = 'select current_account(), current_role(), current_user(), current_database(), current_schema(), current_warehouse(), current_version()'\n",
    "sqlStatment = 'select  current_role(),  current_version()'\n",
    "session.sql(sqlStatment).show()\n",
    "sqlStatment = f\"show warehouses like '{SF_WH}'\"\n",
    "session.sql(sqlStatment).collect()\n",
    "sqlStatment = 'SELECT \"type\" as warehouseType FROM table(result_scan(last_query_id()))'\n",
    "session.sql(sqlStatment).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confirm expected Downloads Sizes (e.g. pytorch_model.bin)\n",
    "  * flan-t5-xl == 11.5 GB\n",
    "  * size could be limited within Sandbox and you may run out of room in /tmp\n",
    "  * there may be multiple BIN files to download for the model\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. If you see this, DO NOT PANIC! This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=True`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5ffeadb4e6942c8b8f05454f1bfa2a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = T5Tokenizer.from_pretrained(modelID, cache_dir=fsRoot)\n",
    "model = T5ForConditionalGeneration.from_pretrained(modelID, cache_dir=fsRoot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Serialize model and tokenizers\n",
    "  \n",
    "  * Use torch to save model as dictionary\n",
    "  * Save the model config.json\n",
    "  * tokenizer is saved to a direcotry then zipped\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving Dictionary Model to: /tmp/gb11/root/saved\n",
      "Saving Tokenizer to: /tmp/gb11/root/token\n"
     ]
    }
   ],
   "source": [
    "overwriteStage = True \n",
    "torch_dir = False\n",
    "\n",
    "if (overwriteStage):\n",
    "    torch.save(model.state_dict(), modelDest + '/pydict.pt')\n",
    "    config = T5Config.from_pretrained(modelID)\n",
    "    config.to_json_file(modelDest + '/config.json')\n",
    "    print ('Saving Dictionary Model to:', modelDest)\n",
    "\n",
    "    if (torch_dir):\n",
    "        model.save_pretrained(modelDest)\n",
    "        print ('Saving  Model to:', modelDest)\n",
    "\n",
    "    tokenizer.save_pretrained(tokenDir)\n",
    "    print ('Saving Tokenizer to:', tokenDir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save some RAM by deleting local model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "\n",
    "def mkZip(inDir, zipName, overwrite=False):\n",
    "    zip_name = zipName + '.zip'\n",
    "    isExists = os.path.exists(zip_name)\n",
    "\n",
    "    if (not isExists) or (overwrite):\n",
    "        with zipfile.ZipFile(zip_name, 'w', zipfile.ZIP_DEFLATED) as zip_ref:\n",
    "            for folder_name, subfolders, filenames in os.walk(inDir):\n",
    "                for filename in filenames:\n",
    "                    file_path = os.path.join(folder_name, filename)\n",
    "                    zip_ref.write(file_path, arcname=os.path.relpath(file_path, zip_name))\n",
    "\n",
    "        zip_ref.close()\n",
    "\n",
    "    file_stats = os.stat(zip_name)\n",
    "    return (zip_name, file_stats.st_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory changed successfully /private/tmp/gb11/root\n"
     ]
    }
   ],
   "source": [
    "# Check current working directory.\n",
    "retval = os.getcwd()\n",
    "\n",
    "# Now change the directory\n",
    "os.chdir( zipRoot )\n",
    "\n",
    "# Check current working directory.\n",
    "retval = os.getcwd()\n",
    "\n",
    "print (\"Directory changed successfully %s\" % retval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the tokenizer ZIP\n",
    "  * tokenizer is small\n",
    "  * tokenizer files can vary (may need 'n' with 2 <= n <=6 ?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ztoken.zip : 0.3941621780395508 MB\n",
      "token /tmp/gb11/root/ztoken.zip\n"
     ]
    }
   ],
   "source": [
    "if (overwriteStage):\n",
    "    inputDir = 'token'\n",
    "    zipName = 'ztoken'\n",
    "    zipFile = zipName + '.zip'\n",
    "    (zip_name, zipSize) = mkZip(inputDir, zipName, overwrite=True)\n",
    "    zipPath = zipRoot + '/' + zipFile\n",
    "    print (f'{zip_name} : {zipSize / (1024*1024)} MB')\n",
    "    print (inputDir, zipPath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Copy the tokenizer ZIP file to a stage\n",
    "  * tokenizer is small\n",
    "  * tokenizer files can vary (may need 'n' with 2 <= n <=6 ?)\n",
    "  * zip file is also useful for file synchronization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if (overwriteStage):\n",
    "    session.sql('CREATE OR REPLACE STAGE TMP_XLRG_LLM_STAGE').collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PUT file:///tmp/gb11/root/ztoken.zip @TMP_XLRG_LLM_STAGE AUTO_COMPRESS=FALSE OVERWRITE=TRUE;\n"
     ]
    }
   ],
   "source": [
    "if (overwriteStage):\n",
    "    cmd = \"PUT file://\" + zipPath + \" @TMP_XLRG_LLM_STAGE AUTO_COMPRESS=FALSE OVERWRITE=TRUE;\"\n",
    "    print (cmd)\n",
    "    cur.execute(cmd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload the 1 Large Model files (dictionary) and model config\n",
    "  * Beware for XL model -- it splits files to at most 9 GB chunks\n",
    "  * Determine other necessary files that need to be present in the stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading files from: /tmp/gb11/root/saved\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['/tmp/gb11/root/saved/config.json', '/tmp/gb11/root/saved/pydict.pt']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import glob\n",
    "print (f'Loading files from: {modelDest}')\n",
    "uploadFiles = glob.glob(modelDest+'/*')\n",
    "uploadFiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PUT file:///tmp/gb11/root/saved/config.json @TMP_XLRG_LLM_STAGE AUTO_COMPRESS=FALSE OVERWRITE=TRUE;\n",
      "/tmp/gb11/root/saved/config.json : 0.001445770263671875 MB\n",
      "PUT file:///tmp/gb11/root/saved/pydict.pt @TMP_XLRG_LLM_STAGE AUTO_COMPRESS=FALSE OVERWRITE=TRUE;\n",
      "/tmp/gb11/root/saved/pydict.pt : 10871.200322151184 MB\n"
     ]
    }
   ],
   "source": [
    "if (overwriteStage):\n",
    "    for f in uploadFiles:\n",
    "        cmd = \"PUT file://\" + f + \" @TMP_XLRG_LLM_STAGE AUTO_COMPRESS=FALSE OVERWRITE=TRUE;\"\n",
    "        print (cmd)\n",
    "        cur.execute(cmd)\n",
    "        file_stats = os.stat(f)\n",
    "        print (f'{f} : {file_stats.st_size / (1024*1024)} MB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------------------------------------------\n",
      "|\"name\"                          |\"size\"       |\"md5\"                                  |\"last_modified\"               |\n",
      "-----------------------------------------------------------------------------------------------------------------------\n",
      "|tmp_xlrg_llm_stage/config.json  |1520         |8eaeb2211b304e5abc63282d9a53998d       |Mon, 4 Sep 2023 18:20:00 GMT  |\n",
      "|tmp_xlrg_llm_stage/pydict.pt    |11399279760  |61deee9df1781b538ee2181735721f05-1359  |Mon, 4 Sep 2023 18:21:45 GMT  |\n",
      "|tmp_xlrg_llm_stage/ztoken.zip   |413312       |e1c063197552bd44bfe51bed3c405edd       |Mon, 4 Sep 2023 18:20:00 GMT  |\n",
      "-----------------------------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "session.sql(\"ls @TMP_XLRG_LLM_STAGE\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<snowflake.connector.cursor.SnowflakeCursor at 0x7fa17cd2ebc0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sql_statement = '''CREATE OR REPLACE FUNCTION stage_xl_llm_infer(str string)\n",
    "RETURNS STRING\n",
    "LANGUAGE PYTHON\n",
    "RUNTIME_VERSION = 3.8\n",
    "HANDLER = 'xl_llm_inference'\n",
    "PACKAGES = ('transformers', 'pytorch', 'sentencepiece')\n",
    "IMPORTS = ('@TMP_XLRG_LLM_STAGE/pydict.pt', '@TMP_XLRG_LLM_STAGE/config.json', '@TMP_XLRG_LLM_STAGE/ztoken.zip')\n",
    "AS\n",
    "$$\n",
    "import fcntl\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "import threading\n",
    "import zipfile\n",
    "from transformers import T5Tokenizer,T5ForConditionalGeneration, T5Config\n",
    "import torch\n",
    "\n",
    "os.environ[\"HF_DATASETS_OFFLINE\"] = \"1\"\n",
    "os.environ[\"TRANSFORMERS_OFFLINE\"] = \"1\" \n",
    "\n",
    "# File lock class for synchronizing write access to /tmp.\n",
    "class FileLock:\n",
    "  def __enter__(self):\n",
    "    self._lock = threading.Lock()\n",
    "    self._lock.acquire()\n",
    "    self._fd = open('/tmp/lockfile.LOCK', 'w+')\n",
    "    fcntl.lockf(self._fd, fcntl.LOCK_EX)\n",
    "\n",
    "  def __exit__(self, type, value, traceback):\n",
    "    self._fd.close()\n",
    "    self._lock.release()\n",
    "\n",
    "# Get the location of the import directory. Snowflake sets the import\n",
    "# directory location so code can retrieve the location via sys._xoptions.\n",
    "IMPORT_DIRECTORY_NAME = \"snowflake_import_directory\"\n",
    "import_dir = sys._xoptions[IMPORT_DIRECTORY_NAME]\n",
    "\n",
    "# Get the path to the ZIP file and set the location to extract to.\n",
    "model_file_path = import_dir + \"pydict.pt\"\n",
    "model_json_file_path = import_dir + \"config.json\"\n",
    "token_zip_file_path = import_dir + \"ztoken.zip\"\n",
    "extracted = '/tmp'\n",
    "modelSrc = extracted + '/token'\n",
    "\n",
    "# make sure the ZIP file is up to date (e.g. OVERWRITE old copy)\n",
    "with FileLock():\n",
    "  if not os.path.isdir(modelSrc):\n",
    "    with zipfile.ZipFile(token_zip_file_path, 'r') as myzip:\n",
    "      myzip.extractall(extracted)\n",
    "\n",
    "\n",
    "# Load the model from the extracted file.\n",
    "tokenizer = T5Tokenizer.from_pretrained(modelSrc)\n",
    "\n",
    "model = T5ForConditionalGeneration.from_pretrained(pretrained_model_name_or_path=None, \\\n",
    "  config=model_json_file_path, \\\n",
    "    state_dict =  torch.load(model_file_path))\n",
    "\n",
    "def xl_llm_inference(text):  \n",
    "  input_ids = tokenizer(text, return_tensors=\"pt\").input_ids\n",
    "  outputs = model.generate(input_ids ,max_length=50)\n",
    "  response = tokenizer.decode(outputs[0])\n",
    "  response = response.replace(tokenizer.pad_token, \"\").replace(tokenizer.eos_token, \"\")\n",
    "  return (response.strip().upper())\n",
    "  \n",
    "$$;\n",
    "'''\n",
    "cur.execute(sql_statement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------\n",
      "|\"INPUT\"                                         |\"LLM_RESULT\"          |\n",
      "-------------------------------------------------------------------------\n",
      "|translate English to French: What time is it??  |QUELLE EST LA HEURE?  |\n",
      "-------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlStatment = f\"select 'translate English to French: What time is it??' as input, stage_xl_llm_infer( 'translate English to French: What time is it??' ) as llm_result\"\n",
    "session.sql(sqlStatment).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional -- Assumes of you have a table of prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nbr of prompts: 14\n"
     ]
    }
   ],
   "source": [
    "prompt_df = session.table(\"FLAN_PROMPT\")\n",
    "print(f\"Nbr of prompts: {prompt_df.count():,}\")\n",
    "#prompt_df.show(max_width=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|\"PROMPT\"                                                                                                                                                |\"STAGE_XL_LLM_INFER(\"\"PROMPT\"\")\"                                                              |\n",
      "---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|translate English to French: What time is it??                                                                                                          |QUELLE EST LA HEURE?                                                                          |\n",
      "|translate English to German: What is your name?                                                                                                         |WAS IST IHR NAME?                                                                             |\n",
      "|Translate this English sentence to Spanish: Cat loves chicken pizza                                                                                     |CATA AMA LA PIZZA DE POLLO                                                                    |\n",
      "|Alice made her friend, Xin, laugh when she told a funny joke.\\n\\nWho is she referring to?\\n\\n(A)Alice \\n(B)Xin                                          |(A)                                                                                           |\n",
      "|Alice made her friend, Xin, laugh when she told a funny joke.\\n\\nWho laughed?\\n\\n(A)Alice \\n(B)Xin                                                      |(B)                                                                                           |\n",
      "|Answer the following yes/no question by reasoning step by step. Can a dog drive a car?                                                                  |A DOG IS NOT ABLE TO DRIVE A CAR. THEREFORE, THE FINAL ANSWER IS NO.                          |\n",
      "|Please answer to the following question. Who is going to be the next Ballon d\\\\'or?                                                                     |ADNAN KHAN                                                                                    |\n",
      "|Translate to German:  My name is Arthur                                                                                                                 |ICH BIN ARTHUR.                                                                               |\n",
      "|Q: Can Geoffrey Hinton have a conversation with George Washington? Give the rationale before answering.                                                 |GEORGE WASHINGTON DIED IN 1799. GEOFFREY HINTON WAS BORN IN 1924. SO THE FINAL ANSWER IS NO.  |\n",
      "|Please answer the following question. What is the boiling point of Nitrogen?                                                                            |0 0 0 0                                                                                       |\n",
      "|Answer the following yes/no question. Can you write a whole Haiku in a single tweet?                                                                    |NO                                                                                            |\n",
      "|Q: ( False or not False or False ) is? A: Let\\\\'s think step by step                                                                                    |THE CORRECT ANSWER IS FALSE OR NOT FALSE OR FALSE.                                            |\n",
      "|The square root of x is the cube root of y. What is y to the power of 2, if x = 4?                                                                      |0                                                                                             |\n",
      "|Premise:  At my age you will probably have learnt one lesson. Hypothesis:  It\\\\'s not certain how many lessons you\\\\'ll learn by your thirties. Doe...  |IT IS NOT POSSIBLE TO TELL                                                                    |\n",
      "---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt_df.select(F.col(\"PROMPT\")).select(F.col(\"PROMPT\"), F.call_function(\"stage_xl_llm_infer\", F.col(\"PROMPT\"))).show(15, max_width=150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Done - ZZZZZ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
