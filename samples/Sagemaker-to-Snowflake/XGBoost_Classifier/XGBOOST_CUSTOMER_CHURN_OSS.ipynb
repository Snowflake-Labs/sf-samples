{
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  },
  "lastEditStatus": {
   "notebookId": "i3hoemszsycuifmde67o",
   "authorId": "6149508575120",
   "authorName": "RPEGU",
   "authorEmail": "ranjeeta.pegu@snowflake.com",
   "sessionId": "5756ca3a-7443-4e3f-82fb-ed44f88d3597",
   "lastEditTime": 1756821000333
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c2528dc-25c7-4442-8cb9-144c02f127c2",
   "metadata": {
    "name": "Introduction",
    "collapsed": false
   },
   "source": "## Introduction ##\n\nCustomer loss can significantly impact a business’s bottom line. By detecting at-risk customers early, companies can proactively engage them with retention strategies. In this workshop, we'll explore how to use machine learning capabilities to automate the identification of dissatisfied customers—commonly referred to as churn prediction\n\n ** Internal** [aws - example ](https://sagemaker-examples.readthedocs.io/en/latest/introduction_to_applying_machine_learning/xgboost_customer_churn/xgboost_customer_churn_outputs.html#Data)\n\n### Configuring the environment "
  },
  {
   "cell_type": "markdown",
   "id": "af05cc8a-7a47-4ea3-812b-554343ab2260",
   "metadata": {
    "name": "prerequisite",
    "collapsed": false
   },
   "source": "I have download the data and uploaded it into snowflake using the  **COPY** Command"
  },
  {
   "cell_type": "code",
   "id": "3775908f-ca36-4846-8f38-5adca39217f2",
   "metadata": {
    "language": "python",
    "name": "Libraries"
   },
   "source": "# Import python packages\nimport streamlit as st\nimport pandas as pd\n\n# We can also use Snowpark for our analyses!\nfrom snowflake.snowpark.context import get_active_session\nsession = get_active_session()\n\n#Snowflake libraries \nfrom snowflake import snowpark\nfrom snowflake.ml import dataset\nfrom snowflake.snowpark.functions import col\nfrom snowflake.snowpark.types import *\n\n\n# python libraries \nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport time\nimport json\nfrom IPython.display import display\n\n## set the database and schema\nsession.use_database('ml_models')\nsession.use_schema('ml_models.ds')\n",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "8d50cbf4-0c8d-4950-86cb-114990437ac9",
   "metadata": {
    "language": "python",
    "name": "downloadData"
   },
   "source": "#download the data \nchurn = session.table(\"CHURN\")\n\nchurn.head(5)",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "beaacb4a-a9ef-4c2b-9bfe-e3ba6067fb5c",
   "metadata": {
    "name": "cell2",
    "collapsed": false
   },
   "source": "## EDA\n\nLet’s explore the dataset further and uncover additional insights."
  },
  {
   "cell_type": "code",
   "id": "c695373e-ac74-4b62-a1f1-08206cbd5c81",
   "metadata": {
    "language": "python",
    "name": "EDA"
   },
   "source": "# get the numerical and categorical features\nnumerical_columns = churn.select_dtypes(include=['number']).columns.tolist()\ncategorical_columns = churn.select_dtypes(include=['object', 'category', 'bool']).columns.tolist()\n\nprint(\"Numerical Columns:\", numerical_columns)\nprint(\"Categorical Columns:\", categorical_columns)",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "de9b1cea-3344-4117-8989-b0af6eaa36be",
   "metadata": {
    "language": "python",
    "name": "Hist"
   },
   "outputs": [],
   "source": "pd.set_option(\"display.max_columns\", 500)\ndf = churn.describe()\ndf\nhist = churn.hist(bins=30, sharey=True, figsize=(10, 10))",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "bd05e5bd-de68-429c-bfe3-95695392996c",
   "metadata": {
    "name": "cell1",
    "collapsed": false
   },
   "source": "We can see immediately that: - State appears to be quite evenly distributed. - Phone takes on too many unique values to be of any practical use. It’s possible that parsing out the prefix could have some value, but without more context on how these are allocated, we should avoid using it. - Most of the numeric features are surprisingly nicely distributed, with many showing bell-like gaussianity. VMail Message is a notable exception (and Area Code showing up as a feature we should convert to non-numeric)."
  },
  {
   "cell_type": "code",
   "id": "e166619a-c6af-4f75-aeac-0281e928a2df",
   "metadata": {
    "language": "python",
    "name": "drop_phone"
   },
   "outputs": [],
   "source": "churn = churn.drop(\"PHONE\", axis=1)\nchurn[\"AREA_CODE\"] = churn[\"AREA_CODE\"].astype(object)\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "41e13eb5-4b31-43a8-ac9e-a241e60b4666",
   "metadata": {
    "language": "python",
    "name": "histplot",
    "collapsed": false,
    "codeCollapsed": true
   },
   "outputs": [],
   "source": "import matplotlib.pyplot as plt\n\n# Histograms of numeric features by CHURN class\nfor column in churn.select_dtypes(include=[\"number\"]).columns:\n    hist = churn[[column, \"CHURN\"]].hist(by=\"CHURN\", bins=30, edgecolor='black', figsize=(4, 3))\n    plt.suptitle(f\"{column} by CHURN\", y=1)  # Add title\n    plt.tight_layout()\n    plt.show()\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "4c410412-42f3-4972-8068-b93d40f8e5e1",
   "metadata": {
    "language": "python",
    "name": "corr"
   },
   "outputs": [],
   "source": "df_corr = churn.select_dtypes(include=['number']).corr()\ndf_corr",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "cd41666f-1de9-40bb-b6b0-e2390b736b5f",
   "metadata": {
    "language": "python",
    "name": "matrix",
    "collapsed": false,
    "codeCollapsed": true
   },
   "outputs": [],
   "source": "# Scatter matrix only on numeric columns\npd.plotting.scatter_matrix(churn.select_dtypes(include=['number']), figsize=(12, 12), diagonal='hist', alpha=0.5)\nplt.suptitle(\"Scatter Matrix of Numeric Features\", y=1)\nplt.show()",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "7dc5ec61-0ef1-494a-af01-65c39e1e3423",
   "metadata": {
    "name": "cell8",
    "collapsed": false
   },
   "source": "We see several features that essentially have 100% correlation with one another. Including these feature pairs in some machine learning algorithms can create catastrophic problems, while in others it will only introduce minor redundancy and bias. Let’s remove one feature from each of the highly correlated pairs: Day Charge from the pair with Day Mins, Night Charge from the pair with Night Mins, Intl Charge from the pair with Intl Mins:"
  },
  {
   "cell_type": "code",
   "id": "534eedd3-31ea-4b44-8292-5a9073aa6521",
   "metadata": {
    "language": "python",
    "name": "cell9",
    "collapsed": true,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "#churn.columns\nchurn = churn.drop([\"Day Charge\", \"Eve Charge\", \"Night Charge\", \"Intl Charge\"], axis=1)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "15b86be3-987d-426d-a3d6-634a1dac6f5b",
   "metadata": {
    "language": "python",
    "name": "cell16"
   },
   "outputs": [],
   "source": "\n# Make a copy to avoid modifying the original\ndf = churn.copy()\n\n# Step 1: Convert bool columns to string so they are treated as categorical\nbool_cols = df.select_dtypes(include='bool').columns\ndf[bool_cols] = df[bool_cols].astype(str)\n\n# Step 2: One-hot encode object and bool columns, dropping first level\ncategorical_cols = df.select_dtypes(include='object').columns.union(bool_cols)\ndf_encoded = pd.get_dummies(df, columns=categorical_cols, drop_first=True)\n\n# Step 3: Check result\n\ndf_encoded.head()",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "32673c44-0071-4f4a-95e3-cae5f2f1fcd6",
   "metadata": {
    "name": "cell11",
    "collapsed": false
   },
   "source": "But first, let’s convert our categorical features into numeric features."
  },
  {
   "cell_type": "markdown",
   "id": "f6383ab1-65bf-401e-b917-06ce75c739b2",
   "metadata": {
    "name": "cell7",
    "collapsed": false
   },
   "source": "Let’s split the data into training, validation, and test sets."
  },
  {
   "cell_type": "code",
   "id": "760188b7-c8fe-4156-938d-9137ae9430ff",
   "metadata": {
    "language": "sql",
    "name": "run_if_needed"
   },
   "outputs": [],
   "source": "ALTER DATASET CHURN_TRAIN_DF DROP VERSION 'v1';\nALTER DATASET CHURN_TEST_DF DROP VERSION 'v1';\nALTER DATASET CHURN_VALIDATION_DF DROP VERSION 'v1';\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a012985c-77cd-4459-a5a5-94d069535901",
   "metadata": {
    "language": "python",
    "name": "snfdataset"
   },
   "outputs": [],
   "source": "train_data, validation_data, test_data = np.split(\n    df_encoded.sample(frac=1, random_state=1729),\n    [int(0.7 * len(df_encoded)), int(0.9 * len(df_encoded))],\n)\n\n\n## we will keep the dataset in snowflake for future use\nfrom snowflake.ml import dataset\n\ntrain_df = session.create_dataframe(train_data)\nvalidation_df =session.create_dataframe(validation_data)\ntest_df = session.create_dataframe(test_data)\n\n# Materialize DataFrame contents into a Dataset\nds1 = dataset.create_from_dataframe(\n    session,\n    \"churn_train_df\",\n    \"v1\",\n    input_dataframe=train_df)\nds2 = dataset.create_from_dataframe(\n    session,\n    \"churn_test_df\",\n    \"v1\",\n    input_dataframe=train_df)\nds3 = dataset.create_from_dataframe(\n    session,\n    \"churn_validation_df\",\n    \"v1\",\n    input_dataframe=train_df)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "963dd88e-2e9e-4ede-995f-7c596e0557ea",
   "metadata": {
    "language": "python",
    "name": "download_traindata"
   },
   "outputs": [],
   "source": "# Create a DataConnector from a Snowflake Dataset\nds_train = dataset.load_dataset(session, \"churn_train_df\", \"v1\")\n# Get a Snowpark DataFrame\ndf_train = ds_train.read.to_snowpark_dataframe().to_pandas()\n\nds_validation = dataset.load_dataset(session, \"churn_validation_df\", \"v1\")\ndf_validation = ds_validation.read.to_snowpark_dataframe().to_pandas()\n\n\nds_test = dataset.load_dataset(session, \"churn_test_df\", \"v1\")\ndf_test = ds_test.read.to_snowpark_dataframe().to_pandas()\n\n\n\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e320dcce-015c-4aab-8384-eff783010070",
   "metadata": {
    "language": "python",
    "name": "cell14"
   },
   "outputs": [],
   "source": "df_train.columns",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "521da291-f095-49c9-8cd1-09b5790f111f",
   "metadata": {
    "language": "python",
    "name": "cell15"
   },
   "outputs": [],
   "source": "import xgboost as xgb # pre-install with snowflake container runtime notebook \nfrom sklearn.metrics import accuracy_score, classification_report\nimport matplotlib.pyplot as plt\n\n# Assuming 'CHURN_Yes' is your target\nX = df_encoded.drop(columns=['CHURN_True.'])\ny = df_encoded['CHURN_True.']\n\n\n# Step 1: Define feature and target columns\ntarget_col = 'CHURN_True.'\nX_train = df_train.drop(columns=['CHURN_True.'])\ny_train = df_train['CHURN_True.']\n\nX_val = df_validation.drop(columns=['CHURN_True.'])\ny_val = df_validation['CHURN_True.']\n\nX_test = df_test.drop(columns=['CHURN_True.'])\ny_test = df_test['CHURN_True.']\n\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "51214f91-08f1-434d-9fc3-3332f22841ef",
   "metadata": {
    "language": "python",
    "name": "cell13"
   },
   "outputs": [],
   "source": "from xgboost import XGBClassifier\nfrom sklearn.metrics import accuracy_score, classification_report\n\n2r\nmodel.fit(\n    X_train,\n    y_train,\n    eval_set=[(X_val, y_val)],\n    verbose=True\n)\n\n\n# Predict on test\ny_pred = model.predict(X_test)\n\n# Evaluate\nprint(\"Test Accuracy:\", accuracy_score(y_test, y_pred))\nprint(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
   "execution_count": null
  }
 ]
}