{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f5b24602-e745-4bb4-af72-d49ae2f07bea",
   "metadata": {
    "collapsed": false,
    "name": "title"
   },
   "source": [
    "# Quickstart: Running DMFs as Quality Gate in ELT Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b7cbbb2-b57b-4832-8403-8d4f81efa1c2",
   "metadata": {
    "collapsed": false,
    "name": "blogpost_link"
   },
   "source": [
    "See the full blog-post from Jan Sommerfeld here on Medium: https://medium.com/snowflake/how-to-add-quality-checks-to-data-pipelines-using-the-new-snowflake-dmfs-e08b4174f3d9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca3269a-05b7-471f-a303-52ac52d3cdda",
   "metadata": {
    "collapsed": false,
    "name": "intro"
   },
   "source": [
    "Snowflake has released Data Metric Functions (DMFs) - a native solution to run a range of quality checks on your data (requires Enterprise edition or higher). Users can either choose from a growing library of system DMFs or write their own “UDMFs” with custom logic and thresholds.\n",
    "\n",
    "Users use Tasks, a native orchestration capability, to schedule, modularize and orchestrate our ELT processing steps by connecting multiple Tasks to a Task Graph (aka DAG). Each Task runs a piece of code on a certain trigger and optionally a defined condition. Since Tasks can run almost anything (python, java, scala, sql, function, stored procedures, notebooks,…) they can also run Data Metric Functions. This allows us to integrate data quality checks deeply into our ingestion and transformation pipelines.\n",
    "\n",
    "***With the following 6 steps we will set up a simple ELT data pipeline based on data quality checks that you can easily apply to your existing or next Task pipeline.***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8256765-a11c-42d6-91b4-d92786463c9c",
   "metadata": {
    "collapsed": false,
    "name": "STEP_1"
   },
   "source": [
    "## 1. Set up Demo Data Ingestion Stream\n",
    "\n",
    "For simplicity we will just use the ACCOUNTADMIN role for this demo setup. If you don’t have it or want to use a separate role for this demo, you can check the Appendix at the end to grant all required privileges.\n",
    "All following code will run in the context of this DEMO schema. So make sure you keep the context or use your own schema and warehouse.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "206c4c07-5ad8-43f7-a458-f1f1a02ea179",
   "metadata": {
    "language": "sql",
    "name": "setup_prep"
   },
   "outputs": [],
   "source": [
    "use role ACCOUNTADMIN;\n",
    "\n",
    "create warehouse if not exists DEX_WH\n",
    "    warehouse_size = XSMALL\n",
    "    auto_suspend = 2;\n",
    "\n",
    "create database if not exists DEX_DB;\n",
    "create schema if not exists DEX_DB.DEMO;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a03e38f4-c1b5-4378-a692-bb61850be81a",
   "metadata": {
    "collapsed": false,
    "name": "get_from_Marketplace"
   },
   "source": [
    "Just to have a live demo we will first set up a Task that loads new rows into our source table to simulate a continuous ingestion. In your case that could be from a user interface, or something like sensor-data or analytics from a connector or some other database.\n",
    "\n",
    "We will use some free weather data from the **Snowflake Marketplace**:\n",
    "+ Go to Snowflake Marketplace \n",
    "+ Get the free **\"Weather Source LLC: frostbyte\"** data share\n",
    "*(This data may be used in connection with the Snowflake Quickstart, but is provided solely by WeatherSource, and not by or on behalf of Snowflake.)*\n",
    "+ Under \"options rename the shared database \"DEMO_WEATHER_DATA\" just to shorten it\n",
    "\n",
    "Now we can run the script below to create a Task that continuously loads small batches of data into a source table, while **intentionally adding some quality issues** to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac753eca-c04e-48c9-bb73-807ef5e649da",
   "metadata": {
    "language": "sql",
    "name": "create_table_ALL_WEATHER_DATA"
   },
   "outputs": [],
   "source": [
    "--- copy a sample of the data share into a new table \n",
    "create or replace table ALL_WEATHER_DATA\n",
    "as\n",
    "select\n",
    "    ROW_NUMBER() over (order by DATE_VALID_STD desc, POSTAL_CODE) as ROW_ID,\n",
    "    DATE_VALID_STD as DS,\n",
    "    POSTAL_CODE as ZIPCODE,\n",
    "    MIN_TEMPERATURE_AIR_2M_F as MIN_TEMP_IN_F,\n",
    "    AVG_TEMPERATURE_AIR_2M_F as AVG_TEMP_IN_F,\n",
    "    MAX_TEMPERATURE_AIR_2M_F as MAX_TEMP_IN_F,\n",
    "from\n",
    "    DEMO_WEATHER_DATA.ONPOINT_ID.HISTORY_DAY\n",
    "where\n",
    "    COUNTRY = 'US'\n",
    "order by\n",
    "    DATE_VALID_STD desc,\n",
    "    POSTAL_CODE\n",
    "limit \n",
    "    100000;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e65137e-77e9-4dfe-89b1-6ff855ec2281",
   "metadata": {
    "language": "sql",
    "name": "create_table_CONTINUOUS_WEATHER_DATA"
   },
   "outputs": [],
   "source": [
    "--- continuously growing table with weather data as \"external data source\"\n",
    "create or replace table CONTINUOUS_WEATHER_DATA(\n",
    "    ROW_ID number,\n",
    "    INSERTED timestamp,\n",
    "    DS date,\n",
    "    ZIPCODE varchar,\n",
    "    MIN_TEMP_IN_F number,\n",
    "    AVG_TEMP_IN_F number,\n",
    "    MAX_TEMP_IN_F number\n",
    ")\n",
    "comment = 'Demo Source table'\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca5aefe-c473-49a6-aeeb-45710b66103c",
   "metadata": {
    "language": "sql",
    "name": "create_task_for_dummy_data"
   },
   "outputs": [],
   "source": [
    "create or replace task ADD_WEATHER_DATA_TO_SOURCE\n",
    "schedule = '5 minutes'\n",
    "comment = 'adding 10 rows of weather data every 5 minutes and adding occasional anomalies'\n",
    "as\n",
    "begin\n",
    "    if (\n",
    "        (select \n",
    "            count(*)\n",
    "        from \n",
    "            ALL_WEATHER_DATA A\n",
    "        left join \n",
    "            CONTINUOUS_WEATHER_DATA C\n",
    "            ON A.ROW_ID = C.ROW_ID\n",
    "        where\n",
    "            C.ROW_ID is NULL\n",
    "        ) != 0 )\n",
    "    then\n",
    "        delete from CONTINUOUS_WEATHER_DATA;\n",
    "    end if;\n",
    "    \n",
    "    insert into CONTINUOUS_WEATHER_DATA (\n",
    "        ROW_ID,\n",
    "        INSERTED,\n",
    "        DS,\n",
    "        ZIPCODE,\n",
    "        MIN_TEMP_IN_F,\n",
    "        AVG_TEMP_IN_F,\n",
    "        MAX_TEMP_IN_F\n",
    "    )\n",
    "    select\n",
    "        A.ROW_ID,\n",
    "        current_timestamp() as INSERTED,\n",
    "        A.DS,\n",
    "        A.ZIPCODE as ZIPCODE,\n",
    "--        case when A.ZIPCODE > 2000 then A.ZIPCODE else NULL end as ZIPCODE,\n",
    "        A.MIN_TEMP_IN_F,\n",
    "        A.AVG_TEMP_IN_F,\n",
    "        case when uniform(1, 100, random()) != 1 then A.MAX_TEMP_IN_F else A.MAX_TEMP_IN_F * 8 end as MAX_TEMP_IN_F\n",
    "    from \n",
    "        ALL_WEATHER_DATA A\n",
    "    left join \n",
    "        CONTINUOUS_WEATHER_DATA C\n",
    "        ON A.ROW_ID = C.ROW_ID\n",
    "    where\n",
    "        C.ROW_ID is NULL\n",
    "    limit\n",
    "        10;\n",
    "        \n",
    "end\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68dd8059-2698-4060-bb0d-13bf19578b5a",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "sql",
    "name": "resume_dummy_data_generator"
   },
   "outputs": [],
   "source": [
    "alter task ADD_WEATHER_DATA_TO_SOURCE resume;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e1b912-a061-44a1-a8ca-1064495df775",
   "metadata": {
    "collapsed": false,
    "name": "STEP_2"
   },
   "source": [
    "## 2.  Setting up the demo transformation pipeline\n",
    "\n",
    "For this demo setup we will use 4 tables:\n",
    "\n",
    "* Source table - where new data comes in\n",
    "* Landing table - where we load the new batch and run the quality checks on it\n",
    "* Target table - for all “clean” data that meets expectations\n",
    "* Quarantine table - for all “bad” data that failed expectations\n",
    "\n",
    "The source table we already have from Step 2. So let’s create the other three:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4269b89b-4b05-40e5-bfb1-1eb6ee199a32",
   "metadata": {
    "collapsed": false,
    "language": "sql",
    "name": "create_raw_table"
   },
   "outputs": [],
   "source": [
    "create or replace table RAW_WEATHER_DATA (\n",
    "    ROW_ID number,\n",
    "    INSERTED timestamp,\n",
    "    DS date, \n",
    "    ZIPCODE varchar,\n",
    "    MIN_TEMP_IN_F number,\n",
    "    AVG_TEMP_IN_F number,\n",
    "    MAX_TEMP_IN_F number\n",
    ")\n",
    "comment = 'Demo Landing table'\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b6f92d-bb0b-490d-a1ea-23f6ac7e3fa2",
   "metadata": {
    "collapsed": false,
    "language": "sql",
    "name": "create_clean_table"
   },
   "outputs": [],
   "source": [
    "create or replace table CLEAN_WEATHER_DATA (\n",
    "    DS date, \n",
    "    ZIPCODE varchar,\n",
    "    MIN_TEMP_IN_F number,\n",
    "    AVG_TEMP_IN_F number,\n",
    "    MAX_TEMP_IN_F number\n",
    ")\n",
    "comment = 'Demo Target table'\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d79e32b8-f8cb-4968-9a6d-9aa2cee998c0",
   "metadata": {
    "collapsed": false,
    "language": "sql",
    "name": "create_table_quarantine_data"
   },
   "outputs": [],
   "source": [
    "create or replace table QUARANTINED_WEATHER_DATA (\n",
    "    INSERTED timestamp,\n",
    "    DS date, \n",
    "    ZIPCODE varchar,\n",
    "    MIN_TEMP_IN_F number,\n",
    "    AVG_TEMP_IN_F number,\n",
    "    MAX_TEMP_IN_F number\n",
    ")\n",
    "comment = 'Demo Quarantine table'\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b6bb69-3006-4a2a-b4a5-c445920e8e5e",
   "metadata": {
    "collapsed": false,
    "name": "cell2"
   },
   "source": [
    "Now we can build a **Task Graph** that runs whenever new data is added to the source table. \n",
    "So first we set up a Stream on the source table CONTINUOUS_WEATHER_DATA:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb00441a-93f1-493b-a465-7929d02ee788",
   "metadata": {
    "collapsed": false,
    "language": "sql",
    "name": "create_stream"
   },
   "outputs": [],
   "source": [
    "create or replace stream NEW_WEATHER_DATA\n",
    "    on table CONTINUOUS_WEATHER_DATA\n",
    "    append_only = TRUE\n",
    "    comment = 'checking for new weather data coming in'\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "799e332b-bea3-42f5-8b37-2d477bd19e4d",
   "metadata": {
    "collapsed": false,
    "name": "Triggered_Tasks"
   },
   "source": [
    "Next we create the first Task to insert all new rows from the Stream into the landing table RAW_WEATHER_TABLE as soon as new data is available.\n",
    "\n",
    "🔔 ***New Feature: “Triggered Tasks”** — We can simplify orchestration by omitting the schedule for our task and just set STREAM_HAS_DATA as a condition for the task to run.* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d50cbf4-0c8d-4950-86cb-114990437ac9",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "sql",
    "name": "Task_1"
   },
   "outputs": [],
   "source": [
    "create or replace task LOAD_RAW_DATA\n",
    "warehouse = 'DEX_WH'\n",
    "when\n",
    "    SYSTEM$STREAM_HAS_DATA('NEW_WEATHER_DATA')\n",
    "as \n",
    "declare\n",
    "    ROWS_LOADED number;\n",
    "    RESULT_STRING varchar;\n",
    "begin\n",
    "    insert into RAW_WEATHER_DATA (\n",
    "        ROW_ID,\n",
    "        INSERTED,\n",
    "        DS,\n",
    "        ZIPCODE,\n",
    "        MIN_TEMP_IN_F,\n",
    "        AVG_TEMP_IN_F,\n",
    "        MAX_TEMP_IN_F\n",
    "    )\n",
    "    select \n",
    "        ROW_ID,\n",
    "        INSERTED,\n",
    "        DS,\n",
    "        ZIPCODE,\n",
    "        MIN_TEMP_IN_F,\n",
    "        AVG_TEMP_IN_F,\n",
    "        MAX_TEMP_IN_F\n",
    "    from \n",
    "        NEW_WEATHER_DATA\n",
    "    ;\n",
    "\n",
    "    --- to see number of rows loaded in the IU\n",
    "    ROWS_LOADED := (select $1 from table(RESULT_SCAN(LAST_QUERY_ID())));\n",
    "    RESULT_STRING := :ROWS_LOADED||' rows loaded into RAW_WEATHER_DATA';\n",
    "    call SYSTEM$SET_RETURN_VALUE(:RESULT_STRING);\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1c56edd-04ab-4027-82e4-823f899ee5a5",
   "metadata": {
    "collapsed": false,
    "name": "Task_2"
   },
   "source": [
    "**Task 2: Transformation**\n",
    "\n",
    "This second task will run directly after the first task and simulate a transformation of the new dataset. In your case this might be much more complex. For our demo we keep it simple and just filter for the hot days with an average temperature over 68°F.\n",
    "\n",
    "Once the new data is inserted into the target table CLEAN_WEATHER_DATA we empty the landing table again.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ecc685d-fd6a-4073-945e-8d92e703c811",
   "metadata": {
    "collapsed": false,
    "language": "sql",
    "name": "task_to_continue_transformation"
   },
   "outputs": [],
   "source": [
    "create or replace task TRANSFORM_DATA\n",
    "warehouse = 'DEX_WH'\n",
    "after \n",
    "    LOAD_RAW_DATA\n",
    "as \n",
    "begin\n",
    "    insert into CLEAN_WEATHER_DATA (\n",
    "            DS,\n",
    "            ZIPCODE,\n",
    "            MIN_TEMP_IN_F,\n",
    "            AVG_TEMP_IN_F,\n",
    "            MAX_TEMP_IN_F\n",
    "            )\n",
    "        select \n",
    "            DS,\n",
    "            ZIPCODE,\n",
    "            MIN_TEMP_IN_F,\n",
    "            AVG_TEMP_IN_F,\n",
    "            MAX_TEMP_IN_F\n",
    "        from \n",
    "            RAW_WEATHER_DATA\n",
    "        where\n",
    "            AVG_TEMP_IN_F > 68\n",
    "        ;\n",
    "    delete from RAW_WEATHER_DATA;\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae7b3d65-35dd-4efc-a71d-0c2024544853",
   "metadata": {
    "language": "sql",
    "name": "Task_3"
   },
   "outputs": [],
   "source": [
    "-- lets just add one more to indicate the potential for further steps\n",
    "create or replace task MORE_TRANSFORMATION\n",
    "warehouse = 'DEX_WH'\n",
    "after \n",
    "    TRANSFORM_DATA\n",
    "as \n",
    "    select \n",
    "count(*) \n",
    "    from\n",
    "        CLEAN_WEATHER_DATA\n",
    ";\n",
    "\n",
    "-- resume all Tasks of the graph\n",
    "select SYSTEM$TASK_DEPENDENTS_ENABLE('LOAD_RAW_DATA');\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a02c2d88-9786-438e-a002-c5d80936fbaf",
   "metadata": {
    "collapsed": false,
    "name": "cell5"
   },
   "source": [
    "Let’s switch to the Task Graph UI to \n",
    "* See the graph we created\n",
    "* Check the run history to see if we have any errors\n",
    "* check the return values for each Task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d54c2edc-5156-400a-9a89-716130315fb9",
   "metadata": {
    "collapsed": false,
    "name": "STEP_3"
   },
   "source": [
    "## 3. Assigning quality checks to the landing table\n",
    "\n",
    "Let’s first have a look at all system Data Metric Functions that are already available by default. We can see them in Snowsight as Functions under the **SNOWFLAKE.CORE** schema or alternatively query for all DMFs in the account that our role is allowed to see:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "216c8e32-8778-420f-aab5-d7dc1a41eff1",
   "metadata": {
    "language": "sql",
    "name": "show_DMFs_in_account"
   },
   "outputs": [],
   "source": [
    "show data metric functions in account;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "635120ff-ab07-473c-98ed-12d1dc8fff7b",
   "metadata": {
    "collapsed": false,
    "name": "cell7"
   },
   "source": [
    "Now for our specific Demo dataset we want to also add a range-check to make sure that our temperature values are plausible and further data analysis from consumers downstream is not impacted by unrealistic values caused by faulty sensors.\n",
    "\n",
    "For that we can write a UDMF (user-defined Data Metric Function) defining a range of plausible fahrenheit values:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f84a71-38b8-47c3-afb5-ad6b4bc13e41",
   "metadata": {
    "collapsed": false,
    "language": "sql",
    "name": "create_custom_DMF"
   },
   "outputs": [],
   "source": [
    "create or replace data metric function CHECK_FARENHEIT_PLAUSIBLE(\n",
    "  TABLE_NAME table(\n",
    "    COLUMN_VALUE number\n",
    "  )\n",
    ")\n",
    "returns NUMBER\n",
    "as\n",
    "$$\n",
    "  select\n",
    "    count(*)\n",
    "  from \n",
    "    TABLE_NAME\n",
    "  where\n",
    "    COLUMN_VALUE is not NULL\n",
    "    and COLUMN_VALUE not between -40 and 140 \n",
    "$$\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c635d722-81ac-4c9d-a833-4aca81677ad5",
   "metadata": {
    "collapsed": false,
    "name": "cell8"
   },
   "source": [
    "We can now test our UDMF by test-running it manually on our source table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c0a942-6eaf-4742-bd81-35303963397a",
   "metadata": {
    "language": "sql",
    "name": "test_UDMF"
   },
   "outputs": [],
   "source": [
    "--- manually test-run the UDMF on our source table\n",
    "select\n",
    "    CHECK_FARENHEIT_PLAUSIBLE(          --- the UDMF\n",
    "        select MAX_TEMP_IN_F            --- table column\n",
    "        from CONTINUOUS_WEATHER_DATA    --- our source table\n",
    ") as WRONG_FARENHEIT_VALUE\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "578b4db2-1098-4b01-b4f2-b761210b304e",
   "metadata": {
    "collapsed": false,
    "name": "cell9"
   },
   "source": [
    "Now we can assign our UDMF together with a few system DMFs to our landing table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e19ccf6-0d30-4917-ba26-b02a9ae5674e",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "sql",
    "name": "assign_DMFs"
   },
   "outputs": [],
   "source": [
    "-- always set the schedule first\n",
    "alter table RAW_WEATHER_DATA\n",
    "    set DATA_METRIC_SCHEDULE = 'TRIGGER_ON_CHANGES';\n",
    "\n",
    "    \n",
    "--- assign DMFs to our RAW_WEATHER_DATA\n",
    "alter table RAW_WEATHER_DATA\n",
    "    add data metric function SNOWFLAKE.CORE.DUPLICATE_COUNT on (ROW_ID);\n",
    "\n",
    "alter table RAW_WEATHER_DATA\n",
    "    add data metric function SNOWFLAKE.CORE.NULL_COUNT on (DS);\n",
    "\n",
    "alter table RAW_WEATHER_DATA\n",
    "    add data metric function SNOWFLAKE.CORE.NULL_COUNT on (ZIPCODE);\n",
    "\n",
    "-- add a custom DMF\n",
    "alter table RAW_WEATHER_DATA\n",
    "    add data metric function CHECK_FARENHEIT_PLAUSIBLE on (MAX_TEMP_IN_F);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb45ec0-1626-4676-8c7f-13bc4e4abdf0",
   "metadata": {
    "collapsed": false,
    "name": "cell10"
   },
   "source": [
    "The results of all scheduled checks performed by Data Metric Functions assigned to tables are stored in the view SNOWFLAKE.LOCAL.DATA_QUALITY_MONITORING_RESULTS. So we can query them or build us a simple Snowsight dashboard by running something like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99cbda12-c92c-4234-b1cf-0974bec4ff6a",
   "metadata": {
    "language": "sql",
    "name": "DMF_History"
   },
   "outputs": [],
   "source": [
    "select\n",
    "    MEASUREMENT_TIME,\n",
    "    METRIC_NAME,\n",
    "    VALUE,\n",
    "    TABLE_NAME,\n",
    "    ARGUMENT_NAMES\n",
    "from\n",
    "    SNOWFLAKE.LOCAL.DATA_QUALITY_MONITORING_RESULTS\n",
    "where\n",
    "    TABLE_NAME = 'RAW_WEATHER_DATA'\n",
    "    and TABLE_SCHEMA = 'DEMO'\n",
    "order by\n",
    "    MEASUREMENT_TIME desc\n",
    "limit \n",
    "    1000;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af72e137-e779-4af1-8677-0b1878a66af9",
   "metadata": {
    "collapsed": false,
    "name": "STEP_4"
   },
   "source": [
    "## 4. Run DMFs as \"Quality gate\" part of the pipeline\n",
    "\n",
    "Because we want our quality check Task to run all DMFs that are assigned to our landing table, even if we add or remove some DMFs later on, we don’t just want to call them explicitly from the Task. Instead we first build a helper function to modularize our code.\n",
    "\n",
    "The function (UDTF) will accept a table name as argument and return all DMFs that are currently assigned to a column of this table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd2f809-ecfc-463a-8fc5-839b18cf5939",
   "metadata": {
    "collapsed": false,
    "language": "sql",
    "name": "Function_to_get_active_DMFs"
   },
   "outputs": [],
   "source": [
    "--- create a helper function to get all DMFs on a table\n",
    "\n",
    "create or replace function GET_ACTIVE_QUALITY_CHECKS(\"TABLE_NAME\" VARCHAR)\n",
    "returns table(DMF VARCHAR, COL VARCHAR)\n",
    "language SQL\n",
    "as \n",
    "$$\n",
    "    select \n",
    "        t1.METRIC_DATABASE_NAME||'.'||METRIC_SCHEMA_NAME||'.'||METRIC_NAME as DMF,\n",
    "        REF.value:name ::string as COL\n",
    "    from\n",
    "        table(\n",
    "            INFORMATION_SCHEMA.DATA_METRIC_FUNCTION_REFERENCES(\n",
    "                REF_ENTITY_NAME => TABLE_NAME,\n",
    "                REF_ENTITY_DOMAIN => 'table'\n",
    "            )) as t1,\n",
    "        table(flatten(input => parse_json(t1.REF_ARGUMENTS))) as REF    \n",
    "    where\n",
    "        SCHEDULE_STATUS = 'STARTED' \n",
    "$$\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1143ce0f-0d74-473e-8100-99a239481f24",
   "metadata": {
    "collapsed": false,
    "name": "cell11"
   },
   "source": [
    "Before we call it within the Task, let’s test run it first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a49aacf3-1540-4828-967d-7304d10072e1",
   "metadata": {
    "language": "sql",
    "name": "test_helper_function"
   },
   "outputs": [],
   "source": [
    "select DMF, COL from table(GET_ACTIVE_QUALITY_CHECKS('DEX_DB.DEMO.RAW_WEATHER_DATA'));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7806eff-6173-4d79-9539-3363eb4c52c7",
   "metadata": {
    "collapsed": false,
    "name": "cell12"
   },
   "source": [
    "Now we can define a new Task to get all DMFs from this function and then run them all.\n",
    "\n",
    "We store the result of each check in a TEST_RESULT variable and then sum them up in a RESULTS_SUMMARY variable.\n",
    "\n",
    "This will give us the total of issues found from all checks and we can pass it on as output to the **Return value** of this Task. \n",
    "\n",
    "If our RESULT_SUMMARY remains ‘0’ then we know all checks have passed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b17524-3f9e-46d8-b122-981b02988cc8",
   "metadata": {
    "collapsed": false,
    "language": "sql",
    "name": "task_running_quality_checks"
   },
   "outputs": [],
   "source": [
    "-- suspend the graph so we can make changes\n",
    "alter task LOAD_RAW_DATA suspend;\n",
    "\n",
    "-- new task to run all DMFs on the landing table\n",
    "create or replace task CHECK_DATA_QUALITY\n",
    "warehouse = 'DEX_WH'\n",
    "after \n",
    "    LOAD_RAW_DATA\n",
    "as \n",
    "declare\n",
    "    TEST_RESULT number;\n",
    "    RESULTS_SUMMARY number default 0;\n",
    "    RESULT_STRING varchar;\n",
    "    c1 CURSOR for \n",
    "            --- get all DMFs and columns for active quality checks on this table by using the custom function \n",
    "                select DMF, COL from table(GET_ACTIVE_QUALITY_CHECKS('DEX_DB.DEMO.RAW_WEATHER_DATA'));\n",
    "begin\n",
    "    OPEN c1;\n",
    "    --- looping throught all DMFs assigned to the table\n",
    "    for REC in c1 DO\n",
    "\n",
    "        --- manually run the DMF\n",
    "        execute immediate 'select '||REC.DMF||'(select '||REC.COL||' from RAW_WEATHER_DATA);';  \n",
    "\n",
    "        ---get the test result\n",
    "        TEST_RESULT := (select $1 from table(RESULT_SCAN(LAST_QUERY_ID())));\n",
    "                    \n",
    "        -- Construct the results summary: if check did not pass then add issues to the counter\n",
    "        if (:TEST_RESULT != 0)\n",
    "            then RESULTS_SUMMARY := (:RESULTS_SUMMARY + :TEST_RESULT);\n",
    "        end if;\n",
    "    \n",
    "    end for;\n",
    "    CLOSE c1;\n",
    "\n",
    "     --- construct result-string to act as condition for downstream tasks and to show number of quality issues found\n",
    "    RESULT_STRING := (:RESULTS_SUMMARY||' separate quality issues found in table RAW_WEATHER_DATA');\n",
    "    \n",
    "    case when :RESULTS_SUMMARY = 0\n",
    "    then\n",
    "        call SYSTEM$SET_RETURN_VALUE('✅ All quality checks on RAW_WEATHER_DATA passed');\n",
    "    else   \n",
    "        call SYSTEM$SET_RETURN_VALUE(:RESULT_STRING);\n",
    "    end;\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3230ad0e-baa2-462f-a2b9-f71abefccaa6",
   "metadata": {
    "collapsed": false,
    "name": "Task_return_value"
   },
   "source": [
    "Now we just have to update our other transformation tasks to run AFTER the new quality check task.\n",
    "\n",
    "And we are adding a condition to run ONLY if all quality checks have passed. For that we can use the Task return value as a condition.\n",
    "\n",
    "🔔 ***New Feature: “Task Return Value as Condition”**  —  We can add a condition for a Child Task to run, based on the Return Value of a predecessor Task.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e213bb9-fbc6-4dc8-b5da-e300856b316a",
   "metadata": {
    "collapsed": false,
    "language": "sql",
    "name": "alter_dependencies"
   },
   "outputs": [],
   "source": [
    "-- changing transformation task to now run after quality checks on only if all checks passed\n",
    "alter task TRANSFORM_DATA remove after LOAD_RAW_DATA;\n",
    "\n",
    "alter task TRANSFORM_DATA add after CHECK_DATA_QUALITY;\n",
    "\n",
    "alter task TRANSFORM_DATA modify when SYSTEM$GET_PREDECESSOR_RETURN_VALUE('CHECK_DATA_QUALITY') = '✅ All quality checks on RAW_WEATHER_DATA passed';\n",
    "\n",
    "-- resume all Tasks of the graph\n",
    "select SYSTEM$TASK_DEPENDENTS_ENABLE('LOAD_RAW_DATA');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a597bf3-ed3e-4fc7-8319-d7079dc5ee61",
   "metadata": {
    "collapsed": false,
    "name": "STEP_5"
   },
   "source": [
    "## 5. Isolate datasets with quality issues\n",
    "\n",
    "Now we could just completely ignore the new dataset, clear the landing table and wait for the next one. More likely though we want to analyze that dataset and potentially even fix the data quality issues. To do that later we will first isolate this batch into our quarantine table.\n",
    "\n",
    "So we add another Task to our graph and invert the condition so that it only runs when a quality check failed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b1b4f43-94a9-4b6a-8c21-91ddc111edb3",
   "metadata": {
    "collapsed": false,
    "language": "sql",
    "name": "task_to_isolate_data_issues"
   },
   "outputs": [],
   "source": [
    "-- suspend the graph so we can make changes\n",
    "alter task LOAD_RAW_DATA suspend;\n",
    "\n",
    "create or replace task ISOLATE_DATA_ISSUES\n",
    "comment = 'isolate bad rows and clear landing table'\n",
    "warehouse = 'DEX_WH'\n",
    "after \n",
    "    CHECK_DATA_QUALITY\n",
    "when \n",
    "    SYSTEM$GET_PREDECESSOR_RETURN_VALUE('CHECK_DATA_QUALITY') != '✅ All quality checks on RAW_WEATHER_DATA passed'\n",
    "as \n",
    "begin\n",
    "    insert into QUARANTINED_WEATHER_DATA (\n",
    "            INSERTED,\n",
    "            DS,\n",
    "            ZIPCODE,\n",
    "            MIN_TEMP_IN_F,\n",
    "            AVG_TEMP_IN_F,\n",
    "            MAX_TEMP_IN_F\n",
    "            )\n",
    "        select \n",
    "            INSERTED,\n",
    "            DS,\n",
    "            ZIPCODE,\n",
    "            MIN_TEMP_IN_F,\n",
    "            AVG_TEMP_IN_F,\n",
    "            MAX_TEMP_IN_F\n",
    "        from \n",
    "            RAW_WEATHER_DATA\n",
    "        ;\n",
    "    delete from RAW_WEATHER_DATA;\n",
    "end;\n",
    "\n",
    "\n",
    "-- resume all Tasks of the graph\n",
    "select SYSTEM$TASK_DEPENDENTS_ENABLE('LOAD_RAW_DATA');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd576611-4d2e-4a71-b30b-0ffae2fdc331",
   "metadata": {
    "collapsed": false,
    "name": "cell15"
   },
   "source": [
    "Now we can let this run, knowing that all batches with quality issues will be isolated and all batches that are good will be transformed further. Since we can not predict if and when this might happen, we want to finish this demo by adding a notification in case of quality issues."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2842577-e047-4d4e-8053-dbf6a857059e",
   "metadata": {
    "collapsed": false,
    "name": "STEP_6"
   },
   "source": [
    "## 6. Add notification about quality issues\n",
    "\n",
    "Let us add another Task to our graph to send a notification when quality issues have been detected and rows were isolated. But maybe we know our data is not perfect and we don't want to get a notification every single time.\n",
    "\n",
    "So let's use DMFs one more time to define a threshold and notify only when more than 1% of new weather data was quarantined. First we create a new UDMF to compare the number of rows in the quarantine table to those in the target table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86be2f1-efee-4ad9-854f-4605d3c78d36",
   "metadata": {
    "collapsed": false,
    "language": "sql",
    "name": "create_UDMF"
   },
   "outputs": [],
   "source": [
    "--- create a custom DMF for comparing isolated rows vs clean rows\n",
    "create or replace data metric function OVER_1PCT_ISOLATED_ROWS(\n",
    "    TABLE_NAME table(\n",
    "        DS date\n",
    "    )\n",
    ")\n",
    "returns NUMBER\n",
    "as\n",
    "$$\n",
    "  select\n",
    "    case \n",
    "        when (select count(*) from QUARANTINED_WEATHER_DATA) > (select count(*) from CLEAN_WEATHER_DATA)\n",
    "        then 1 \n",
    "        else\n",
    "            case when\n",
    "                (select count(*) from QUARANTINED_WEATHER_DATA) * 100.0 / \n",
    "                (select count(*) from CLEAN_WEATHER_DATA) > 1\n",
    "            then 1\n",
    "            else 0\n",
    "        end\n",
    "    end\n",
    "$$\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c63e1a00-714d-431c-aae9-8bb31dd8119f",
   "metadata": {
    "collapsed": false,
    "name": "cell16"
   },
   "source": [
    "Now we assign it to the quarantine table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc9368e-6905-4377-b338-889c8c940ac3",
   "metadata": {
    "collapsed": false,
    "language": "sql",
    "name": "add_DMFs_to_quaratine"
   },
   "outputs": [],
   "source": [
    "-- always set the schedule first\n",
    "alter table QUARANTINED_WEATHER_DATA\n",
    "    set DATA_METRIC_SCHEDULE = 'TRIGGER_ON_CHANGES';\n",
    "\n",
    "-- assign UDMF to QUARANTINED_WEATHER_DATA\n",
    "alter table QUARANTINED_WEATHER_DATA\n",
    "    add data metric function OVER_1PCT_ISOLATED_ROWS on (DS);\n",
    "\n",
    "-- add a row-count system DMF for additional context \n",
    "alter table QUARANTINED_WEATHER_DATA\n",
    "    add data metric function SNOWFLAKE.CORE.ROW_COUNT on ();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be15ea0-bd84-4b79-af79-c9f51381335e",
   "metadata": {
    "collapsed": false,
    "name": "cell17"
   },
   "source": [
    "And now we can create another task that runs only if new rows were isolated and then checks if they surpass the 1% threshold and only then sends us a notification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c04e1e9b-8fd3-4ae7-8fbc-ee40933565de",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "sql",
    "name": "task_to_send_notifications",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "alter task LOAD_RAW_DATA suspend;\n",
    "\n",
    "create or replace task NOTIFY_ABOUT_QUALITY_ISSUE\n",
    "warehouse = 'DEX_WH'\n",
    "after \n",
    "    ISOLATE_DATA_ISSUES\n",
    "as \n",
    "declare\n",
    "    TEST_RESULT integer;\n",
    "begin\n",
    "\n",
    "    TEST_RESULT := (select OVER_1_PERCENT from(\n",
    "        select OVER_1PCT_ISOLATED_ROWS( select DS from QUARANTINED_WEATHER_DATA)as OVER_1_PERCENT\n",
    "        )\n",
    "    );\n",
    "\n",
    "    case when :TEST_RESULT > 0 then\n",
    "        call SYSTEM$SEND_SNOWFLAKE_NOTIFICATION(\n",
    "            SNOWFLAKE.NOTIFICATION.TEXT_HTML(\n",
    "                'More than 1 percent of new weather data was quarantined due to data quality issues.'   -- my html message for emails\n",
    "            ),       \n",
    "            SNOWFLAKE.NOTIFICATION.EMAIL_INTEGRATION_CONFIG(\n",
    "                'YOUR_EMAIL_NOTIFICATION_INTEGRATION',             -- email integration\n",
    "                'Snowflake DEMO Pipeline Alert',                   -- email header\n",
    "                ARRAY_CONSTRUCT('YOUR_EMAIL_HERE')                 -- validated user email addresses\n",
    "            )       \n",
    "        );\n",
    "\n",
    "        call SYSTEM$SET_RETURN_VALUE('Over 1% bad rows. Notification sent to YOUR_EMAIL_NOTIFICATION_INTEGRATION');\n",
    "            \n",
    "    else   \n",
    "        call SYSTEM$SET_RETURN_VALUE('Less than 1% bad rows. No notification sent.');\n",
    "    end;\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea6543c-ea5c-4742-88d2-d1101db1084b",
   "metadata": {
    "collapsed": false,
    "language": "sql",
    "name": "resume_graph"
   },
   "outputs": [],
   "source": [
    "-- resume all Tasks of the graph\n",
    "select SYSTEM$TASK_DEPENDENTS_ENABLE('LOAD_RAW_DATA');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93817b1a-d0ac-46f4-9500-5e70b0708ac2",
   "metadata": {
    "collapsed": false,
    "name": "Run_Pipeline"
   },
   "source": [
    "With this dependency setup we are also reducing redundant notifications, as they will only trigger when new quality issues are detected and the percentage of bad rows is still above 1%.\n",
    "\n",
    "Once our Task Graph had a few runs we can now also see the 2 different paths that can occur. \n",
    "Navigate to **Monitoring / Task History** and filter to our DEX_DB/DEMO schema and our LOAD_RAW_DATA root task to see the history of graph runs. \n",
    "\n",
    "We can see they are all successful, as they are handling both cases (quality checks passed or failed).\n",
    "\n",
    "Selecting a run from the History list we will mostly see graphs where the checks passed and data was processed mixed with a few occasional runs that did detect quality issues and isolated the dataset instead.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "831f1d91-a750-4303-a60a-f41f8cc4534f",
   "metadata": {
    "collapsed": false,
    "name": "Make_it_yours"
   },
   "source": [
    "## Now make it yours!\n",
    "\n",
    "While this setup should be generic enough for you to apply to your existing ELT Task graphs there are many opportunities for you to further customize and automate this according to your needs.\n",
    "+ You can start by writing and running your own DMFs. \n",
    "+ You can customize the notifications logic and message content.\n",
    "+ Or you can Automatically process the isolated rows by adding more Tasks to the isolated data branch of the graph that can delete, sanitize or extrapolate data and then merge it back into the clean-data table.\n",
    "+ Or we add a Streamlit App with a data-editor for a data expert to manually review and correct the isolated rows before merging them…\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be23b163-884a-4131-ba1d-c386373058d8",
   "metadata": {
    "collapsed": false,
    "name": "APPENDIX"
   },
   "source": [
    "## Appendix\n",
    "\n",
    "**Official Snowflake documentation:**\n",
    "\n",
    "+ https://docs.snowflake.com/en/user-guide/data-quality-intro\n",
    "+ https://docs.snowflake.com/en/user-guide/tasks-intro\n",
    "+ https://docs.snowflake.com/en/user-guide/tasks-intro#label-tasks-triggered  \n",
    "+ https://docs.snowflake.com/en/sql-reference/functions/system_set_return_value \n",
    "+ https://docs.snowflake.com/en/sql-reference/functions/system_get_predecessor_return_value \n",
    "\n",
    "\n",
    "**Granting required role privileges**\n",
    "\n",
    "+ if you don't want to use the ACCOUNTADMIN role, then create a new role and grant all required privileges for this setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd544fe3-506f-4dbc-b535-c0793192481c",
   "metadata": {
    "collapsed": false,
    "language": "sql",
    "name": "grant_privileges"
   },
   "outputs": [],
   "source": [
    "create role if not exists DEMO_USER;\n",
    "grant role DEMO_USER to user YOUR_USERNAME;         -- insert your username here\n",
    "\n",
    "grant create table on schema DEX_DB.DEMO to role DEMO_USER;\n",
    "grant create stream on schema DEX_DB.DEMO to role DEMO_USER;\n",
    "grant create task on schema DEX_DB.DEMO to role DEMO_USER;\n",
    "grant create function on schema DEX_DB.DEMO to role DEMO_USER;\n",
    "\n",
    "grant usage on warehouse DEX_WH to role DEMO_USER;\n",
    "\n",
    "-- to create notification integrations (optional)\n",
    "grant create integration on account to role DEMO_USER;\n",
    " \n",
    "-- to create and run data metrics functions and see their results\n",
    "grant create data metric function on schema DEX_DB.DEMO to role DEMO_USER;\n",
    "grant execute data metric function on account to role DEMO_USER;\n",
    "grant application role SNOWFLAKE.DATA_QUALITY_MONITORING_VIEWER to role DEMO_USER;\n",
    "\n",
    "use role DEMO_USER;"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
