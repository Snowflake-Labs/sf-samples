{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorized UDTFs: Building Forecasting Models in Parallel\n",
    "\n",
    "This notebook provides an example of building multiple forecasting models in parallel using Vectorized UDTFs. Unlike regular UDTFs, the vectorized variants allows you to operate on batches of rows in one pass, allowing for more efficient processing of the input data. \n",
    "\n",
    "For more details, refer to the documentation: [Vectorized UDTFs](https://docs.snowflake.com/en/developer-guide/udf/python/udf-python-tabular-vectorized). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will be using the UDTF with the vectorized `end_partition` method. This is useful for when you want to: \n",
    "\n",
    "1. Process your data partition-by-partition instead of row-by-row. In our forecasting use-case this makes sense, we do not necessarily want to do and processing against each individual row for a particular sku/country/logical entity, we want to process it in one go. \n",
    "2. You want to return multiple rows or columns for each partition\n",
    "3. Use libraries that operate on pandas DataFrame objects natively. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports: \n",
    "\n",
    "**Important**: Vectorized UDTF's require the Snowpark Library for Python version 1.14.0 or later!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import snowflake.snowpark\n",
    "from snowflake.snowpark.session import Session\n",
    "import snowflake.snowpark.types as T\n",
    "import snowflake.snowpark.functions as F\n",
    "from snowflake.snowpark.functions import col\n",
    "\n",
    "from snowflake.snowpark.functions import udf\n",
    "from snowflake.snowpark.types import IntegerType, FloatType, StringType,StructType, StructField\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "from datetime import date, timedelta\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.16.0\n"
     ]
    }
   ],
   "source": [
    "#check to make sure we have the right version of snowpark > 1.14.0\n",
    "print(snowflake.snowpark.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Connect to Snowflake:  \n",
    "connection_parameters = json.load(open('/Users/hapatel/.config/creds.json'))\n",
    "session = Session.builder.configs(connection_parameters).create()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Set Up: \n",
    "\n",
    "We will be using a dataset called `time_series_1k.csv` which has 1000 series for which we want to create forecasting models over. \n",
    "\n",
    "We will read this data in locally and create a snowflake table for us to work with. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DATE</th>\n",
       "      <th>SERIES_ID</th>\n",
       "      <th>TRAFFIC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-01-02</td>\n",
       "      <td>1</td>\n",
       "      <td>138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-01-03</td>\n",
       "      <td>1</td>\n",
       "      <td>134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-01-04</td>\n",
       "      <td>1</td>\n",
       "      <td>124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-01-05</td>\n",
       "      <td>1</td>\n",
       "      <td>103</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         DATE  SERIES_ID  TRAFFIC\n",
       "0  2018-01-01          1      119\n",
       "1  2018-01-02          1      138\n",
       "2  2018-01-03          1      134\n",
       "3  2018-01-04          1      124\n",
       "4  2018-01-05          1      103"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('time_series_1k.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SERIES_ID\n",
       "1       2046\n",
       "672     2046\n",
       "659     2046\n",
       "660     2046\n",
       "661     2046\n",
       "        ... \n",
       "339     2046\n",
       "340     2046\n",
       "341     2046\n",
       "342     2046\n",
       "1000    2046\n",
       "Name: count, Length: 1000, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Confirm the number of values for each series: \n",
    "df['SERIES_ID'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a total of 1000 unique series_id's, all with 2046 observations each. We will next write the table into Snowflake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.use_database('DEMO')\n",
    "session.use_schema('PUBLIC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.write_pandas(df = df, table_name = 'TIME_SERIES_1K', auto_create_table = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "|\"DATE\"      |\"SERIES_ID\"  |\"TRAFFIC\"  |\n",
      "----------------------------------------\n",
      "|2018-01-01  |1            |119        |\n",
      "|2018-01-02  |1            |138        |\n",
      "|2018-01-03  |1            |134        |\n",
      "|2018-01-04  |1            |124        |\n",
      "|2018-01-05  |1            |103        |\n",
      "|2018-01-06  |1            |88         |\n",
      "|2018-01-07  |1            |93         |\n",
      "|2018-01-08  |1            |116        |\n",
      "|2018-01-09  |1            |141        |\n",
      "|2018-01-10  |1            |147        |\n",
      "----------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Read snowpark dataframe to confirm upload: \n",
    "sdf_raw = session.table('TIME_SERIES_1K')\n",
    "sdf_raw.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType([StructField('DATE', StringType(16777216), nullable=True), StructField('SERIES_ID', LongType(), nullable=True), StructField('TRAFFIC', LongType(), nullable=True)])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sdf_raw.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorized UDTF: \n",
    "\n",
    "In this section we will implement the body of our vectorized UDTF. We will be making use of the library Prophet for forecasting, and build an independent model for each partition. For API reference details, [see here](https://docs.snowflake.com/developer-guide/snowpark/reference/python/latest/snowpark/api/snowflake.snowpark.udtf.UDTFRegistration#snowflake.snowpark.udtf.UDTFRegistration)\n",
    "\n",
    "### Defining the Input/Output Schema: \n",
    "The first step is to define the input/output data-types we expect as a result of executing the function. In this case, we will be returning 6 columns, and will be passing in the 2 columns from our input table (we will be partitioning over the series_id column). Pro Tip: Use the `.schema` function to get back the types of the input (see above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the input/output schema to be used: \n",
    "input_types = [T.StringType(), T.LongType()]\n",
    "\n",
    "output_schema = T.StructType([\n",
    "    T.StructField(\"TIMESTAMP\", T.DateType()), #Date of the observation/forecast\n",
    "    T.StructField(\"FORECAST\", T.FloatType()), #Model output for the forecast\n",
    "    T.StructField(\"TRAIN_START\", T.DateType()), #Date at which the training time period started\n",
    "    T.StructField(\"TRAIN_END\", T.DateType()), #Date at which the training time period ended\n",
    "    T.StructField(\"FORECAST_HORIZON\", T.IntegerType()), #Length in days we are forecasting into the future for\n",
    "    T.StructField(\"LIBRARY_VERSION\", T.StringType()) #Collect metadata for the Prophet library version we are using\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the Function body: \n",
    "\n",
    "Compared to regular UDTFs, we do not have to process each row individually. In our use-case, we will not implement the `process` method that is usually required, and only implement the `end_partition` method. The `end_partition` method will expect a pandas dataframe as an input, and can return either a pandas Dataframe object, or a list of pandas arrays/ pandas series. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class forecast: \n",
    "    def end_partition(self, df: pd.DataFrame) -> pd.DataFrame: #Need to have type annotation to mark as a vectorized UDTF!\n",
    "        \"\"\"Reads in the data for the logical partition, and builds a forecasting model\"\"\"\n",
    "        #Imports: \n",
    "        import prophet\n",
    "\n",
    "        #Pre-process\n",
    "        df['ds'] = pd.to_datetime(df['DATE'])\n",
    "        df = df.groupby('ds').sum('TRAFFIC').reset_index()\n",
    "        df = df.rename(columns = {'TRAFFIC':'y'})\n",
    "        df = df[['ds', 'y']]\n",
    "        df = df.sort_values(by=['ds']).reset_index(drop = True)\n",
    "\n",
    "        #set training parameters: \n",
    "        train_length = 600\n",
    "        forecast_horizon = 30\n",
    "        train_end = max(df['ds'])\n",
    "        train_start = train_end - pd.Timedelta(days = train_length)\n",
    "\n",
    "        #get training data: \n",
    "        df = df.loc[(df['ds'] > train_start) & (df['ds'] <= train_end)]\n",
    "        \n",
    "        #train model and predict: \n",
    "        model = prophet.Prophet()\n",
    "        model.fit(df)\n",
    "        future = model.make_future_dataframe(periods = forecast_horizon)\n",
    "        forecast = model.predict(future)\n",
    "\n",
    "        #post process forecast results\n",
    "        forecast = forecast[['ds','yhat']]\n",
    "        forecast.columns = ['TIMESTAMP','FORECAST']\n",
    "        forecast['TRAIN_START'] = train_start\n",
    "        forecast['TRAIN_END'] = train_end\n",
    "        forecast['FORECAST_HORIZON'] = forecast_horizon\n",
    "        forecast['LIBRARY_VERSION'] = str(prophet.__version__)\n",
    "\n",
    "        yield forecast\n",
    "\n",
    "#Make sure to annotate as vectorized\n",
    "forecast.end_partition._sf_vectorized_input = pd.DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Register the UDTF with Snowflake\n",
    "\n",
    "Having implemented the python handler method that will act on the batch of rows, we will now register this UDTF with Snowflake using the [UDTF Registration Method](https://docs.snowflake.com/developer-guide/snowpark/reference/python/latest/snowpark/api/snowflake.snowpark.udtf.UDTFRegistration.register):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_udtf = session.udtf.register(\n",
    "    handler = forecast, \n",
    "    output_schema = output_schema, \n",
    "    input_types = input_types, \n",
    "    name = 'VECTORIZED_UDTF_PROPHET', \n",
    "    stage_location = \"@DEMO.PUBLIC.TMP_STAGE\", \n",
    "    packages=['pandas==1.5.3','prophet', 'holidays==0.18', 'snowflake-snowpark-python','tqdm'], \n",
    "    replace = True,\n",
    "    is_permanent= True, #We want to persist this\n",
    "    input_names = ['DATE', 'TRAFFIC'] #pass in the column names that you want to make use of in the function body\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comments: \n",
    "1. Pass in the `input_names` argument in the registration call to name the input of the columns that will be used within the python handler. Default will be `ARG1, ARG2, ....`\n",
    "2. Make sure to annotate the function body in the handler method with a type signature of accepting pandas dataframe as an input and output, or calling the function will throw an error. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Call the UDTF on our Data: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10230"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#take sample of our data\n",
    "input_df = sdf_raw.filter(F.col('SERIES_ID').isin([1,2,3,4,5]))\n",
    "input_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Call the UDTF\n",
    "forecast_sdf = input_df.select(F.col('SERIES_ID'),\n",
    "                               forecast_udtf(\"DATE\", \"TRAFFIC\").over(partition_by = ['SERIES_ID']),\n",
    "                               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------------------------------------------------\n",
      "|\"SERIES_ID\"  |\"TIMESTAMP\"  |\"FORECAST\"          |\"TRAIN_START\"  |\"TRAIN_END\"  |\"FORECAST_HORIZON\"  |\"LIBRARY_VERSION\"  |\n",
      "-------------------------------------------------------------------------------------------------------------------------\n",
      "|4            |2021-12-17   |139.35180714549912  |2021-12-16     |2023-08-08   |30                  |1.1.3              |\n",
      "|4            |2021-12-18   |127.18513288834738  |2021-12-16     |2023-08-08   |30                  |1.1.3              |\n",
      "|4            |2021-12-19   |132.0650153488563   |2021-12-16     |2023-08-08   |30                  |1.1.3              |\n",
      "|4            |2021-12-20   |148.9449512200764   |2021-12-16     |2023-08-08   |30                  |1.1.3              |\n",
      "|4            |2021-12-21   |165.7318834669096   |2021-12-16     |2023-08-08   |30                  |1.1.3              |\n",
      "|4            |2021-12-22   |169.93875218452519  |2021-12-16     |2023-08-08   |30                  |1.1.3              |\n",
      "|4            |2021-12-23   |158.2055012867263   |2021-12-16     |2023-08-08   |30                  |1.1.3              |\n",
      "|4            |2021-12-24   |139.6798633257863   |2021-12-16     |2023-08-08   |30                  |1.1.3              |\n",
      "|4            |2021-12-25   |127.51318906864455  |2021-12-16     |2023-08-08   |30                  |1.1.3              |\n",
      "|4            |2021-12-26   |132.3930715291658   |2021-12-16     |2023-08-08   |30                  |1.1.3              |\n",
      "-------------------------------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "forecast_sdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_predictions = sdf_raw.select(F.col('SERIES_ID'),\n",
    "                               forecast_udtf(\"DATE\", \"TRAFFIC\").over(partition_by = ['SERIES_ID']),\n",
    "                               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_predictions.write.save_as_table('PROPHET_VECTORIZED_UDTF', mode = \"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "snowpark-ml-hol",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
