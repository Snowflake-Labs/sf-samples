{
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  },
  "lastEditStatus": {
   "notebookId": "f5auohpv3qix5mft6lle",
   "authorId": "423196307954",
   "authorName": "APRODDUTOOR",
   "authorEmail": "ashwin.proddutoor@snowflake.com",
   "sessionId": "d6f8009d-a8aa-4752-b491-4d867fb3415e",
   "lastEditTime": 1759895467141
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5dbf4073-c0a9-4586-9ea7-9865c60d0f12",
   "metadata": {
    "name": "Intro",
    "collapsed": false
   },
   "source": "# Concurrent Model Training - NEW\n\nThis notebook allows multiple models to be trained at the same time, each model is trained on a separate Snowpark Container Services instance. This can reduce the overall elapsed time when training lots of models byt distributing the training to available compute resources.\n\n## Background\n\nThe ray framework is used to create training tasks, each task is a ML training workflow. A ray cluster is created and then the tasks are submitted to the cluster, compute instances then are assigned a task to execute, waiting tasks queue for the next available compute instance.\n\n## Usage Notes\n\nAlthough this might sound complicated to set up and use, it is really simple within the Snowflake environment.\n\n1. The parameters to change are grouped into a single cell, these define values for your environment\n2. Training results for each model are saved to a Snowflake Stage table\n3. This training example uses AutoGluon as the training package, but other packages can be used\n4. The resulting models can be saved to the model registry and deployed\n\n\nUNSUPPORTED BY SNOWFLAKE - CUSTOMER SUPPORTED ONLY\n\nCopyright (c) 2025 Snowflake Inc. All rights reserved."
  },
  {
   "cell_type": "code",
   "id": "9ab144f3-72c7-4c8a-85dc-e7bb5e41a8e1",
   "metadata": {
    "language": "python",
    "name": "Change_as_needed"
   },
   "outputs": [],
   "source": "# These are the settings that should be reviewed for your environment\n\n# name of snowflake table to use for training.\ntable_name = 'DEMO_BOSTON_HOUSING_GENERATED_DATA_100000'\n\n# target column name that we will train on \nlabel = 'MEDV'\n\n# unique / key column name to drop or [] to indicate no drop columns\ndrop_cols = ['ID']\n\n# list of models in AutoGluon to train\nmodels_to_train = ['NN_TORCH','GBM','CAT','XGB','FASTAI','RF','XT','KNN']\n\n# if training the same model, but with different parameters add the suffix _<number> to the model name\n# make sure the hpo dict specifies the parameters. \n#models_to_train = ['CAT','XGB','CAT_1','XGB_1','FASTAI','RF']\n\n \n# model hyperparameter tuning options\nhpo={'NN_TORCH': {},\n     'GBM': {},\n     'CAT': {'iterations': 10000, 'learning_rate': 0.05, 'random_seed': 0, 'allow_writing_files': False, 'eval_metric': 'Accuracy', 'thread_count': 6},\n     'XGB': {},\n     'CAT_1': {'iterations': 20000, 'learning_rate': 0.07, 'random_seed': 42, },\n     'XGB_1': {'num_estimators': 100,'learning_rate':  0.1,'max_depth': 5},\n     'FASTAI': {},\n     'RF': {},\n     'XT': {},\n     'KNN': {}\n    }\n\n# autogluon training preset:\n# Available Presets: ['best_quality', 'high_quality', 'good_quality', 'medium_quality', \n#    'experimental_quality', 'optimize_for_deployment', 'interpretable', 'ignore_text']\npreset = 'medium_quality'\n\n# max number of seconds to train model\ntime_limit = 3600 * 24\n\n# stage where the training output should be saved\nresult_stage = 'NOTEBOOK_FILES/AutoGluon'\n\n# number of SPCS notebook containers in the cluster (not including this instance )\n# either set the number of workers you would like to run concurrently or have one per model\n#number_of_workers = 3\nnumber_of_workers = len(models_to_train) - 1\n\n# the resources per SPCS notebook container. Set to auto to configured based on the instance family\n# or set to the number of cpus /gpus:\n#number_of_cpus = 6\nnumber_of_cpus = 'auto'\nnumber_of_gpus = 0\n\n# if true then notebook will check on training status and wait until training completes\nwait_until_completed = True",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ce7c24d4-e20c-41a9-ac20-2425515f5c8e",
   "metadata": {
    "language": "python",
    "name": "Pip_install"
   },
   "outputs": [],
   "source": "!pip install autogluon==1.3.1 bokeh==2.0.1 numpy==2.1.3 --quiet",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f4f924f3-994a-4a43-8819-1a0e58c0c26f",
   "metadata": {
    "language": "python",
    "name": "Package_imports"
   },
   "outputs": [],
   "source": "# Import python packages (standard to all container notebooks)\nimport streamlit as st\nimport pandas as pd\n\n# Used to scale the cluster\nfrom snowflake.ml.runtime_cluster import scale_cluster\n\nfrom autogluon.tabular import TabularDataset, TabularPredictor\nfrom autogluon.features.generators import AutoMLPipelineFeatureGenerator\n\n# used to creat train and test datasets\nfrom sklearn.model_selection import train_test_split\n#from sklearn.pipeline import Pipeline\n#from sklearn.compose import ColumnTransformer\n    \n# ray cluster package\nimport ray\n\n# use to setup the environment\nimport os\nimport psutil\nimport shutil\nfrom datetime import datetime",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "3775908f-ca36-4846-8f38-5adca39217f2",
   "metadata": {
    "language": "python",
    "name": "Session_setup"
   },
   "source": "# We can also use Snowpark for our analyses!\nfrom snowflake.snowpark.context import get_active_session\nsession = get_active_session()\n",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "81ac168f-da29-4263-ad91-a507bae05cca",
   "metadata": {
    "language": "python",
    "name": "Get_notebook_name"
   },
   "outputs": [],
   "source": "notebook_name = os.environ.get('OBJECT_NAME', 'NOTEBOOK')",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b49c2534-8bec-44c9-8cd3-b71c4a2de657",
   "metadata": {
    "language": "python",
    "name": "Set_env_vars"
   },
   "outputs": [],
   "source": "os.environ[\"AG_DISTRIBUTED_MODE\"] = \"True\"\nos.environ[\"AG_FORCE_PARALLEL\"] = \"True\"",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b5b860e2-b063-48a7-b3d3-91b9e4a907aa",
   "metadata": {
    "language": "python",
    "name": "Check_for_stage",
    "collapsed": false,
    "codeCollapsed": true
   },
   "outputs": [],
   "source": "try:\n    result = session.sql(\"ls @\"+result_stage).collect()\n    print(f\"Training results will be saved to the Snowflake stage @{result_stage}/<model-name>\")\nexcept Exception as e:\n    print(f\"The Snowflake stage @{result_stage} used to save model training results is not accessable.\")\n    print(f\"{e}\")\n    \n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "12f3c28f-e033-4b17-9441-3128a9c52574",
   "metadata": {
    "language": "python",
    "name": "Check_table",
    "collapsed": false,
    "codeCollapsed": true
   },
   "outputs": [],
   "source": "try:\n    result = session.table(table_name).limit(1)\n    print(f\"The table {table_name} is available.\")\nexcept Exception as e:\n    print(f\"The table {table_name} is not accessable.\")\n    print(f\"{e}\")\n\ntry:\n    result.select(label).collect()\n    print(f\"The column label (target) {label} is available.\")\nexcept Exception as e:\n    print(f\"The column label (target) {label} is not accessable.\")\n    print(f\"{e}\")\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "759489f2-576e-4f38-ab27-80f8304dcb6a",
   "metadata": {
    "language": "python",
    "name": "Check_memory_requirements",
    "collapsed": false,
    "codeCollapsed": true
   },
   "outputs": [],
   "source": "# create an estimated memory size for training. this is a rough guide so that the job does not fail due to memory limits.\n\nrows = session.table(table_name).count()\ncolumns = len(session.table(table_name).columns)\nprint(f\"Rows: {rows}, Columns: {columns}\")\ndataset_size = rows * columns * 8 # Assuming each value is a float64 (8 bytes)\n\nif rows < 100000 and columns < 100:\n    memory_factor = 5\nelse:\n    if rows <= 1000000 and columns <= 1000:\n        memory_factor = 10\n    else:\n        memory_factor = 20\n\ndataset_size = dataset_size * memory_factor / (1024 ** 3)\nprint(f\"The estimated memory size per worker for the data is: {dataset_size:.2f} GB and a total memory requirement: {dataset_size  *(1+number_of_workers):.2f} GB\")\n\nmemory = psutil.virtual_memory().available / (1024 **3)\nprint(f\"Each worker has {memory:.0f} GB available\" )\n\nif dataset_size >= (memory*0.75):\n    print(\"Warning the service available memory might be too small for training, consider changing to a larger compute pool instance family.\")",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "89a3e524-f120-4c91-890b-fff15f37e61d",
   "metadata": {
    "language": "python",
    "name": "Current_resources",
    "collapsed": false,
    "codeCollapsed": true
   },
   "outputs": [],
   "source": "result = session.sql(\"describe service \"+os.environ[\"SNOWFLAKE_SERVICE_NAME\"]).collect()\ncompute_poolname = pd.DataFrame(result).loc[0, \"compute_pool\"]\n\nresult = session.sql(\"describe compute pool \"+compute_poolname).collect()\ncompute_maxnodes = pd.DataFrame(result).loc[0, \"max_nodes\"]\n\nif (number_of_workers > compute_maxnodes ):\n    print(f\"The number of worker instances is larger than the SPCS compute pool {compute_poolname}\")\n    print(f\"Increasing the max_nodes to {number_of_workers} from {compute_maxnodes}\")\n    session.sql(\"alter compute pool \"+compute_poolname+\" set max_nodes = \"+str(number_of_workers)+\";\").collect()\nelse:\n    print(f\"The compute pool {compute_poolname} has enough instances to execute the workers.\")",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ec7569e6-204d-4763-8389-acc392b433d3",
   "metadata": {
    "language": "python",
    "name": "Check_cpu_count",
    "collapsed": false,
    "codeCollapsed": true
   },
   "outputs": [],
   "source": "if type(number_of_gpus) == str:\n    number_of_gpus = -1\n\nif type(number_of_cpus) == str:\n    number_of_cpus = -1\n\n# 2 cores are reserved for internal use\nnotebook_cpus = os.cpu_count()-2\n\nif number_of_cpus == -1:\n    number_of_cpus = notebook_cpus\n    print(f\"CPU resource set to auto, assigning all available CPU's ({number_of_cpus})\")  \nelse:\n    if (number_of_cpus > notebook_cpus):\n        print(f\"The compute pool {compute_poolname} only has {notebook_cpus} CPU's available but the varaiable number_of_cpus is set to {number_of_cpus}.\")\n        print(\"Training will not be able to run. Either restart the notebook on a larger SPCS instance family or reduce the number_of_cpus setting.\")\n    else:\n        print(f\"The compute pool {compute_poolname} has enough CPU ({number_of_cpus})\")\n\n# check for gpus\nncmd = !nvidia-smi --list-gpus\nnotebook_gpus = len(ncmd)\n\nif number_of_gpus == -1:\n    if \"not found\" in ncmd[0]:\n        print(\"No GPU's are available\")\n        number_of_gpus = 0\n    else:\n        number_of_gpus = notebook_gpus\n        print(f\"GPU resource set to auto, assigning all available GPU's ({number_of_gpus})\")  \nelse:\n    if (number_of_gpus > notebook_gpus):\n        print(f\"The compute pool {compute_poolname} only has {notebook_gpus} GPU's available but the varaiable number_of_gpus is set to {number_of_gpus}.\")\n        print(\"Training will not be able to run. Either restart the notebook on a larger SPCS instance family or reduce the number_of_gpus setting.\")\n    else:\n        print(f\"The compute pool {compute_poolname} has enough GPU ({number_of_gpus})\")\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "8de3913c-ff4b-482d-9827-5ed375f3771e",
   "metadata": {
    "language": "python",
    "name": "Scale_cluster",
    "collapsed": false,
    "codeCollapsed": true
   },
   "outputs": [],
   "source": "if (scale_cluster(number_of_workers)) == True:\n    print(f\"Ray cluster is ready with 1 head node and {number_of_workers} worker nodes in compute pool {compute_poolname}\")\nelse:\n    print(f\"Error: Unable to scale the compute pool {compute_poolname} see logs for additional details.\")",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "41fb7adc-f163-424e-a512-14e1fcd51f02",
   "metadata": {
    "language": "python",
    "name": "Define_cluster_runtime"
   },
   "outputs": [],
   "source": "runtime_env = {\"pip\": [\"autogluon==1.3.1\",\"numpy==1.26.4\"], \n               \"log_to_driver\":False,\n               \"env_vars\": {\"AG_DISTRIBUTED_MODE\" :\"True\",\n                            \"AG_FORCE_PARALLEL\":\"True\"\n                           }\n              }",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "26bde000-df4b-43de-aff8-72eddee0cf4e",
   "metadata": {
    "language": "python",
    "name": "maybe_needed"
   },
   "outputs": [],
   "source": "ray.shutdown()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a8bbb11d-b55e-4cd9-8681-75e579bc2106",
   "metadata": {
    "language": "python",
    "name": "Start_cluster"
   },
   "outputs": [],
   "source": "ray_cluster = ray.init(runtime_env=runtime_env )\n!ray list nodes",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "7a766acf-925c-4eb1-a8ac-c7da505f81e4",
   "metadata": {
    "language": "python",
    "name": "Training_definition"
   },
   "outputs": [],
   "source": "# this defines the remote function that will execute on the ray cluster\n\n@ray.remote(scheduling_strategy=\"SPREAD\")\ndef train_model(model, preset, table_name, label, hpo, time_limit, result_stage):\n    \n    # imports that each worker needs in the cluster\n    import shutil\n    import os\n    from datetime import datetime\n    import time\n    import pandas\n    import random\n\n    from snowflake.snowpark import Session\n\n    # record some info for this execution\n    tid = ray.get_runtime_context().get_task_id()\n    ip = ray.util.get_node_ip_address()\n    ts_s = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n    logging.info(f'Queued {ip} Model {model} TS {ts_s} tid {tid} ', flush=True)\n    print(f'Queued {ip} Model {model} TS {ts_s} tid {tid} ', flush=True)\n    print(f'HPO {type(hpo)} {hpo}')\n    print(f'preset {preset}')\n    \n    # read the SPCS token for this session\n    with open('/snowflake/session/token', 'r') as f:\n                token = f.read()\n\n    # set up connection to to Snowflake\n    connection_parameters = {\n        \"host\": os.getenv('SNOWFLAKE_HOST'),\n        \"account\": os.getenv('SNOWFLAKE_ACCOUNT'),\n        \"token\": token,\n        \"authenticator\": 'oauth',\n        \"warehouse\": os.getenv('SNOWFLAKE_WAREHOUSE'),\n        \"database\": os.getenv('SNOWFLAKE_DATABASE'),\n        \"schema\": os.getenv('SNOWFLAKE_SCHEMA')\n    }\n\n    # create session from the ray worker to Snowflake\n    session = Session.builder.configs(connection_parameters).getOrCreate()\n    print(\"Connection to Snowflake sucessful\")\n\n    # retrieve the data from Snowflake\n    data = session.table(table_name).limit(100000).to_pandas()\n    print(\"Retrieved data sucessfully \")\n\n    # identify the target the we will be predicting and remove it from the data used from training (inputs)\n    target = data[label]\n    inputs = data\n    inputs.drop(columns=drop_cols, axis=1, inplace=True)\n\n    # create create a train and test dataset\n    x_train, x_test, y_train, y_test = train_test_split(inputs, target, test_size=0.2, random_state=42)\n    print(\"Split of data sucessful\")\n    \n    # train one model\n    ts_s = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(f'Starting {ip} Model {model} TS {ts_s} tid {tid} ', flush=True)\n    model_path = '/tmp/autogluon/'+model+'/'\n    predictor = TabularPredictor(label=label, path=model_path).fit(x_train, hyperparameters=hpo, presets=preset, time_limit=3600)\n    print(\"Fit finished\")\n\n    m = TabularPredictor.load(\"/tmp/autogluon/\"+model)\n    print(m.predict(data))\n    # , ag_args_fit={\"ag.max_memory_usage_ratio\": 1.5}\n    # verbosity=3\n\n    # show the model results on the training data\n    #predictor.evaluate(x_train)\n    \n    #y_pred = predictor.predict(train_data.drop(columns=[label]))\n    #predictor.evaluate(train_data, silent=True)\n\n    predictor.save(model_path)\n    #predictor.fit_summary()\n\n    # create arhive of the training output\n    print(\"Creating archive\")\n    local_file = '/tmp/'+model\n    shutil.make_archive(local_file, 'zip', model_path )\n    \n    # get the current token as the training step could take sometime to complete\n    with open('/snowflake/session/token', 'r') as f:\n            token = f.read()\n\n    # create session from the ray worker to Snowflake\n    session = Session.builder.configs(connection_parameters).getOrCreate()\n    print(\"upload session created\")\n    \n    # stage location\n    stage_location = \"@\"+os.getenv('SNOWFLAKE_DATABASE')+\".\"+os.getenv('SNOWFLAKE_SCHEMA')+\".\"+result_stage+\"/\"+model+\"/\"\n    \n    try:\n        print(\"uploading artifacts\")\n        session.file.put(local_file+'.zip', stage_location, auto_compress=False, overwrite=True)\n        session.file.put(model_path+'/models/*/model.pkl', stage_location, auto_compress=False, overwrite=True)\n        logging.info(f\"File '{local_file}' successfully uploaded to stage '{stage_location}'.\")\n        print(\"upload sucessful\")\n        shutil.rmtree(model_path)\n        os.remove(local_file+'.zip')\n    except Exception as e:\n        logging.error(f\"Error uploading file: {e}\")\n    finally:\n        if session:\n           session.close()\n    \n  \n    ts_e = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n    logging.info(f'Completed {ip} Model {model} TS {ts_e} tid {tid} ', flush=True)\n    print(f'Completed {ip} Model {model} TS {ts_e} tid {tid} ', flush=True)\n    \n    return \n    ",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "23c7c486-7999-4ba4-9c36-a6d2dace6956",
   "metadata": {
    "language": "python",
    "name": "ray_output_helper"
   },
   "outputs": [],
   "source": "def ray_output(opt, output, fields):\n\n    js = json.loads(output.s)\n    result = ''\n    task_add = False\n    for i, item in enumerate(json.loads(output.s)):\n        for column_name, column_value in item.items():\n            if column_name in fields.split():\n                if opt == 'list':\n                    if fields.index(column_name) == 0:\n                        print()\n                \n                    print(f\"{column_name}: {column_value} \", end=\"\")\n                if opt == 'value':\n                    if len(result) < 2:\n                        result = '{\"'+column_name+'\":\"'+str(column_value)+'\"'\n                    else:\n                         result += ',\"'+column_name+'\":\"'+str(column_value)+'\"'\n                if opt == 'parse':\n                    if column_name == \"task_id\":\n                        task_add = True\n                        if len(result) < 2:\n                            result = '{\"'+str(column_value)+'\"'\n                        else:\n                             result += ',\"'+str(column_value)+'\"' \n                    else:\n                        if task_add:\n                            result += ':{\"'+column_name+'\":\"'+str(column_value)+'\"'\n                            task_add = False\n                        else:\n                             result += ',\"'+column_name+'\":\"'+str(column_value)+'\"' \n        if opt == 'parse':\n            result += '}'\n            \n    result += '}'\n    if opt == 'list':\n        return\n        \n    if opt == 'parse':\n        task = json.loads(result)\n        print(\"{:<50} {:<9} {:<12} {:<14} {:<10} {:<8} {:<10} {:<8}\".format(\"Task id\",\"Status\", \"Model\", \"Service\", \"Start\", \"Time\", \"End\", \"Time\"))\n        for k, value in task.items():\n            \n            print(f\"{k:<50} {value['state']:<9} \" ,end=\"\")\n            \n            os.system('ray logs task --id '+k+' --tail -1 > /tmp/task.txt ')\n            try:\n                with open('/tmp/task.txt', 'r') as f:\n                    content = f.read()\n                    lines = content.splitlines()\n                    for line in lines:\n                       # print(line)\n                        if len(line) >= 8:\n                            words = line.split()\n                            #print(words[0])\n                            if words[0] == \"Starting\":\n                                print(f\"{words[3]:<12} {words[1]:<10} {words[5]:<10} {words[6]:<8} \", end=\"\")\n                            if words[0] == \"Completed\":\n                                print(f\"{words[5]:<10} {words[6]:<10}  \", end=\"\")\n                    print()\n            except Exception as e:\n                print(f\"Cannot find log for task {k}  {e}\")\n\n    else:\n        return result",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "61b1dc1e-30b8-4b3d-9c0d-fd0bd565bf12",
   "metadata": {
    "language": "python",
    "name": "Submit_training",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "print(\"Training starting \"+datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"))\nfor model in models_to_train:\n    suffix = model.split(\"_\",1)\n    if len(suffix) ==2 and suffix[1].isdigit():\n        hpo_model = {suffix[0]:hpo[model]}\n    else:\n        hpo_model = {model:hpo[model]}\n    print(f'Starting for model {model} with hyperparameters {str(hpo_model)}')\n    tid = train_model.options(num_cpus=number_of_cpus, num_gpus=number_of_gpus, name=model, scheduling_strategy=\"SPREAD\").remote(model, preset, table_name, label, hpo_model, time_limit, result_stage) \n    print(tid)\n    print()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "09d97140-f6fb-42c8-a253-bf0ebb55320b",
   "metadata": {
    "language": "python",
    "name": "task_summary",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "!ray summary tasks",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d15f986b-accc-40ca-b766-e1cf39eb8da5",
   "metadata": {
    "language": "python",
    "name": "ray_status"
   },
   "outputs": [],
   "source": "!ray status ",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b51394b8-08a3-4147-90cf-f269dd7183ef",
   "metadata": {
    "language": "python",
    "name": "training_status"
   },
   "outputs": [],
   "source": "output = !ray list tasks --format json \nray_output('list', output, 'task_id node_id name state error_type')",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "8b15b1da-505f-4c73-b9d9-a2d8f6defbed",
   "metadata": {
    "language": "python",
    "name": "Check_Blocking"
   },
   "outputs": [],
   "source": "while wait_until_completed == True:\n    output = !ray list tasks --format json\n    waiting = ray_output('value', output, 'name state')\n    \n    if \"RUNNING\" not in waiting and \"PENDING\" not in waiting:\n        print(\"Training completed \"+datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"))\n        break\n    else:\n        print(\"Pending: \"+str(waiting.count(\"PENDING\"))+\" RUNNING: \"+str(waiting.count(\"RUNNING\"))+\" FAILED: \"+str(waiting.count(\"FAILED\"))+\" FINISHED: \"+str(waiting.count(\"FINISHED\")))\n        time.sleep(60)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f71f3394-a026-4ea4-8173-dd37dc8b45cf",
   "metadata": {
    "language": "python",
    "name": "Show_stage_files"
   },
   "outputs": [],
   "source": "# output files for each model\nsession.sql('ls @'+result_stage)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "848b1a86-45bc-419e-ab9f-d302e163a13a",
   "metadata": {
    "language": "python",
    "name": "Execution_summary"
   },
   "outputs": [],
   "source": "output = !ray list tasks --format json \nray_output('parse', output, 'task_id name state error_type')",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "67f79b4e-8c45-4187-8b08-b1d615f872a6",
   "metadata": {
    "language": "python",
    "name": "training_log",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "# worker output\ntask_progress = !ray logs task --id 75b8161497c41428ffffffffffffffffffffffff07000000 --tail -1  \n\n# training output\ntask_output =   !ray logs task --id 975b61ee4287345bffffffffffffffffffffffff07000000  --err   --tail -1  \n\ntry:\n    if \"Traceback\" in task_progress[0]:\n        print(\"Task id is invalid, check taskid in the Execution_summary cell\")\n    else:\n        print(\"[Task Progress]\")\n        for line in task_progress:\n            print(line)\n    \n    if \"Traceback\" in task_output[0]:\n        print(\"Task id is invalid, check taskid in the Execution_summary cell\")\n    else:\n        print(\"[Task Output]\")\n        for line in task_output:\n            print(line)\nexcept:\n    print(\"No logs was available, check the task status.\")",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "040f44eb-77ab-4e7d-8f82-27a75a364928",
   "metadata": {
    "name": "Next_steps",
    "collapsed": false
   },
   "source": "# Whats Next?\nThe model pickle file is saved into a Snowflake Stage, that was defined in the **Change_as_needed** cell in this Notebook.\n\nThe model can be registered in the Model Registry and then deployed. A Notebook that performs those steps is [available](https://docs.google.com/presentation/u/0/d/1JTFTH2a1RgQnubebpz3_oaHYxS3Irunjvktcep4lowc/edit) refer to the **Model Registry** section of that Notebook.\n\nUsing the trained models, they could also be ensemabled into one model if needed, this would require loading each model into a fit() and saving the resulting model pickle."
  },
  {
   "cell_type": "markdown",
   "id": "658edf35-40be-4a6e-8833-b1986d4bae47",
   "metadata": {
    "name": "Predictions",
    "collapsed": false
   },
   "source": "# Get Predictions\nThe most scalable way to get predictions and persist them into a Snowflake table can be achieved by registering the model, see the Notebook referened in cell *Whats Next*, but you can also execute the Model in a cell, which is handy for testing the model before deployment."
  },
  {
   "cell_type": "code",
   "id": "1bcd931d-491a-42dc-b69d-ccce129a2a68",
   "metadata": {
    "language": "python",
    "name": "Get_trained_model"
   },
   "outputs": [],
   "source": "# we are just doing to use one of the models, but this could be a loop to use them all\nmodel =  models_to_train[models_to_train.index('RF')]\n\ntry:\n    session.file.get(f\"@{result_stage}/{model}/{model}.zip\", f\"/tmp/{model}/\")\n    print(f\"Model {model} downloaded to Notebook\")\nexcept Exception as e:\n        logging.error(f\"Error downloading file: {e}\")",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f37bc446-c3d5-4b35-a44b-3250c97b9a1a",
   "metadata": {
    "language": "python",
    "name": "Prepare_model"
   },
   "outputs": [],
   "source": "# unzip the model files these are the files from the dirstributed trainer\ntry:\n    shutil.unpack_archive(f\"/tmp/{model}/{model}.zip\", f\"/tmp/{model}/\", \"zip\")\n    print(f\"Model {model} file unpacked\")\nexcept Exception as e:\n        logging.error(f\"Error unpacking model file: {e}\")   ",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "bc78fe5d-f19d-44c5-8d01-ac46ef2bd896",
   "metadata": {
    "language": "python",
    "name": "Data_table"
   },
   "outputs": [],
   "source": "data = session.table(table_name).limit(100000).to_pandas()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "afb7244f-7dda-439b-ad7c-51c36bbc56b5",
   "metadata": {
    "language": "python",
    "name": "Get_predictions"
   },
   "outputs": [],
   "source": "# load the model and make the prediction\nm = TabularPredictor.load(f\"/tmp/{model}\")\nm.predict(data)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "70b96bb8-dda5-43c1-8c18-b2ff4b273298",
   "metadata": {
    "language": "python",
    "name": "Fit_summary"
   },
   "outputs": [],
   "source": "m.fit_summary()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ea7cd866-7a61-4d51-a6fc-a98b567cb729",
   "metadata": {
    "language": "python",
    "name": "Model_details",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "# show the details about this model\nm.info()",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "ac8b2fb9-be16-4dd3-bfba-7035606fe142",
   "metadata": {
    "name": "End",
    "collapsed": false
   },
   "source": "# End of Notebook"
  }
 ]
}