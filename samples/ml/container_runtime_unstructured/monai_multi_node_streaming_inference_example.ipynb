{
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  },
  "lastEditStatus": {
   "notebookId": "n3wmxebn3hiuix7aoipt",
   "authorId": "103971156568",
   "authorName": "TEST_ENG_ML",
   "authorEmail": "ml-eng-dev-DL@snowflake.com",
   "sessionId": "ec774ba4-01a1-4e6d-bd0d-e0bc512627b6",
   "lastEditTime": 1742584736412
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90286fb7-3cc9-457f-931e-b0c2a191f3b4",
   "metadata": {
    "name": "cell21",
    "collapsed": false
   },
   "source": "## Scalable pipeline for segment CT images using Container Runtime + MONAI Pretained model"
  },
  {
   "cell_type": "code",
   "id": "3775908f-ca36-4846-8f38-5adca39217f2",
   "metadata": {
    "language": "python",
    "name": "cell1"
   },
   "source": "# Import python packages\nimport streamlit as st\nimport pandas as pd\n\n# We can also use Snowpark for our analyses!\nfrom snowflake.snowpark.context import get_active_session\nsession = get_active_session()\n",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "799492c0-10ea-4c8b-8ff5-b15042428be4",
   "metadata": {
    "name": "cell15",
    "collapsed": false
   },
   "source": "### Setup the cluster"
  },
  {
   "cell_type": "code",
   "id": "a9c09da2-f70c-489d-93e4-f32b0406d93b",
   "metadata": {
    "language": "python",
    "name": "cell6"
   },
   "outputs": [],
   "source": "import ray \n\nray.init(address='auto', ignore_reinit_error=True)\n\n\ndef configure_ray_logger() -> None:\n    #Configure Ray logging\n    ray_logger = logging.getLogger(\"ray\")\n    ray_logger.setLevel(logging.CRITICAL)\n\n    data_logger = logging.getLogger(\"ray.data\")\n    data_logger.setLevel(logging.CRITICAL)\n\n    #Configure root logger\n    logger = logging.getLogger()\n    logger.setLevel(logging.CRITICAL)\n\n    #Configure Ray's data context\n    context = ray.data.DataContext.get_current()\n    context.execution_options.verbose_progress = False\n    context.enable_operator_progress_bars = False\n\nconfigure_ray_logger()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "80e5ddf4-adf9-45da-adf1-b144bbebc266",
   "metadata": {
    "language": "python",
    "name": "cell7"
   },
   "outputs": [],
   "source": "ray.nodes()",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "b956b059-e64c-407a-bae9-d1bdd1fa05c9",
   "metadata": {
    "name": "cell12",
    "collapsed": false
   },
   "source": ""
  },
  {
   "cell_type": "markdown",
   "id": "2c4eccf3-152a-490c-83be-e116f823300d",
   "metadata": {
    "name": "cell16",
    "collapsed": false
   },
   "source": "#### if want to increase num of nodes"
  },
  {
   "cell_type": "code",
   "id": "d86ed126-c7f8-44b1-b70a-ee67523f9a8f",
   "metadata": {
    "language": "python",
    "name": "cell5"
   },
   "outputs": [],
   "source": "\nfrom snowflake.ml.runtime_cluster import scale_cluster\n\n# Example 1: Scale up the cluster\nscale_cluster(\"ANDA_TEST_MULTI_NODE_INSTALL\", 2)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e05d0529-c87d-4d14-8abd-6dac38221f73",
   "metadata": {
    "language": "python",
    "name": "cell3"
   },
   "outputs": [],
   "source": "monai.config.print_config()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a40b317d-b7f3-4021-9f0c-bca35491c44e",
   "metadata": {
    "language": "python",
    "name": "cell13"
   },
   "outputs": [],
   "source": "@ray.remote(num_cpus=0)  # Ensures task does not consume CPU slots\ndef install_deps():\n    try:\n        packages = [\"monai\", \"pytorch-ignite\", \"itk\", \"gdown\", \"torchvision\", \"lmdb\", \"transformers\", \"einops\", \"nibabel\"]\n        \n        # Install dependencies\n        subprocess.run([\"pip\", \"install\"] + packages, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n\n        # Verify installation\n        result = subprocess.run([\"pip\", \"show\", \"monai\"], capture_output=True, text=True, check=True)\n        return f\"✅ Dependencies installed on {ray.util.get_node_ip_address()}:\\n{result.stdout.splitlines()[0]}\"\n    \n    except subprocess.CalledProcessError as e:\n        return f\"❌ Failed on {ray.util.get_node_ip_address()}: {e.stderr if e.stderr else e.stdout}\"\n\n# Get unique node IPs in the cluster\nnodes = {node[\"NodeManagerAddress\"] for node in ray.nodes() if node[\"Alive\"]}\n\n# Install ffmpeg on each unique node\ntasks = [install_deps.options(resources={f\"node:{node}\": 0.01}).remote() for node in nodes]\nresults = ray.get(tasks)\n\n# Print results\nfor res in results:\n    print(res)",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "163b3eb0-dedc-401e-8a03-1d2ec55c2591",
   "metadata": {
    "name": "cell4",
    "collapsed": false
   },
   "source": "#### Define inference operations"
  },
  {
   "cell_type": "code",
   "id": "ada1b3d5-280e-46cb-b4d2-47a7ab9dbe71",
   "metadata": {
    "language": "python",
    "name": "cell14"
   },
   "outputs": [],
   "source": "import monai\nimport pandas as pd\nimport tempfile\n\nclass MonaiInferencer:\n    def __init__(self):\n        self.session = get_active_session()\n        # Download the bundle which includes pretrained-model check points\n        monai.bundle.download(name='spleen_ct_segmentation', bundle_dir='/tmp')\n        bundle_root = \"/tmp/spleen_ct_segmentation/configs\"\n        config_file = f\"{bundle_root}/inference.json\"\n        \n        # Parse MONAI Bundle configuration, initialize MONAI \n        config = monai.bundle.ConfigParser()\n        config.read_config(config_file)\n        self.device = config.get_parsed_content('device')\n        self.network = config.get_parsed_content(\"network\")\n        self.inferer = config.get_parsed_content(\"inferer\")\n        self.preprocessing = config.get_parsed_content(\"preprocessing\")\n        self.postprocessing = config.get_parsed_content(\"postprocessing\")\n        self.checkpointloader = config.get_parsed_content(\"checkpointloader\")\n        self.output_dir = config.get_parsed_content(\"output_dir\")\n\n    def _infer(self, files):\n        # Create data loader\n        dataset = monai.data.Dataset(data=[{\"image\": file} for file in files], transform=self.preprocessing)\n        dataloader = monai.data.DataLoader(dataset, batch_size=1, num_workers=0)\n\n        # Set up evaluator# based on inference.json\n        evaluator = monai.engines.SupervisedEvaluator(\n            device=self.device,\n            val_data_loader=dataloader,\n            network=self.network,\n            inferer=self.inferer,\n            postprocessing=self.postprocessing,\n            amp=True\n        )\n\n        # Run inference\n        evaluator.run()\n\n        # Save output files to Snowflake stage\n        for root, _, files in os.walk(self.output_dir):\n            for file in files:\n                local_path = os.path.join(root, file)\n                stage_path = f\"@ANDA_TEST_STAGE/{file}\"\n                self.session.file.put(local_path, stage_path, overwrite=True)\n\n        # # Collect results\n        # outputs = [output[\"pred\"].cpu().numpy() for output in evaluator.state.output]\n        return\n    \n    def __call__(self, batch: pd.DataFrame) -> pd.DataFrame:\n        temp_files = []\n        try:\n            # Write each binary to a temporary file.\n            for binary_content in batch[\"file_binary\"]:\n                tmp_file = tempfile.NamedTemporaryFile(delete=False, suffix=\".nii.gz\")\n                tmp_file.write(binary_content)\n                tmp_file.close()\n                temp_files.append(tmp_file.name)\n            \n            # Use the temporary file paths for inference.\n            self._infer(temp_files)\n            batch.drop(columns=['file_binary'], inplace=True)\n        finally:\n            # Clean up temporary files.\n            for file_path in temp_files:\n                try:\n                    os.remove(file_path)\n                except OSError:\n                    pass\n        return batch",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "5d197e15-8139-40bf-a8a1-51a86fe66712",
   "metadata": {
    "language": "python",
    "name": "cell17"
   },
   "outputs": [],
   "source": "from snowflake.ml.ray.datasource import SFStageBinaryFileDataSource\n\ndata_source = SFStageBinaryFileDataSource(\n    stage_location=\"ANDA_TEST_STAGE/Task09_Spleen/\",\n    database=session.get_current_database(),\n    schema=session.get_current_schema(),\n    # file_pattern=\"*.nii.gz\",\n)\n\nray_dataset = ray.data.read_datasource(data_source)",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "e75bcc95-e553-4116-a63e-f77a4a074015",
   "metadata": {
    "name": "cell2",
    "collapsed": false
   },
   "source": "### Apply inference operations"
  },
  {
   "cell_type": "code",
   "id": "00dcde06-1bd0-4e6d-9915-dd6f27a88495",
   "metadata": {
    "language": "python",
    "name": "cell23"
   },
   "outputs": [],
   "source": "batch_size=10\n\nprocessed_ds = ray_dataset.map_batches(\n    MonaiInferencer,\n    batch_size=batch_size,\n    batch_format='pandas',\n    concurrency=2,\n    num_gpus=1,\n)",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "81c3ae24-b3d7-48ac-bdb9-976574d16ed2",
   "metadata": {
    "name": "cell19",
    "collapsed": false
   },
   "source": "### Major benefits\n* Only need to worries about core inference logic\n* Can reuse pre-trained MONAI model/bundle\n* Inference is scalable to multi-node cluster for best performance-cost ratio\n* Medical images is streamed to Container Runtime, no requirement on local disk space. "
  },
  {
   "cell_type": "code",
   "id": "fa9f542b-3dde-4604-9cee-0c1a56a18eb6",
   "metadata": {
    "language": "python",
    "name": "cell18"
   },
   "outputs": [],
   "source": "processed_ds.to_pandas()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "cbd72080-5c3c-4c54-aadc-a05535570cb9",
   "metadata": {
    "language": "python",
    "name": "cell20"
   },
   "outputs": [],
   "source": "session.sql(\"GET @ANDA_TEST_S3_STAGE/Task09_Spleen/imagesTr/spleen_47.nii.gz file:///tmp/\").collect()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "4bac418f-c531-46db-b640-98edce95397c",
   "metadata": {
    "language": "python",
    "name": "cell22"
   },
   "outputs": [],
   "source": "session.file.get(\"@ANDA_TEST_S3_STAGE/Task09_Spleen/imagesTr/spleen_47.nii.gz\", target_directory=\".\")",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "cdb39061-5b63-4b55-9f91-da32ad4c764e",
   "metadata": {
    "language": "python",
    "name": "cell8"
   },
   "outputs": [],
   "source": "fs = SFStageFileSystem(\n    db=session.get_current_database(),\n    schema=session.get_current_schema(),\n    stage=\"ANDA_TEST_S3_STAGE\",\n    snowpark_session=get_active_session(),\n)\n# fs._USE_FALLBACK_FILE_ACCESS=True",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d5921db5-1d52-4ace-9a83-b17f4184eab1",
   "metadata": {
    "language": "python",
    "name": "cell11"
   },
   "outputs": [],
   "source": "fs.ls('/') s3://b",
   "execution_count": null
  }
 ]
}