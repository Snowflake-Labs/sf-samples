{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90286fb7-3cc9-457f-931e-b0c2a191f3b4",
   "metadata": {
    "collapsed": false,
    "name": "cell21"
   },
   "source": [
    "## Scalable pipeline for segment CT images using Container Runtime + MONAI Pretained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3775908f-ca36-4846-8f38-5adca39217f2",
   "metadata": {
    "language": "python",
    "name": "cell1"
   },
   "outputs": [],
   "source": [
    "# Import python packages\n",
    "import streamlit as st\n",
    "import pandas as pd\n",
    "\n",
    "# We can also use Snowpark for our analyses!\n",
    "from snowflake.snowpark.context import get_active_session\n",
    "session = get_active_session()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "799492c0-10ea-4c8b-8ff5-b15042428be4",
   "metadata": {
    "collapsed": false,
    "name": "cell15"
   },
   "source": [
    "### Setup the cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c09da2-f70c-489d-93e4-f32b0406d93b",
   "metadata": {
    "language": "python",
    "name": "cell6"
   },
   "outputs": [],
   "source": [
    "import ray \n",
    "\n",
    "# Configure some logging behavior\n",
    "\n",
    "ray.init(address='auto', ignore_reinit_error=True)\n",
    "\n",
    "\n",
    "def configure_ray_logger() -> None:\n",
    "    #Configure Ray logging\n",
    "    ray_logger = logging.getLogger(\"ray\")\n",
    "    ray_logger.setLevel(logging.CRITICAL)\n",
    "\n",
    "    data_logger = logging.getLogger(\"ray.data\")\n",
    "    data_logger.setLevel(logging.CRITICAL)\n",
    "\n",
    "    #Configure root logger\n",
    "    logger = logging.getLogger()\n",
    "    logger.setLevel(logging.CRITICAL)\n",
    "\n",
    "    #Configure Ray's data context\n",
    "    context = ray.data.DataContext.get_current()\n",
    "    context.execution_options.verbose_progress = False\n",
    "    context.enable_operator_progress_bars = False\n",
    "\n",
    "configure_ray_logger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e5ddf4-adf9-45da-adf1-b144bbebc266",
   "metadata": {
    "language": "python",
    "name": "cell7"
   },
   "outputs": [],
   "source": [
    "ray.nodes()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b956b059-e64c-407a-bae9-d1bdd1fa05c9",
   "metadata": {
    "collapsed": false,
    "name": "cell12"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2c4eccf3-152a-490c-83be-e116f823300d",
   "metadata": {
    "collapsed": false,
    "name": "cell16"
   },
   "source": [
    "#### if want to increase num of nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d86ed126-c7f8-44b1-b70a-ee67523f9a8f",
   "metadata": {
    "language": "python",
    "name": "cell5"
   },
   "outputs": [],
   "source": [
    "from snowflake.ml.runtime_cluster import scale_cluster\n",
    "\n",
    "# Example 1: Scale up the cluster\n",
    "scale_cluster(\"ANDA_TEST_MULTI_NODE_INSTALL\", 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f185f5b9",
   "metadata": {},
   "source": [
    "#### Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a40b317d-b7f3-4021-9f0c-bca35491c44e",
   "metadata": {
    "language": "python",
    "name": "cell13"
   },
   "outputs": [],
   "source": [
    "@ray.remote(num_cpus=0)  # Ensures task does not consume CPU slots\n",
    "def install_deps():\n",
    "    try:\n",
    "        import subprocess\n",
    "        packages = [\"monai\", \"pytorch-ignite\", \"itk\", \"gdown\", \"torchvision\", \"lmdb\", \"transformers\", \"einops\", \"nibabel\"]\n",
    "        \n",
    "        # Install dependencies\n",
    "        subprocess.run([\"pip\", \"install\"] + packages, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "\n",
    "        # Verify installation\n",
    "        result = subprocess.run([\"pip\", \"show\", \"monai\"], capture_output=True, text=True, check=True)\n",
    "        return f\"✅ Dependencies installed on {ray.util.get_node_ip_address()}:\\n{result.stdout.splitlines()[0]}\"\n",
    "    \n",
    "    except subprocess.CalledProcessError as e:\n",
    "        return f\"❌ Failed on {ray.util.get_node_ip_address()}: {e.stderr if e.stderr else e.stdout}\"\n",
    "\n",
    "# Get unique node IPs in the cluster\n",
    "nodes = {node[\"NodeManagerAddress\"] for node in ray.nodes() if node[\"Alive\"]}\n",
    "\n",
    "# Install ffmpeg on each unique node\n",
    "tasks = [install_deps.options(resources={f\"node:{node}\": 0.01}).remote() for node in nodes]\n",
    "results = ray.get(tasks)\n",
    "\n",
    "# Print results\n",
    "for res in results:\n",
    "    print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "163b3eb0-dedc-401e-8a03-1d2ec55c2591",
   "metadata": {
    "collapsed": false,
    "name": "cell4"
   },
   "source": [
    "#### Define inference operations\n",
    "\n",
    "The following code is a MonaiInferencer class that is used to perform inference on a batch of images. \n",
    "It first by loading the pretrained model from a MONAI bundle, together with other components for inference, such as preprocessing, postprocessing. \n",
    "It will save the the output file to a Snowflake stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada1b3d5-280e-46cb-b4d2-47a7ab9dbe71",
   "metadata": {
    "language": "python",
    "name": "cell14"
   },
   "outputs": [],
   "source": [
    "import monai\n",
    "import pandas as pd\n",
    "import tempfile\n",
    "import os\n",
    "\n",
    "class MonaiInferencer:\n",
    "    def __init__(self):\n",
    "        self.session = get_active_session()\n",
    "        # Download the bundle which includes pretrained-model check points\n",
    "        monai.bundle.download(name='spleen_ct_segmentation', bundle_dir='/tmp')\n",
    "        bundle_root = \"/tmp/spleen_ct_segmentation/configs\"\n",
    "        config_file = f\"{bundle_root}/inference.json\"\n",
    "        \n",
    "        # Parse MONAI Bundle configuration, initialize MONAI \n",
    "        config = monai.bundle.ConfigParser()\n",
    "        config.read_config(config_file)\n",
    "        self.device = config.get_parsed_content('device')\n",
    "        self.network = config.get_parsed_content(\"network\")\n",
    "        self.inferer = config.get_parsed_content(\"inferer\")\n",
    "        self.preprocessing = config.get_parsed_content(\"preprocessing\")\n",
    "        self.postprocessing = config.get_parsed_content(\"postprocessing\")\n",
    "        self.checkpointloader = config.get_parsed_content(\"checkpointloader\")\n",
    "        self.output_dir = config.get_parsed_content(\"output_dir\")\n",
    "\n",
    "    def _infer(self, files):\n",
    "        # Create data loader\n",
    "        dataset = monai.data.Dataset(data=[{\"image\": file} for file in files], transform=self.preprocessing)\n",
    "        dataloader = monai.data.DataLoader(dataset, batch_size=1, num_workers=0)\n",
    "\n",
    "        # Set up evaluator# based on inference.json\n",
    "        evaluator = monai.engines.SupervisedEvaluator(\n",
    "            device=self.device,\n",
    "            val_data_loader=dataloader,\n",
    "            network=self.network,\n",
    "            inferer=self.inferer,\n",
    "            postprocessing=self.postprocessing,\n",
    "            amp=True\n",
    "        )\n",
    "\n",
    "        # Run inference\n",
    "        evaluator.run()\n",
    "\n",
    "        # Save output files to Snowflake stage\n",
    "        for root, _, files in os.walk(self.output_dir):\n",
    "            for file in files:\n",
    "                local_path = os.path.join(root, file)\n",
    "                stage_path = f\"@ANDA_TEST_STAGE/{file}\"\n",
    "                self.session.file.put(local_path, stage_path, overwrite=True)\n",
    "\n",
    "        return\n",
    "    \n",
    "    def __call__(self, batch: pd.DataFrame) -> pd.DataFrame:\n",
    "        temp_files = []\n",
    "        try:\n",
    "            # Write each binary to a temporary file.\n",
    "            for binary_content in batch[\"file_binary\"]:\n",
    "                tmp_file = tempfile.NamedTemporaryFile(delete=False, suffix=\".nii.gz\")\n",
    "                tmp_file.write(binary_content)\n",
    "                tmp_file.close()\n",
    "                temp_files.append(tmp_file.name)\n",
    "            \n",
    "            # Use the temporary file paths for inference.\n",
    "            self._infer(temp_files)\n",
    "            batch.drop(columns=['file_binary'], inplace=True)\n",
    "        finally:\n",
    "            # Clean up temporary files.\n",
    "            for file_path in temp_files:\n",
    "                try:\n",
    "                    os.remove(file_path)\n",
    "                except OSError:\n",
    "                    pass\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b40759",
   "metadata": {},
   "source": [
    "#### Define Data Source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d197e15-8139-40bf-a8a1-51a86fe66712",
   "metadata": {
    "language": "python",
    "name": "cell17"
   },
   "outputs": [],
   "source": [
    "from snowflake.ml.ray.datasource import SFStageBinaryFileDataSource\n",
    "\n",
    "data_source = SFStageBinaryFileDataSource(\n",
    "    stage_location=\"ANDA_TEST_STAGE/imagesTs_replica2/\",\n",
    "    database=session.get_current_database(),\n",
    "    schema=session.get_current_schema(),\n",
    "    file_pattern=\"*.nii.gz\",\n",
    ")\n",
    "\n",
    "ray_dataset = ray.data.read_datasource(data_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef9c7f9-3699-40ae-bc5f-fbbf99823a96",
   "metadata": {
    "language": "python",
    "name": "cell25"
   },
   "outputs": [],
   "source": [
    "ray_dataset.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e75bcc95-e553-4116-a63e-f77a4a074015",
   "metadata": {
    "collapsed": false,
    "name": "cell2"
   },
   "source": [
    "### Apply inference operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00dcde06-1bd0-4e6d-9915-dd6f27a88495",
   "metadata": {
    "language": "python",
    "name": "cell23"
   },
   "outputs": [],
   "source": [
    "batch_size=50\n",
    "\n",
    "processed_ds = ray_dataset.map_batches(\n",
    "    MonaiInferencer,\n",
    "    batch_size=batch_size,\n",
    "    batch_format='pandas',\n",
    "    concurrency=4,\n",
    "    num_gpus=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c3ae24-b3d7-48ac-bdb9-976574d16ed2",
   "metadata": {
    "collapsed": false,
    "name": "cell19"
   },
   "source": [
    "### Major benefits\n",
    "* Only need to worries about core inference logic\n",
    "* Can reuse pre-trained MONAI model/bundle\n",
    "* Inference is scalable to multi-node cluster for best performance-cost ratio\n",
    "* Medical images is streamed to Container Runtime, no requirement on local disk space. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e03585f3",
   "metadata": {},
   "source": [
    "#### Apply the inference operations to the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa9f542b-3dde-4604-9cee-0c1a56a18eb6",
   "metadata": {
    "language": "python",
    "name": "cell18"
   },
   "outputs": [],
   "source": [
    "processed_ds.to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d55ebee",
   "metadata": {},
   "source": [
    "### Use the Ray Dataset directly with Distributed Pytorch Training API\n",
    "\n",
    "**Note** the following code will be fully available in the release at end of April 2025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee24c1a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from snowflake.ml.data.data_conenctor import DataConnector\n",
    "from snowflake.ml.modeling.distributors.pytorch import PyTorchDistributor\n",
    "from snowflake.ml.modeling.distributors.pytorch import PyTorchDistributor, PyTorchScalingConfig, WorkerResourceConfig\n",
    "from snowflake.ml.data.sharded_data_connector import DataConnector, ShardedDataConnector\n",
    "from snowflake.ml.modeling.distributors.pytorch import get_context\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from monai.losses import DiceLoss\n",
    "from monai.optimizers import Novograd\n",
    "\n",
    "import torch\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "\n",
    "def train_func():\n",
    "    # get RaySGD context\n",
    "    context = get_context()\n",
    "    rank = context.get_rank()\n",
    "    local_rank = context.get_local_rank()\n",
    "    world_size = context.get_world_size()\n",
    "    num_epochs = 5\n",
    "    batch_size = 30\n",
    "    lr = 1e-3\n",
    "    # init NCCL\n",
    "    dist.init_process_group(\n",
    "        backend=\"nccl\",\n",
    "        init_method=\"env://\",\n",
    "        rank=rank,\n",
    "        world_size=world_size\n",
    "    )\n",
    "    torch.cuda.set_device(local_rank)\n",
    "    device = torch.device(f\"cuda:{local_rank}\")\n",
    "\n",
    "    # grab your Ray dataset shard\n",
    "    train_ds = context.get_dataset_map()['train'].get_shard().to_torch_dataset()\n",
    "\n",
    "    # build model, loss, optimizer and definds your own pytorch model\n",
    "    model = MyPytorchModel().to(device)\n",
    "    model = DDP(model, device_ids=[local_rank])\n",
    "    loss_fn = DiceLoss()\n",
    "    optimizer = Novograd(model.parameters(), lr=lr)\n",
    "    data_loader = DataLoader(\n",
    "        train_ds,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        batch_cnt = 0\n",
    "\n",
    "        # this will rewind automatically each epoch\n",
    "        for batch in data_loader:\n",
    "            optimizer.zero_grad()\n",
    "            # stack all non‐target cols, move to GPU\n",
    "            features = torch.stack(\n",
    "                [batch[k] for k in batch if k != \"target\"],\n",
    "                dim=1\n",
    "            ).to(device)\n",
    "            targets = batch[\"target\"].unsqueeze(1).to(device)\n",
    "\n",
    "            preds = model(features)\n",
    "            loss = loss_fn(preds, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            batch_cnt += 1\n",
    "\n",
    "        # only let rank 0 print\n",
    "        if rank == 0:\n",
    "            avg_loss = total_loss / batch_cnt\n",
    "            print(f\"[Epoch {epoch+1}/{num_epochs}] loss: {avg_loss:.4f}\")\n",
    "\n",
    "\n",
    "train_data_connector = DataConnector.from_ray_ds(ray_dataset)\n",
    "\n",
    "# Create pytorch distributor.\n",
    "pytorch_trainer = PyTorchDistributor(  \n",
    "    train_func=train_func,\n",
    "    scaling_config=PyTorchScalingConfig(  \n",
    "        num_nodes=4,  \n",
    "        num_workers_per_node=1,  \n",
    "        resource_requirements_per_worker=WorkerResourceConfig(num_cpus=6, num_gpus=1),  \n",
    "    )  \n",
    ") \n",
    "\n",
    "pytorch_trainer.run(\n",
    "    dataset_map={'train': train_data_connector}\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  },
  "lastEditStatus": {
   "authorEmail": "ml-eng-dev-DL@snowflake.com",
   "authorId": "103971156568",
   "authorName": "TEST_ENG_ML",
   "lastEditTime": 1744935380243,
   "notebookId": "n3wmxebn3hiuix7aoipt",
   "sessionId": "788abe48-d09e-4214-8183-0b09e9b0d6d5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
