{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Sentence Transformer Benchmark\n",
        "\n",
        "This notebook benchmarks a deployed Sentence Transformer service by sending concurrent requests and measuring latency statistics.\n",
        "\n",
        "## Prerequisites\n",
        "\n",
        "- A Sentence Transformer service must be deployed and accessible\n",
        "- A valid PAT (Personal Access Token) for authentication\n",
        "\n",
        "## Running the Notebook\n",
        "\n",
        "1. **Update the service configuration**:\n",
        "   - Set `pat_token` to your Snowflake PAT token\n",
        "   - Set `URL` to your deployed service endpoint\n",
        "2. **Modify the benchmark configs** if needed (clients, requests per client, duration)\n",
        "3. **Run the cells in order** to execute the benchmarks\n",
        "\n",
        "## Benchmark Configurations\n",
        "\n",
        "Test configs are specified as a list of dictionaries with the following fields:\n",
        "- `clients`: Number of concurrent clients\n",
        "- `requests`: Number of sentences per request\n",
        "- `duration_seconds`: How long to run the test\n",
        "\n",
        "Example:\n",
        "```python\n",
        "configs = [\n",
        "  {\"clients\": 10, \"requests\": 1, \"duration_seconds\": 20},\n",
        "  {\"clients\": 50, \"requests\": 100, \"duration_seconds\": 20},\n",
        "]\n",
        "```\n",
        "\n",
        "Each configuration runs for the specified duration and reports latency statistics (min, median, max, p90, p95, p99).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "%pip install requests pandas aiohttp nest-asyncio huggingface-hub\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load and display sample of the sentences file\n",
        "dataset_path = \"hf://datasets/datastax/philosopher-quotes/philosopher-quotes.csv\"\n",
        "\n",
        "df = pd.read_csv(dataset_path)\n",
        "df.head(10)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Imports\n",
        "import pandas as pd\n",
        "import requests\n",
        "import os\n",
        "from datetime import datetime\n",
        "import json\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()\n",
        "import numpy as np\n",
        "import asyncio\n",
        "import time\n",
        "import aiohttp\n",
        "import statistics\n",
        "\n",
        "print(\"Imports loaded\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load sentence data from CSV\n",
        "def load_sentence_data():\n",
        "    \"\"\"Load sentences from sentences.csv\"\"\"\n",
        "    df = pd.read_csv(dataset_path)\n",
        "    sentences = df[\"quote\"].tolist()\n",
        "    return sentences\n",
        "\n",
        "# Load sentence data\n",
        "sentences = load_sentence_data()\n",
        "print(f\"Loaded {len(sentences)} sentences from CSV\")\n",
        "print(f\"Sample sentences: {sentences[:3]}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Sentence Transformer service configuration\n",
        "pat_token = \"\"  # TODO: Set your Snowflake PAT here\n",
        "SERVICE_ENDPOINT = \"\"  # TODO: Set your service endpoint (e.g., abc123.snowflakecomputing.app)\n",
        "headers = {'Authorization': f'Snowflake Token=\"{pat_token}\"', 'Content-Type': 'application/json'}\n",
        "URL = f\"https://{SERVICE_ENDPOINT}/encode\"\n",
        "\n",
        "print(f\"Sentence Transformer Service Configuration:\")\n",
        "print(f\"  URL: {URL}\")\n",
        "print(f\"  Headers: Authorization with Snowflake Token\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Sentence Transformer invoke_endpoint function\n",
        "async def invoke_endpoint(session, requests):\n",
        "    \"\"\"Call Sentence Transformer service with the specified number of sentences\"\"\"\n",
        "    data_array = []\n",
        "    for i in range(0, requests):\n",
        "        sentence = sentences[i % len(sentences)]\n",
        "        sentence_row = [i, sentence]  # [index, input_feature_0]\n",
        "        data_array.append(sentence_row)\n",
        "\n",
        "    client_data = {\"data\": data_array}\n",
        "\n",
        "    # Start timing after data preparation\n",
        "    start_time = time.monotonic()\n",
        "    try:\n",
        "        async with session.post(URL, headers=headers, json=client_data, timeout=300) as response:\n",
        "            # Read response content\n",
        "            resp = await response.text()\n",
        "\n",
        "            # Only measure successful requests with valid responses\n",
        "            if response.status == 200:\n",
        "                try:\n",
        "                    resp_data = json.loads(resp)\n",
        "                    if \"data\" in resp_data and isinstance(resp_data[\"data\"], list):\n",
        "                        # Calculate latency only for successful, valid responses\n",
        "                        latency = (time.monotonic() - start_time) * 1000\n",
        "                        latencies.append(latency)\n",
        "                    else:\n",
        "                        print(f\"Invalid response structure: {resp[:200]}\")\n",
        "                except json.JSONDecodeError:\n",
        "                    print(f\"Invalid JSON response: {resp[:200]}\")\n",
        "            else:\n",
        "                print(f\"Error response (HTTP {response.status}): {resp[:200]}\")\n",
        "\n",
        "            http_status[response.status] = http_status.get(response.status, 0) + 1\n",
        "            return resp\n",
        "    except Exception as e:\n",
        "        print(f\"Request failed: {str(e)}\")\n",
        "        http_status[500] = http_status.get(500, 0) + 1\n",
        "        return str(e)\n",
        "\n",
        "print(\"Sentence Transformer invoke_endpoint function loaded with accurate latency measurement\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Main benchmark function\n",
        "async def main(num_clients, num_requests, print_results=False, show_responses=False):\n",
        "    async with aiohttp.ClientSession() as session:\n",
        "        tasks = [invoke_endpoint(session, num_requests) for i in range(num_clients)]\n",
        "        responses = await asyncio.gather(*tasks)\n",
        "\n",
        "        if show_responses and print_results:\n",
        "            print(f'Responses from {num_clients} clients:')\n",
        "            for i, response in enumerate(responses):\n",
        "                print(f'Response {i + 1}: {response}')\n",
        "\n",
        "        # Calculate latency statistics\n",
        "        latencies.sort()\n",
        "        min_latency = latencies[0]\n",
        "        max_latency = latencies[-1]\n",
        "        median_latency = latencies[num_clients // 2] if num_clients > 0 else 0\n",
        "        p90_latency = latencies[int(0.90 * num_clients)] if num_clients >= 10 else latencies[-1]\n",
        "        p95_latency = latencies[int(0.95 * num_clients)] if num_clients >= 20 else latencies[-1]\n",
        "        p99_latency = latencies[int(0.99 * num_clients)] if num_clients >= 100 else latencies[-1]\n",
        "\n",
        "        if print_results:\n",
        "            print(f'\\nSentence Transformer Benchmark Results:')\n",
        "            print(f'  Clients: {num_clients}')\n",
        "            print(f'  Sentences per client: {num_requests}')\n",
        "            print(f'  Total embeddings: {num_clients * num_requests}')\n",
        "            print(f'\\nLatency Statistics:')\n",
        "            print(f'  Min: {min_latency:.2f} ms')\n",
        "            print(f'  Max: {max_latency:.2f} ms')\n",
        "            print(f'  Median: {median_latency:.2f} ms')\n",
        "            print(f'  P90: {p90_latency:.2f} ms')\n",
        "            print(f'  P95: {p95_latency:.2f} ms')\n",
        "            print(f'  P99: {p99_latency:.2f} ms')\n",
        "\n",
        "            print(f'\\nHTTP Status Summary:')\n",
        "            for status, count in http_status.items():\n",
        "                if count > 0:\n",
        "                    print(f'  HTTP {status}: {count}')\n",
        "\n",
        "            # Calculate throughput\n",
        "            total_time = max_latency / 1000  # Convert to seconds\n",
        "            if total_time > 0:\n",
        "                throughput = (num_clients * num_requests) / total_time\n",
        "                print(f'\\nThroughput: {throughput:.2f} requests/second')\n",
        "\n",
        "print(\"Main benchmark function loaded\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Reset function\n",
        "def reset_test_state():\n",
        "    \"\"\"Reset global state for next test iteration\"\"\"\n",
        "    global latencies, http_status\n",
        "    latencies.clear()\n",
        "    http_status[200] = 0\n",
        "    http_status[400] = 0\n",
        "    http_status[500] = 0\n",
        "    http_status[429] = 0\n",
        "\n",
        "print(\"Reset function loaded\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Simple single request test - list of lists format\n",
        "def test_single_request():\n",
        "    \"\"\"Test a single request with the correct list of lists format\"\"\"\n",
        "    # Use the correct format: [index, sentence]\n",
        "    sample_sentence = sentences[0]  # Get first sentence\n",
        "    sample_row = [0, sample_sentence]  # [index, sentence_text]\n",
        "    # Sentence Transformer service expects: {\"data\": [[0, \"sentence1\"], [1, \"sentence2\"], ...]}\n",
        "    test_payload = {\"data\": [sample_row]}\n",
        "\n",
        "    print(\"Testing single Sentence Transformer request (list of lists format):\")\n",
        "    print(f\"  URL: {URL}\")\n",
        "    print(f\"  Format: [index, sentence_text]\")\n",
        "    print(f\"  Sample row: [0, \\\"{sample_sentence[:50]}...\\\"]\")\n",
        "    print(f\"  Total elements: {len(sample_row)} (1 index + 1 sentence)\")\n",
        "\n",
        "    try:\n",
        "        response = requests.post(URL, headers=headers, json=test_payload, timeout=300)\n",
        "        print(f\"\\\\nResponse:\")\n",
        "        print(f\"  Status: {response.status_code}\")\n",
        "        print(f\"  Response: {response.text}\")\n",
        "        return response.status_code == 200\n",
        "    except Exception as e:\n",
        "        print(f\"Request failed: {e}\")\n",
        "        return False\n",
        "\n",
        "print(\"Single request test function loaded (list of lists format)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test single request first\n",
        "print(\"Testing single request first...\")\n",
        "single_test_success = test_single_request()\n",
        "\n",
        "if single_test_success:\n",
        "    print(\"\\\\nSingle request successful! Proceeding with full benchmark...\")\n",
        "else:\n",
        "    print(\"\\\\nSingle request failed. Fix the issue before running full benchmark.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Benchmark helpers\n",
        "latencies = []\n",
        "http_status = {200: 0, 400: 0, 500: 0, 429: 0}\n",
        "\n",
        "def _compute_run_stats(lat_list):\n",
        "    if not lat_list:\n",
        "        return {\"min_ms\": 0.0, \"median_ms\": 0.0, \"max_ms\": 0.0, \"p90_ms\": 0.0, \"p95_ms\": 0.0, \"p99_ms\": 0.0, \"count\": 0}\n",
        "    l = sorted(lat_list); n = len(l)\n",
        "    p = lambda q: l[min(int(q * (n - 1)), n - 1)]\n",
        "    return {\n",
        "        \"min_ms\": l[0],\n",
        "        \"median_ms\": statistics.median(l),\n",
        "        \"max_ms\": l[-1],\n",
        "        \"p90_ms\": p(0.90), \"p95_ms\": p(0.95), \"p99_ms\": p(0.99),\n",
        "        \"count\": n,\n",
        "    }\n",
        "\n",
        "async def run_once_async(num_clients, num_requests, print_results=False, show_responses=False):\n",
        "    reset_test_state()\n",
        "    await main(num_clients, num_requests, print_results=print_results, show_responses=show_responses)\n",
        "    s = _compute_run_stats(latencies)\n",
        "    s[\"http\"] = dict(http_status)\n",
        "    return s\n",
        "\n",
        "async def run_for_duration_async(clients: int, requests_per_client: int, duration_seconds: int, \n",
        "                                  show_progress: bool = True, print_results: bool = False, show_responses: bool = False):\n",
        "    end = time.time() + max(1, int(duration_seconds))\n",
        "    runs, pure_time, i = [], 0.0, 0\n",
        "    while time.time() < end:\n",
        "        t0 = time.time()\n",
        "        res = await run_once_async(clients, requests_per_client, print_results=print_results, show_responses=show_responses)\n",
        "        pure_time += (time.time() - t0)\n",
        "        runs.append(res); i += 1\n",
        "        if show_progress:\n",
        "            print(f\"  run {i}: med={res['median_ms']:.1f}ms min={res['min_ms']:.1f}ms max={res['max_ms']:.1f}ms http={res['http']}\")\n",
        "    if not runs:\n",
        "        return {\"runs\": 0, \"aggregated\": {}, \"runs_detail\": []}\n",
        "\n",
        "    meds = [r[\"median_ms\"] for r in runs]\n",
        "    mins = [r[\"min_ms\"] for r in runs]\n",
        "    maxs = [r[\"max_ms\"] for r in runs]\n",
        "    p90s = [r[\"p90_ms\"] for r in runs]; p95s = [r[\"p95_ms\"] for r in runs]; p99s = [r[\"p99_ms\"] for r in runs]\n",
        "\n",
        "    http_agg = {}\n",
        "    for r in runs:\n",
        "        for code, cnt in r.get(\"http\", {}).items():\n",
        "            http_agg[code] = http_agg.get(code, 0) + cnt\n",
        "\n",
        "    aggregated = {\n",
        "        \"runs\": len(runs), \"duration_sec\": round(pure_time, 3),\n",
        "        \"median_of_medians_ms\": statistics.median(meds), \"avg_median_ms\": statistics.fmean(meds),\n",
        "        \"min_of_mins_ms\": min(mins), \"max_of_maxs_ms\": max(maxs),\n",
        "        \"avg_p90_ms\": statistics.fmean(p90s), \"avg_p95_ms\": statistics.fmean(p95s), \"avg_p99_ms\": statistics.fmean(p99s),\n",
        "        \"http\": http_agg,\n",
        "    }\n",
        "    return {\"runs\": len(runs), \"aggregated\": aggregated, \"runs_detail\": runs}\n",
        "\n",
        "async def run_config_queue_async(configs, print_results=False, show_responses=False):\n",
        "    results = []\n",
        "    for cfg in configs:\n",
        "        clients = cfg.get(\"clients\", 10)\n",
        "        reqs = cfg.get(\"requests\", 1)\n",
        "        duration = cfg.get(\"duration_seconds\")\n",
        "        print(f\"\\\\nConfig: {clients} clients x {reqs} requests\" + (f\" for {duration}s\" if duration else \"\"))\n",
        "        if duration:\n",
        "            summary = await run_for_duration_async(clients, reqs, duration, show_progress=False, \n",
        "                                                   print_results=print_results, show_responses=show_responses)\n",
        "            agg = summary[\"aggregated\"]\n",
        "            print(f\"  -> runs={agg['runs']} dur={agg['duration_sec']}s med_of_meds={agg['median_of_medians_ms']:.1f}ms avg_med={agg['avg_median_ms']:.1f}ms\")\n",
        "            results.append({\"config\": {\"clients\": clients, \"requests\": reqs, \"duration_seconds\": duration}, **summary})\n",
        "        else:\n",
        "            res = await run_once_async(clients, reqs, print_results=print_results, show_responses=show_responses)\n",
        "            print(\"  ->\", {k: (round(v, 2) if isinstance(v, (int, float)) else v) for k, v in res.items() if k != \"http\"})\n",
        "            results.append({\"config\": {\"clients\": clients, \"requests\": reqs}, \"aggregated\": res, \"runs\": 1, \"runs_detail\": [res]})\n",
        "    return results\n",
        "\n",
        "# Loop-aware sync wrapper for scripts\n",
        "def run_config_queue(configs, print_results=False, show_responses=False):\n",
        "    try:\n",
        "        loop = asyncio.get_running_loop()\n",
        "        if loop.is_running():\n",
        "            print(\"Notebook loop is running. Use:\\\\n  await run_config_queue_async(configs)\")\n",
        "            return None\n",
        "    except RuntimeError:\n",
        "        pass\n",
        "    return asyncio.run(run_config_queue_async(configs, print_results=print_results, show_responses=show_responses))\n",
        "\n",
        "print(\"Benchmark helpers ready\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Single sentence benchmark\n",
        "configs = [\n",
        "  {\"clients\": 1, \"requests\": 1, \"duration_seconds\": 20},\n",
        "  {\"clients\": 5, \"requests\": 1, \"duration_seconds\": 20},\n",
        "  {\"clients\": 10, \"requests\": 1, \"duration_seconds\": 20},\n",
        "  {\"clients\": 20, \"requests\": 1, \"duration_seconds\": 20},\n",
        "  {\"clients\": 30, \"requests\": 1, \"duration_seconds\": 20},\n",
        "  {\"clients\": 35, \"requests\": 1, \"duration_seconds\": 20},\n",
        "  {\"clients\": 40, \"requests\": 1, \"duration_seconds\": 20},\n",
        "  {\"clients\": 45, \"requests\": 1, \"duration_seconds\": 20},\n",
        "  {\"clients\": 50, \"requests\": 1, \"duration_seconds\": 20},\n",
        "  {\"clients\": 100, \"requests\": 1, \"duration_seconds\": 20},\n",
        "  {\"clients\": 200, \"requests\": 1, \"duration_seconds\": 20}\n",
        "]\n",
        "results = await run_config_queue_async(configs, print_results=False, show_responses=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Batch benchmark (larger request sizes)\n",
        "configs_large_batches = [\n",
        "  {\"clients\": 1, \"requests\": 100, \"duration_seconds\": 20},\n",
        "  {\"clients\": 5, \"requests\": 100, \"duration_seconds\": 20},\n",
        "  {\"clients\": 10, \"requests\": 100, \"duration_seconds\": 20},\n",
        "  {\"clients\": 20, \"requests\": 100, \"duration_seconds\": 20},\n",
        "  {\"clients\": 30, \"requests\": 100, \"duration_seconds\": 20},\n",
        "  {\"clients\": 35, \"requests\": 100, \"duration_seconds\": 20},\n",
        "  {\"clients\": 40, \"requests\": 100, \"duration_seconds\": 20},\n",
        "  {\"clients\": 45, \"requests\": 100, \"duration_seconds\": 20},\n",
        "  {\"clients\": 50, \"requests\": 100, \"duration_seconds\": 20},\n",
        "  {\"clients\": 100, \"requests\": 100, \"duration_seconds\": 20},\n",
        "  {\"clients\": 200, \"requests\": 100, \"duration_seconds\": 20}\n",
        "]\n",
        "results_large = await run_config_queue_async(configs_large_batches, print_results=False, show_responses=False)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "snowml-env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
