{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "5c4602bf",
      "metadata": {},
      "source": [
        "# Sentence Transformer Batch Inference Quickstart\n",
        "\n",
        "This notebook demonstrates how to perform batch inference using a Sentence Transformer model in Snowflake. It walks through the complete workflow from model deployment to generating embeddings for text data.\n",
        "\n",
        "## Overview\n",
        "\n",
        "Sentence Transformers are models that convert text into dense vector embeddings, useful for semantic search, clustering, and similarity comparisons. This quickstart shows how to:\n",
        "\n",
        "1. Set up Snowflake resources (database, schema, stage, compute pool)\n",
        "2. Load and log a pre-trained Sentence Transformer model\n",
        "3. Create an input dataset with text sentences\n",
        "4. Run batch inference to generate embeddings\n",
        "5. Inspect and output and load it into a table\n",
        "6. Clean up resources\n",
        "\n",
        "## Prerequisites\n",
        "\n",
        "- Snowflake account with appropriate privileges\n",
        "- `snowflake-ml-python>=1.26.0` (for batch inference support)\n",
        "- `sentence-transformers==5.1.1`\n",
        "- `numpy==1.26.4`\n",
        "- A valid Snowflake connection configuration\n",
        "- (Optional) A Snowflake stage and a compute pool. \n",
        "- (Optional) A Sentence Transformer model logged in Snowflake registry.\n",
        "- (Optional) A test dataset.\n",
        "\n",
        "## Running the Notebook\n",
        "\n",
        "Run the cells in order to run the quickstart guide. Depending on whether the resouces have been created and the model has been logged, you need to skip some of the cells"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b685e155",
      "metadata": {},
      "source": [
        "# Install the Requeried Dependency"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1399422b",
      "metadata": {},
      "outputs": [],
      "source": [
        "# uncomment to install the packages\n",
        "# restart the session after installing the packages\n",
        "\n",
        "\n",
        "# ! pip install sentence-transformers==5.1.1 --upgrade\n",
        "# ! pip install numpy==1.26.4 --upgrade"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b7317628",
      "metadata": {},
      "source": [
        "# Make Sure the snowflake-ml-python has the Right Version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "b073c3cf",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1.26.0\n"
          ]
        }
      ],
      "source": [
        "from importlib.metadata import version\n",
        "# batch inference PuPr in snowflake-ml-python>=1.26.0\n",
        "print(version('snowflake-ml-python'))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6948b345",
      "metadata": {},
      "source": [
        "# Establish a Connection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "689ccc32",
      "metadata": {},
      "outputs": [],
      "source": [
        "from snowflake.snowpark import Session\n",
        "session = Session.builder.config(\"connection_name\", \"preprod8\").create()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "11d7e40d",
      "metadata": {},
      "source": [
        "# Create Resources\n",
        "\n",
        "If you already have a stage a and compute pool, please fill in the DB_NAME, SCHEMA_NAME, STAGE_NAME, and COMPUTE_POOL_NAME and skip the following cell.\n",
        "\n",
        "Otherwise, please leave them as they are and run the cells to create the resources."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "74a0c5b4",
      "metadata": {},
      "outputs": [],
      "source": [
        "DB_NAME = \"BATCH_INFERENCE_QUICKSTART_SENTENCE_TRANSFORMER_DB\"\n",
        "SCHEMA_NAME = \"PUBLIC\"\n",
        "STAGE_NAME = \"BATCH_INFERENCE_QUICKSTART_STAGE\"\n",
        "COMPUTE_POOL_NAME = \"BATCH_INFERENCE_QUICKSTART_SENTENCE_TRANSFORMER_COMPUTE_POOL\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "a2ddbb57",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Created database: BATCH_INFERENCE_QUICKSTART_SENTENCE_TRANSFORMER_DB\n",
            "Created schema: BATCH_INFERENCE_QUICKSTART_SENTENCE_TRANSFORMER_DB.PUBLIC\n",
            "Created stage: BATCH_INFERENCE_QUICKSTART_SENTENCE_TRANSFORMER_DB.PUBLIC.BATCH_INFERENCE_QUICKSTART_STAGE\n",
            "Created compute pool: BATCH_INFERENCE_QUICKSTART_SENTENCE_TRANSFORMER_COMPUTE_POOL\n"
          ]
        }
      ],
      "source": [
        "# Create database\n",
        "session.sql(f\"CREATE DATABASE IF NOT EXISTS {DB_NAME}\").collect()\n",
        "\n",
        "# Create schema\n",
        "session.sql(f\"CREATE SCHEMA IF NOT EXISTS {DB_NAME}.{SCHEMA_NAME}\").collect()\n",
        "\n",
        "# Create stage with SSE encryption\n",
        "session.sql(f\"\"\"\n",
        "    CREATE STAGE IF NOT EXISTS {DB_NAME}.{SCHEMA_NAME}.{STAGE_NAME}\n",
        "    ENCRYPTION = (TYPE = 'SNOWFLAKE_SSE')\n",
        "\"\"\").collect()\n",
        "\n",
        "# Create compute pool with smallest CPU tier\n",
        "# use GPU_NV_S for GPU workloads\n",
        "session.sql(f\"\"\"\n",
        "    CREATE COMPUTE POOL IF NOT EXISTS {COMPUTE_POOL_NAME}\n",
        "    MIN_NODES = 1\n",
        "    MAX_NODES = 2\n",
        "    INSTANCE_FAMILY = CPU_X64_XS\n",
        "\"\"\").collect()\n",
        "\n",
        "# Set the session to use the newly created database and schema\n",
        "session.use_database(DB_NAME)\n",
        "session.use_schema(SCHEMA_NAME)\n",
        "\n",
        "print(f\"Created database: {DB_NAME}\")\n",
        "print(f\"Created schema: {DB_NAME}.{SCHEMA_NAME}\")\n",
        "print(f\"Created stage: {DB_NAME}.{SCHEMA_NAME}.{STAGE_NAME}\")\n",
        "print(f\"Created compute pool: {COMPUTE_POOL_NAME}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "45bf63e1",
      "metadata": {},
      "source": [
        "### If you don't already have a model logged, load and log the SentenceTransformer model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "748776db",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/envs/env_8_8/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Logging model: validating model and dependencies...:   0%|          | 0/6 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/envs/env_8_8/lib/python3.12/site-packages/snowflake/ml/registry/_manager/model_parameter_reconciler.py:155: UserWarning: Models logged specifying `pip_requirements` cannot be executed in a Snowflake Warehouse without specifying `artifact_repository_map`. This model can be run in Snowpark Container Services. See https://docs.snowflake.com/en/developer-guide/snowflake-ml/model-registry/container.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Logging model: creating model manifest...:  33%|███▎      | 2/6 [00:33<01:07, 16.79s/it]  "
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/envs/env_8_8/lib/python3.12/site-packages/snowflake/ml/model/_packager/model_env/model_env.py:149: UserWarning: Dependencies specified from pip requirements. This may prevent model deploying to Snowflake Warehouse. Use 'artifact_repository_map' to deploy the model to Warehouse.\n",
            "  self._warn_once(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model logged successfully.: 100%|██████████| 6/6 [00:51<00:00,  8.56s/it]                          \n"
          ]
        }
      ],
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "from snowflake.ml.registry import registry\n",
        "\n",
        "sample_input_data = [\n",
        "    \"This is the first sentence.\",\n",
        "    \"Here's another sentence for testing.\",\n",
        "]\n",
        "\n",
        "reg = registry.Registry(session=session, database_name=DB_NAME, schema_name=SCHEMA_NAME)\n",
        "\n",
        "embed_model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
        "\n",
        "mv = reg.log_model(\n",
        "    embed_model,\n",
        "    model_name=\"sentence_transformer_minilm\",\n",
        "    sample_input_data=sample_input_data,\n",
        "    pip_requirements=[\n",
        "        \"numpy==1.26.4\",\n",
        "        \"sentence-transformers==5.1.1\",\n",
        "        \"torch==2.9.0\",\n",
        "        \"transformers==4.57.1\",\n",
        "    ],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0862efb3",
      "metadata": {},
      "source": [
        "### If you already have a model logged, please fill in MODEL_NAME and VERSION_NAME"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9cf43f9f",
      "metadata": {},
      "outputs": [],
      "source": [
        "from snowflake.ml.registry import registry\n",
        "\n",
        "MODEL_NAME = \"MODEL_NAME\"\n",
        "VERSION_NAME = \"V1\"\n",
        "\n",
        "reg = registry.Registry(session=session, database_name=DB_NAME, schema_name=SCHEMA_NAME)\n",
        "mv = reg.get_model(MODEL_NAME).get_version(VERSION_NAME)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "95556688",
      "metadata": {},
      "source": [
        "# Create/Load Inference Input Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "03efebef",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/envs/env_8_8/lib/python3.12/site-packages/snowflake/snowpark/_internal/utils.py:1148: UserWarning: data is a pandas DataFrame, parameter schema is ignored. To silence this warning pass schema=None.\n",
            "  return func(*args, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------------------------------------------------------------------------------------\n",
            "|\"input_feature_0\"                             |\"ID\"                                  |\n",
            "---------------------------------------------------------------------------------------\n",
            "|The quick brown fox jumps over the lazy dog.  |a1b2c3d4-e5f6-7890-1234-567890abcdef  |\n",
            "---------------------------------------------------------------------------------------\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Define the data for the DataFrame\n",
        "# The first column is the text to be embedded, the second column is the pass through ID column to map back to the input.\n",
        "data = [\n",
        "    (\"The quick brown fox jumps over the lazy dog.\", \"a1b2c3d4-e5f6-7890-1234-567890abcdef\"),\n",
        "    (\"Snowpark is a great library for data processing.\", \"f9e8d7c6-b5a4-3210-fedc-ba9876543210\"),\n",
        "    (\"Python is a versatile programming language.\", \"1a2b3c4d-5e6f-7080-9101-112131415161\")\n",
        "]\n",
        "\n",
        "# Define the column names and data types\n",
        "columns = [\"input_feature_0\", \"ID\"]\n",
        "schema = [\"input_feature_0 VARCHAR\", \"ID VARCHAR(36)\"]\n",
        "\n",
        "# Create a pandas DataFrame\n",
        "pandas_df = pd.DataFrame(data, columns=columns)\n",
        "\n",
        "# Create the Snowpark DataFrame from the pandas DataFrame\n",
        "X = session.create_dataframe(pandas_df, schema=schema)\n",
        "\n",
        "# show the first row of the input dataset to verify the data is loaded correctly\n",
        "X.show(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "53690ede",
      "metadata": {},
      "source": [
        "# Run the Batch Inference Job"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "db492523",
      "metadata": {},
      "outputs": [],
      "source": [
        "from snowflake.ml.model import JobSpec, OutputSpec, SaveMode\n",
        "\n",
        "output_stage_location = f\"@{DB_NAME}.{SCHEMA_NAME}.{STAGE_NAME}/output/\"\n",
        "\n",
        "job = mv.run_batch(\n",
        "    X=X,\n",
        "    compute_pool=COMPUTE_POOL_NAME,\n",
        "    output_spec=OutputSpec(stage_location=output_stage_location, mode=SaveMode.OVERWRITE),\n",
        "    job_spec=JobSpec(function_name=\"encode\"),\n",
        ")\n",
        "\n",
        "job.wait() # Wait for the job to complete"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "769a4d19",
      "metadata": {},
      "source": [
        "# Inspect the Inference Output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "76b3c09e",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------------------------------------------------------------------------------------------------------------------------------\n",
            "|\"name\"                                              |\"size\"  |\"md5\"                             |\"last_modified\"               |\n",
            "---------------------------------------------------------------------------------------------------------------------------------\n",
            "|batch_inference_quickstart_stage/output/3_63012...  |11627   |27033a315883bacf5e508df8af3a3daa  |Mon, 2 Feb 2026 23:05:07 GMT  |\n",
            "|batch_inference_quickstart_stage/output/_SUCCESS    |0       |d41d8cd98f00b204e9800998ecf8427e  |Mon, 2 Feb 2026 23:05:07 GMT  |\n",
            "---------------------------------------------------------------------------------------------------------------------------------\n",
            "\n"
          ]
        }
      ],
      "source": [
        "session.sql(f'LS {output_stage_location}').show()\n",
        "# The parquet files are the result of the inference job.\n",
        "# The \"_SUCCESS\" file is a marker file that indicates the job has completed successfully."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4849b88e",
      "metadata": {},
      "outputs": [],
      "source": [
        "session.read.option(\"pattern\", \".*\\\\.parquet\").parquet(output_stage_location).show(1, max_width=200)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3e035eee",
      "metadata": {},
      "source": [
        "## Copy the Output Stage Files into a Table"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bbbeb467",
      "metadata": {},
      "outputs": [],
      "source": [
        "output_table = \"batch_inference_output_table\"\n",
        "session.read.option(\"pattern\", \".*\\\\.parquet\").parquet(output_stage_location).write.mode(\"overwrite\").saveAsTable(output_table)\n",
        "# show the first row of the output table to verify the data is loaded correctly\n",
        "session.table(f'{DB_NAME}.{SCHEMA_NAME}.{output_table}').show(1) "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0c65b176",
      "metadata": {},
      "source": [
        "# Clean Up"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f4588e5f",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Row(status='BATCH_INFERENCE_QUICKSTART_SENTENCE_TRANSFORMER_COMPUTE_POOL successfully dropped.')]"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# uncomment to clean up the database and compute pool\n",
        "# session.sql(f'DROP DATABASE IF EXISTS {DB_NAME}').collect()\n",
        "# session.sql(f'DROP COMPUTE POOL IF EXISTS {COMPUTE_POOL_NAME}').collect()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "env_8_8",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
