{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Snowflake Online Feature Store:  Predicting Taxi Trip Durations\n",
    "\n",
    "This notebook demonstrates how to use the Snowflake Online Feature Store and Snowpark ML to build features, register feature views, and train a model to predict taxi trip durations in New York City. It covers entity and feature view registration, feature engineering, online and offline store usage, and real-time inference.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from snowflake.ml.feature_store import CreationMode, FeatureStore, feature_view\n",
    "from snowflake.ml.feature_store.entity import Entity\n",
    "from snowflake.snowpark import Session\n",
    "from snowflake.snowpark.exceptions import SnowparkSQLException\n",
    "from snowflake.snowpark.functions import avg, col, datediff, dayofweek, hour, month, nullifzero, when\n",
    "from snowflake.snowpark.types import DecimalType\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Snowflake connection parameters\n",
    "connection_parameters = {\n",
    "    \"account\": \"<account_name>\",\n",
    "    \"user\": \"<username>\",\n",
    "    \"password\": \"<programmatic_access_token>\",\n",
    "    \"role\": \"<role>\",\n",
    "    \"host\": \"<host - if using private link>\",\n",
    "    \"warehouse\": \"<warehouse>\",\n",
    "    \"database\": \"<database>\",\n",
    "    \"schema\": \"<schema>\"\n",
    "}\n",
    "session = Session.builder.configs(connection_parameters).create()\n",
    "\n",
    "# If you are in an interactive Notebook session, you can instead get the active session with:\n",
    "# from snowflake.snowpark.context import get_active_session\n",
    "# session = get_active_session()\n",
    "\n",
    "TAXI_DB = session.get_current_database()\n",
    "TAXI_SCHEMA = session.get_current_schema()\n",
    "TAXI_TABLE = \"NYC_YELLOW_TRIPS\"\n",
    "TAXI_TABLE_FULL_NAME = f\"{TAXI_DB}.{TAXI_SCHEMA}.{TAXI_TABLE}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Setup\n",
    "Load a sample dataset of NYC taxi trips. The data includes pickup and dropoff location IDs, timestamps, fare amounts, and more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-time setup: load example data\n",
    "try:\n",
    "    session.table(TAXI_TABLE_FULL_NAME).limit(0).collect()\n",
    "    print(\"NYC taxi table already exists\")\n",
    "except SnowparkSQLException as e:\n",
    "    print(\"Loading NYC taxi table\")\n",
    "    from snowflake.ml.feature_store.examples.example_helper import ExampleHelper\n",
    "    example_helper = ExampleHelper(session, session.get_current_database(), session.get_current_schema())\n",
    "    source_tables = example_helper.load_example('new_york_taxi_features')\n",
    "    for table in source_tables:\n",
    "        print(f\"{table}:\")\n",
    "        df = session.table(table).limit(5).to_pandas()\n",
    "        print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering with Snowflake Feature Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up Feature Store and context\n",
    "fs = FeatureStore(\n",
    "    session=session,\n",
    "    database=TAXI_DB,\n",
    "    name=TAXI_SCHEMA,\n",
    "    default_warehouse=session.get_current_warehouse(),\n",
    "    creation_mode=CreationMode.CREATE_IF_NOT_EXIST,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Register Entities\n",
    "Entities represent the keys used to join features. Here we define entities for location, trip, and route.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define and register entities\n",
    "route_entity = Entity(\n",
    "    name=\"route\",\n",
    "    join_keys=[\"PULOCATIONID\", \"DOLOCATIONID\"],\n",
    "    desc=\"A taxi route defined by pickup and dropoff location IDs.\"\n",
    ")\n",
    "pickup_time_entity = Entity(\n",
    "    name=\"pickup_time\",\n",
    "    join_keys=[\"PICKUP_HOUR\", \"PICKUP_DAY_OF_WEEK\"],\n",
    "    desc=\"Pickup time bucketed by hour and day of week\"\n",
    ")\n",
    "\n",
    "fs.register_entity(route_entity)\n",
    "fs.register_entity(pickup_time_entity)\n",
    "print(\"List Entities:\")\n",
    "fs.list_entities().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data and Create Features\n",
    "Read the NYC taxi trip data and engineer features for ETA, speed, rush hour, and more.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "df = session.table(TAXI_TABLE_FULL_NAME)\n",
    "\n",
    "# Create features\n",
    "df = df.with_column(\"TRIP_DISTANCE_INT\", col(\"TRIP_DISTANCE\").cast(DecimalType(10, 0)))\n",
    "df = df.with_column(\"ETA_MINUTES\", (datediff(\"second\", col(\"TPEP_PICKUP_DATETIME\"), col(\"TPEP_DROPOFF_DATETIME\")) / 60.0))\n",
    "df = df.with_column(\"ETA_MINUTES_INT\", col(\"ETA_MINUTES\").cast(DecimalType(10, 0)))\n",
    "df = df.with_column(\"PICKUP_HOUR\", hour(col(\"TPEP_PICKUP_DATETIME\")))\n",
    "df = df.with_column(\"PICKUP_DAY_OF_WEEK\", dayofweek(col(\"TPEP_PICKUP_DATETIME\")))  # 0=Sunday, 6=Saturday\n",
    "df = df.with_column(\"IS_WEEKEND\", when(col(\"PICKUP_DAY_OF_WEEK\").isin([0, 6]), 1).otherwise(0))\n",
    "df = df.with_column(\"PICKUP_MONTH\", month(col(\"TPEP_PICKUP_DATETIME\")))\n",
    "df = df.with_column(\"SPEED_MPH\", df[\"TRIP_DISTANCE\"] / nullifzero((df[\"ETA_MINUTES\"] / 60)))\n",
    "df = df.with_column(\"SPEED_MPH_INT\", col(\"SPEED_MPH\").cast(DecimalType(10, 0)))\n",
    "df = df.with_column(\"IS_RUSH_HOUR\", when((col(\"PICKUP_HOUR\").isin([7, 8, 9, 16, 17, 18, 19])),1).otherwise(0))\n",
    "df = df.with_column(\"IS_LOOP_ROUTE\", when(col(\"PULOCATIONID\") == col(\"DOLOCATIONID\"), 1).otherwise(0))\n",
    "print(\"Trip Data with Basic Features:\")\n",
    "df.show(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time-series rolling aggregations for route using Snowpark analytics.time_series_agg\n",
    "def _ts_col_namer(input_col, agg, window):\n",
    "    if agg == \"AVG\" and input_col == \"ETA_MINUTES_INT\":\n",
    "        return \"AVG_ETA_ROUTE\"\n",
    "    if agg == \"AVG\" and input_col == \"SPEED_MPH_INT\":\n",
    "        return \"AVG_SPEED_ROUTE\"\n",
    "    if agg == \"AVG\" and input_col == \"TRIP_DISTANCE_INT\":\n",
    "        return \"AVG_DISTANCE_ROUTE\"\n",
    "    return f\"{agg}_{input_col}_{window.replace('-', 'past_')}\"\n",
    "\n",
    "# Averaging on int columns allows the table to refresh incrementally\n",
    "df = df.analytics.time_series_agg(\n",
    "    aggs={\n",
    "        \"ETA_MINUTES_INT\": [\"AVG\"],\n",
    "        \"SPEED_MPH_INT\": [\"AVG\"],\n",
    "        \"TRIP_DISTANCE_INT\": [\"AVG\"],\n",
    "    },\n",
    "    windows=[\"-30D\"],\n",
    "    group_by=[\"PULOCATIONID\", \"DOLOCATIONID\"],\n",
    "    time_col=\"TPEP_PICKUP_DATETIME\",\n",
    "    col_formatter=_ts_col_namer,\n",
    ")\n",
    "\n",
    "print(\"Trip Data with Aggregated Features (time-series):\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select features for the feature view\n",
    "feature_df = df.select(\n",
    "    # Columns direct from table\n",
    "    \"PULOCATIONID\", \"DOLOCATIONID\", \"TRIP_DISTANCE\",\n",
    "    \"VENDORID\", \"FARE_AMOUNT\", \"TOTAL_AMOUNT\", \"TPEP_PICKUP_DATETIME\",\n",
    "    \"TIP_AMOUNT\", \"TOLLS_AMOUNT\",\n",
    "    # Feature columns created from table (exclude final label ETA_MINUTES)\n",
    "    \"PICKUP_HOUR\", \"PICKUP_DAY_OF_WEEK\", \"IS_WEEKEND\", \"PICKUP_MONTH\",\n",
    "    \"SPEED_MPH\", \"IS_RUSH_HOUR\", \"IS_LOOP_ROUTE\",\n",
    "    # Feature columns created from aggregating by column combinations\n",
    "    \"AVG_ETA_ROUTE\", \"AVG_DISTANCE_ROUTE\", \"AVG_SPEED_ROUTE\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Register Feature View\n",
    "Create and register a feature view for trip-based features, enabling online serving.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define and register the feature view\n",
    "route_fv = feature_view.FeatureView(\n",
    "    name=\"nyc_taxi_trip_fv\",\n",
    "    entities=[route_entity, pickup_time_entity],\n",
    "    feature_df=feature_df,\n",
    "    timestamp_col=\"TPEP_PICKUP_DATETIME\",\n",
    "    refresh_freq=\"60s\", # Dynamic Table refresh minimum\n",
    "    desc=\"Trip-based features for taxi ETA prediction\",\n",
    "    online_config=feature_view.OnlineConfig(enable=True, target_lag=\"10s\"),\n",
    ")\n",
    "\n",
    "registered_route_fv = fs.register_feature_view(route_fv, \"v1\", overwrite=True)\n",
    "\n",
    "print(\"Registered feature view:\", registered_route_fv.name, registered_route_fv.version)\n",
    "print(\"Online feature table:\", registered_route_fv.fully_qualified_online_table_name())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check refresh history\n",
    "fs.get_refresh_history(registered_route_fv, store_type=feature_view.StoreType.ONLINE).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch features from the online store\n",
    "online_df = fs.read_feature_view(\n",
    "    registered_route_fv,\n",
    "    store_type=feature_view.StoreType.ONLINE,\n",
    ")\n",
    "online_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore online tables\n",
    "fs.list_feature_views().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/Test Split\n",
    "Split the data into training and test sets for ML model development.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train/test split for ML\n",
    "spine_cols = [\n",
    "    \"ETA_MINUTES\",           # label\n",
    "    \"PULOCATIONID\",          # join key\n",
    "    \"DOLOCATIONID\",          # join key\n",
    "    \"PICKUP_HOUR\",           # join key\n",
    "    \"PICKUP_DAY_OF_WEEK\",    # join key\n",
    "    \"TPEP_PICKUP_DATETIME\"   # for point-in-time correctness (as spine_timestamp_col)\n",
    "]\n",
    "train_spine_df, test_spine_df = df.select(spine_cols).random_split([0.85, 0.15], seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate datasets with features for training and testing\n",
    "train_df = fs.generate_training_set(\n",
    "    spine_df=train_spine_df,\n",
    "    features=[registered_route_fv],\n",
    "    spine_label_cols=[\"ETA_MINUTES\"],  # Target column for regression\n",
    "    save_as=\"TAXI_TRAIN_SET\",\n",
    "    spine_timestamp_col=\"TPEP_PICKUP_DATETIME\"\n",
    ")\n",
    "test_df = fs.generate_training_set(\n",
    "    spine_df=test_spine_df,\n",
    "    features=[registered_route_fv],\n",
    "    spine_label_cols=[\"ETA_MINUTES\"],\n",
    "    save_as=\"TAXI_TEST_SET\",\n",
    "    spine_timestamp_col=\"TPEP_PICKUP_DATETIME\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train set sample:\")\n",
    "print(train_df.limit(5).to_pandas())\n",
    "print(\"Test set sample:\")\n",
    "print(test_df.limit(5).to_pandas())\n",
    "print(f\"Train set count: {train_df.count()}\")\n",
    "print(f\"Test set count: {test_df.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train XGBoost Model\n",
    "Train an XGBoost regressor using Snowpark ML on the generated training set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train XGBoost model using Snowpark ML\n",
    "from snowflake.ml.modeling.xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Define feature columns (exclude label and keys)\n",
    "feature_columns = [\n",
    "    col for col in train_df.columns\n",
    "    if col not in [\"ETA_MINUTES\", \"TPEP_PICKUP_DATETIME\"]\n",
    "]\n",
    "label_column = \"ETA_MINUTES\"\n",
    "\n",
    "regressor = XGBRegressor(\n",
    "    input_cols=feature_columns,\n",
    "    label_cols=[label_column],\n",
    "    output_cols=[\"predicted_eta\"]\n",
    ")\n",
    "regressor.fit(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on test set\n",
    "predictions = regressor.predict(test_df)\n",
    "predictions_pd = predictions.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model\n",
    "mse = mean_squared_error(predictions_pd[label_column], predictions_pd[\"predicted_eta\"])\n",
    "r2 = r2_score(predictions_pd[label_column], predictions_pd[\"predicted_eta\"])\n",
    "print(f\"Test MSE: {mse}\")\n",
    "print(f\"Test R2: {r2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importances\n",
    "xgb_native = regressor.to_xgboost()\n",
    "\n",
    "if hasattr(xgb_native, \"feature_importances_\"):\n",
    "    importances = xgb_native.feature_importances_\n",
    "    importance_df = pd.DataFrame({\n",
    "        'feature': feature_columns,\n",
    "        'importance': importances\n",
    "    }).sort_values(by='importance', ascending=False)\n",
    "    print(\"Feature importances:\")\n",
    "    print(importance_df)\n",
    "\n",
    "if hasattr(xgb_native, \"get_booster\"):\n",
    "    booster = xgb_native.get_booster()\n",
    "\n",
    "    weights = booster.get_score(importance_type='weight')\n",
    "    weights_df = pd.DataFrame(list(weights.items()), columns=['feature', 'weight']).sort_values(by='weight', ascending=False)\n",
    "    print(\"Booster weights:\")\n",
    "    print(weights_df)\n",
    "\n",
    "    gains = booster.get_score(importance_type='gain')\n",
    "    gains_df = pd.DataFrame(list(gains.items()), columns=['feature', 'gain']).sort_values(by='gain', ascending=False)\n",
    "    print(\"Booster gains:\")\n",
    "    print(gains_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real-time Inference Example\n",
    "Fetch the latest features from the online store and predict duration for a new trip."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_trip_duration(pu_location_id, do_location_id, pickup_hour, pickup_day_of_week):\n",
    "    trip = [\n",
    "        [pu_location_id, do_location_id, pickup_hour, pickup_day_of_week],\n",
    "    ]\n",
    "\n",
    "    # Fetch latest features from the online store\n",
    "    features_df = fs.read_feature_view(\n",
    "        registered_route_fv,\n",
    "        keys=trip,\n",
    "        store_type=feature_view.StoreType.ONLINE\n",
    "    )\n",
    "    \n",
    "    features_pd = features_df.to_pandas()\n",
    "    if features_pd.empty:\n",
    "        print(\"No online features found, skipping prediction\")\n",
    "        return None\n",
    "    \n",
    "    print(\"Online features:\")\n",
    "    print(features_pd)\n",
    "\n",
    "    return regressor.predict(features_pd)\n",
    "    \n",
    "\n",
    "prediction = predict_trip_duration(141, 236, 8, 0)\n",
    "if prediction is not None:\n",
    "    print(\"Predicted Trip Duration (minutes):\")\n",
    "    print(prediction['predicted_eta'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New Data Arrives\n",
    "Simulate a recently ended trip between the same pickup and dropoff location ids that also occurred on a Sunday between 8am and 9am. In this case, the trip takes longer than normal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from snowflake.snowpark.functions import max as sf_max, coalesce, lit\n",
    "\n",
    "# Compute next TRIP_ID\n",
    "next_trip_id = (\n",
    "    session.table(TAXI_TABLE_FULL_NAME)\n",
    "    .select(coalesce(sf_max(\"TRIP_ID\"), lit(0)) + lit(1))\n",
    "    .first()[0]\n",
    ")\n",
    "\n",
    "# Create single-row DataFrame for the new trip\n",
    "new_trip_df = session.create_dataframe(\n",
    "    [\n",
    "        (\n",
    "            1,                 # VENDORID\n",
    "            1,                 # PASSENGER_COUNT\n",
    "            3.8,               # TRIP_DISTANCE\n",
    "            1,                 # RATECODEID\n",
    "            \"N\",               # STORE_AND_FWD_FLAG\n",
    "            141,               # PULOCATIONID\n",
    "            236,               # DOLOCATIONID\n",
    "            1,                 # PAYMENT_TYPE\n",
    "            14.50,             # FARE_AMOUNT\n",
    "            3.00,              # EXTRA\n",
    "            0.50,              # MTA_TAX\n",
    "            3.65,              # TIP_AMOUNT\n",
    "            0.00,              # TOLLS_AMOUNT\n",
    "            0.30,              # IMPROVEMENT_SURCHARGE\n",
    "            29.95,             # TOTAL_AMOUNT\n",
    "            4.5,               # CONGESTION_SURCHARGE\n",
    "            0,                 # AIRPORT_FEE\n",
    "            datetime(2025, 8, 31, 8, 55, 0),  # TPEP_PICKUP_DATETIME\n",
    "            datetime(2025, 8, 31, 9, 15, 0),  # TPEP_DROPOFF_DATETIME\n",
    "            next_trip_id,      # TRIP_ID\n",
    "        )\n",
    "    ],\n",
    "    schema=[\n",
    "        \"VENDORID\",\n",
    "        \"PASSENGER_COUNT\",\n",
    "        \"TRIP_DISTANCE\",\n",
    "        \"RATECODEID\",\n",
    "        \"STORE_AND_FWD_FLAG\",\n",
    "        \"PULOCATIONID\",\n",
    "        \"DOLOCATIONID\",\n",
    "        \"PAYMENT_TYPE\",\n",
    "        \"FARE_AMOUNT\",\n",
    "        \"EXTRA\",\n",
    "        \"MTA_TAX\",\n",
    "        \"TIP_AMOUNT\",\n",
    "        \"TOLLS_AMOUNT\",\n",
    "        \"IMPROVEMENT_SURCHARGE\",\n",
    "        \"TOTAL_AMOUNT\",\n",
    "        \"CONGESTION_SURCHARGE\",\n",
    "        \"AIRPORT_FEE\",\n",
    "        \"TPEP_PICKUP_DATETIME\",\n",
    "        \"TPEP_DROPOFF_DATETIME\",\n",
    "        \"TRIP_ID\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "# Append to the existing table\n",
    "new_trip_df.write.mode(\"append\").save_as_table(TAXI_TABLE_FULL_NAME)\n",
    "print(\"Inserted 1 new trip row into\", TAXI_TABLE_FULL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction Reflects Latest Data\n",
    "After a maximum of 70 seconds (60 seconds of data lag for the Dynamic Table and 10 for the Online Feature Table), the latest data has been transformed into its features and is being used for inference. The predicted ETA has increased since the most recently completed trip took longer than normal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = predict_trip_duration(141, 236, 8, 0)\n",
    "if prediction is not None:\n",
    "    print(\"Predicted Trip Duration (minutes):\")\n",
    "    print(prediction['predicted_eta'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Feature View During Online Serving (optional)\n",
    "## Save the Model\n",
    "Log the trained model to Snowflake Model Registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snowflake.ml.registry import Registry\n",
    "\n",
    "registry = Registry(session=session)\n",
    "model_name = \"NYC_TAXI_ETA_XGB\"\n",
    "\n",
    "mv = registry.log_model(\n",
    "    model=regressor,\n",
    "    model_name=model_name,\n",
    "    comment=\"Predict NYC taxi trip durations (ETA) using Feature Store features\",\n",
    "    metrics={\"test_mse\": float(mse), \"test_r2\": float(r2)},\n",
    "    version_name=\"v1\",\n",
    ")\n",
    "print(\"Logged model version:\", mv)\n",
    "registry.show_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy the Model to Snowpark Container Services (SPCS)\n",
    "\n",
    "Deploy the logged model version to SPCS. This builds a container with the model’s dependencies, creates a service in a compute pool, and exposes service functions for inference.\n",
    "\n",
    "Prerequisites:\n",
    "- Existing compute pool with USAGE (or OWNERSHIP) for your role.\n",
    "- You can create one like this:\n",
    "```sql\n",
    "CREATE COMPUTE POOL IF NOT EXISTS trip_eta_prediction_pool\n",
    "  MIN_NODES = 1\n",
    "  MAX_NODES = 1\n",
    "  INSTANCE_FAMILY = 'CPU_X64_M'\n",
    "  AUTO_RESUME = TRUE;\n",
    "GRANT USAGE ON COMPUTE POOL trip_eta_prediction_pool TO ROLE <your_role>;\n",
    "GRANT BIND SERVICE ENDPOINT ON ACCOUNT TO ROLE <your_role>;\n",
    "SHOW COMPUTE POOLS LIKE 'trip_eta_prediction_pool';\n",
    "```\n",
    "\n",
    "References:\n",
    "- Snowflake docs: [Model Serving in SPCS](https://docs.snowflake.com/en/developer-guide/snowflake-ml/model-registry/container#label-model-registry-container-xgboost-cpu-inference)\n",
    "- Quickstart: [Deploy custom models to Model Registry](https://quickstarts.snowflake.com/guide/deploying_custom_models_to_snowflake_model_registry/index.html?index=..%2F..index#0)\n",
    "- Quickstart: [Snowpark Container Services Model Serving Guide](https://quickstarts.snowflake.com/guide/snowpark-container-services-model-serving-guide/index.html?index=..%2F..index#3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latest_model = registry.get_model(model_name).version(\"v1\")\n",
    "print(\"Deploying model:\", latest_model)\n",
    "\n",
    "service_name = \"NYC_TAXI_ETA_V1\"\n",
    "latest_model.create_service(\n",
    "    service_name=service_name,\n",
    "    service_compute_pool=\"trip_eta_prediction_pool\",\n",
    "    ingress_enabled=True  # expose the service\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use the Model in SPCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Direct prediction from the model prediction service in Python\n",
    "\n",
    "# Check estimated duration from pickup location 141 to dropoff location 236 at 8am on a Sunday\n",
    "trip = [[141, 236, 8, 0]]\n",
    "\n",
    "features_df = fs.read_feature_view(\n",
    "    registered_route_fv,\n",
    "    keys=trip,\n",
    "    store_type=feature_view.StoreType.ONLINE\n",
    ")\n",
    "\n",
    "features_pd = features_df.to_pandas()\n",
    "if features_pd.empty:\n",
    "    print(\"No online features found, skipping prediction\")\n",
    "else:\n",
    "    print(\"Online features:\")\n",
    "    print(features_pd)\n",
    "\n",
    "    spcs_prediction = latest_model.run(features_pd, function_name='predict', service_name=service_name)\n",
    "    print(\"Predicted Trip Duration (minutes) from SPCS:\")\n",
    "    print(spcs_prediction['\\\"predicted_eta\\\"'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the exposed service URL\n",
    "services_df = latest_model.list_services()\n",
    "# You can also view the inference endpoint via SQL: \"show endpoints in service {service_name}\"\n",
    "base = str(services_df[\"inference_endpoint\"].iloc[0])\n",
    "URL = \"https://\" + base + \"/predict\"\n",
    "print(\"Service predict URL:\", URL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use the ingress endpoint to get the service prediction via HTTP\n",
    "\n",
    "This example uses session token authorization for API access. Most services will prefer key-pair authentication. An example of using JSON Web Token (JWT) key-pair to access SPCS services is [linked here](https://docs.snowflake.com/en/developer-guide/snowpark-container-services/tutorials/tutorial-1#optional-access-the-public-endpoint-programmatically)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "import requests\n",
    "\n",
    "# Header uses Programmatic Access Token (PAT) from the connection to authenticate\n",
    "headers = {'Authorization': f'Snowflake Token=\\\"{connection_parameters[\"password\"]}\\\"'}\n",
    "\n",
    "def build_service_payload(features_pd):\n",
    "    df = features_pd.copy()\n",
    "    # Convert datetime columns to ISO strings\n",
    "    for col_name, dtype in df.dtypes.items():\n",
    "        if str(dtype).startswith(\"datetime\"):\n",
    "            df[col_name] = df[col_name].dt.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    # Replace NaN with None\n",
    "    df = df.where(pd.notnull(df), None)\n",
    "    # Convert NumPy scalar types to Python scalars\n",
    "    df = df.apply(lambda s: s.map(lambda x: x.item() if isinstance(x, np.generic) else x))\n",
    "\n",
    "    # Select exactly the model's expected input columns (order matters)\n",
    "    payload_df = df[feature_columns]\n",
    "\n",
    "    # Build payload: each row is [row_index, {column_name: value, ...}]\n",
    "    records = payload_df.to_dict(orient=\"records\")\n",
    "    return {\"data\": [[i, rec] for i, rec in enumerate(records)]}\n",
    "\n",
    "# Build JSON payload\n",
    "data = build_service_payload(features_pd)\n",
    "\n",
    "# Send over HTTP\n",
    "def send_request(data: dict):\n",
    "    output = requests.post(URL, json=data, headers=headers)\n",
    "    if output.status_code != 200:\n",
    "        try:\n",
    "            print(\"Response body (error):\", output.text)\n",
    "        except Exception:\n",
    "            pass\n",
    "    assert (output.status_code == 200), f\"Failed to get response from the service. Status code: {output.status_code}\"\n",
    "    return output.content\n",
    "\n",
    "# Do the prediction\n",
    "results = send_request(data=data)\n",
    "print(\"Predicted Trip Duration (minutes) from SPCS via HTTP:\")\n",
    "pprint(json.loads(results))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "online-feature-table-demo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
