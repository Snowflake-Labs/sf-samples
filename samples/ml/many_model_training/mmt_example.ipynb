{
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  },
  "lastEditStatus": {
   "notebookId": "cfopp363nkjkoavnww5c",
   "authorId": "426741307362",
   "authorName": "SHCHEN",
   "authorEmail": "shulin.chen@snowflake.com",
   "sessionId": "31122e9e-7d4c-40b2-881f-fa7d34b469a5",
   "lastEditTime": 1754950929565
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c8dc41e-3636-465a-a317-4ead3c143ba6",
   "metadata": {
    "name": "cell6",
    "collapsed": false
   },
   "source": [
    "## Many Model Training (MMT) – Example Walkthrough\n",
    "\n",
    "This notebook demonstrates how to use Snowflake's Many Model Training (MMT) API to train multiple models in parallel.\n",
    "\n",
    "We’ll:\n",
    "- Define a custom training function\n",
    "- Run MMT on a synthetic dataset\n",
    "- Optionally scale with multiple nodes to speed up MMT.\n",
    "- Monitor training progress and debug failures\n",
    "- Inspect model logs and metadata from previous runs (even after the notebook/session is closed)\n",
    "- Show ways to run inference on the trained models.\n",
    "\n",
    "\n",
    "🛠️ **Callout**:  \n",
    "There are two ways to invoke the MMT API:\n",
    "1. **Inside the Snowflake Notebook environment**.\n",
    "\n",
    "2. **Outside the notebook via ML Jobs** — This option allows you to run the MMT API in a headless setup. Please refer to the [headless setup guide](../ml_jobs)\n",
    "for details.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9dab66e-d310-4fbc-93a8-5ed8946b805e",
   "metadata": {
    "name": "cell7",
    "collapsed": false
   },
   "source": [
    "## Step 1: Basic Setup\n",
    "Set up training function and generate synthetic training data."
   ]
  },
  {
   "cell_type": "code",
   "id": "3775908f-ca36-4846-8f38-5adca39217f2",
   "metadata": {
    "language": "python",
    "name": "session_setup"
   },
   "source": [
    "from snowflake.snowpark.context import get_active_session\n",
    "session = get_active_session()\n",
    "\n",
    "# Create a stage that will be used to store various training artifacts, including models, logs and etc.\n",
    "stage_name = \"MY_STAGE\" \n",
    "session.sql(f\"CREATE STAGE IF NOT EXISTS {stage_name}\").collect()\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "65d6ef5a-ac5c-4c66-9dec-cac15b94db20",
   "metadata": {
    "language": "python",
    "name": "define_training_func"
   },
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from snowflake.ml.data import DataConnector\n",
    "from snowflake.ml.modeling.distributors.distributed_partition_function.partition_context import (\n",
    "    PartitionContext,\n",
    ")\n",
    "\n",
    "def user_training_func(data_connector: DataConnector, context: PartitionContext) -> Any:\n",
    "    \"\"\"\n",
    "    User-defined function that takes in a DataConnector object and returns a trained model.\n",
    "        \n",
    "    Args:\n",
    "        data_connector: A Snowflake DataConnector object containing partitioned training data. This is passed in by the \n",
    "        Snowflake data ingestion framework, which handles extracting data from the warehouse  and converting it into a \n",
    "        DataConnector object which contains only the partitioned data.\n",
    "            \n",
    "    Returns:\n",
    "        Trained model object.\n",
    "    \"\"\"\n",
    "    partition_id = context.partition_id\n",
    "    assert partition_id is not None\n",
    "\n",
    "    # Load partitioned data.\n",
    "    pandas_df: pd.DataFrame = data_connector.to_pandas()\n",
    "    \n",
    "    # Define feature and label columns\n",
    "    NUMERICAL_COLUMNS = [\"X1\", \"X2\", \"X3\"]\n",
    "    LABEL_COLUMNS = \"X4\"\n",
    "    \n",
    "    # Train the model\n",
    "    model = xgb.XGBRegressor()\n",
    "    model.fit(pandas_df[NUMERICAL_COLUMNS], pandas_df[LABEL_COLUMNS])\n",
    "    \n",
    "    # Evaluate on training data\n",
    "    preds = model.predict(pandas_df[NUMERICAL_COLUMNS])\n",
    "    mse = mean_squared_error(pandas_df[LABEL_COLUMNS], preds)\n",
    "    r2 = r2_score(pandas_df[LABEL_COLUMNS], preds)\n",
    "    \n",
    "    # Print metrics\n",
    "    print(f\"[Partition {partition_id}] Training MSE: {mse:.4f}, R²: {r2:.4f}\")\n",
    "    \n",
    "    return model\n"
   ],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "32d9082a-1d5a-4800-a83e-0f768844d58f",
   "metadata": {
    "language": "python",
    "name": "define_training_data"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import numpy as np\n",
    "import uuid\n",
    "\n",
    "def _init_snowpark_df(curr_session, partition_counts=2):\n",
    "    \"\"\"\n",
    "    Initializes and returns a Snowpark DataFrame containing synthetic regression data.\n",
    "\n",
    "    This function generates a dataset with 4 numerical features using `make_regression`,\n",
    "    where the first three columns (\"X1\", \"X2\", \"X3\") are treated as input features and \n",
    "    the fourth column (\"X4\") as the target variable. Each row is also assigned:\n",
    "      - A LOCATION_ID (Partition key based on modulo of total rows and `partition_counts`)\n",
    "      - A randomly selected date between 2020-01-01 and 2023-01-01\n",
    "\n",
    "    The resulting DataFrame is uploaded to Snowflake as a permanent table with a \n",
    "    unique name in the current database and schema. The table name is returned as a\n",
    "    Snowpark DataFrame object.\n",
    "\n",
    "    Args:\n",
    "        curr_session: A valid Snowpark session object.\n",
    "        partition_counts (int, optional): Number of unique partition values \n",
    "            for the LOCATION_ID column. Defaults to 2.\n",
    "\n",
    "    Returns:\n",
    "        Snowpark DataFrame: A reference to the saved table in Snowflake.\n",
    "    \n",
    "    \"\"\"\n",
    "    # Generate synthetic data\n",
    "    cols = [\"X1\", \"X2\", \"X3\", \"X4\"]\n",
    "    X, _ = make_regression(n_samples=1000, n_features=4, noise=0.1, random_state=0)\n",
    "    df = pd.DataFrame(X, columns=cols)\n",
    "    df[\"LOCATION_ID\"] = np.arange(len(df)) % partition_counts\n",
    "\n",
    "    # Add random dates between 2020-01-01 and 2023-01-01\n",
    "    date_range = pd.date_range(\"2020-01-01\", \"2023-01-01\", freq=\"D\")\n",
    "    df[\"DATE\"] = np.random.choice(date_range, size=len(df))\n",
    "\n",
    "    # Create Snowpark DataFrame and save to a uniquely named table\n",
    "    snowpark_df = curr_session.create_dataframe(df)\n",
    "    table_name = f\"{curr_session.get_current_database()}.{curr_session.get_current_schema()}.mmt_test_{uuid.uuid4().hex.upper()}\"\n",
    "    snowpark_df.write.mode(\"overwrite\").save_as_table(table_name)\n",
    "\n",
    "    return curr_session.table(table_name)\n",
    "\n",
    "\n",
    "snowpark_df = _init_snowpark_df(session)"
   ],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "62be28ec-bd49-4dfd-ae5d-b11df1b64d5b",
   "metadata": {
    "name": "cell8",
    "collapsed": false
   },
   "source": [
    "## Step 2: Invoke MMT API & Monitor MMT Training Run\n",
    "\n",
    "User can optionally choose to scale up the cluster to multi-nodes prior to run the many model training. "
   ]
  },
  {
   "cell_type": "code",
   "id": "c3100c37-2f17-4348-a884-09bfdae0666d",
   "metadata": {
    "language": "python",
    "name": "scale_to_multi_nodes"
   },
   "outputs": [],
   "source": [
    "# Optional step to scale to multiple nodes for speed up overall many model trainings.\n",
    "# from snowflake.ml.runtime_cluster import cluster_manager\n",
    "# TOTAL_NODES=5\n",
    "# cluster_manager.scale_cluster(expected_cluster_size=TOTAL_NODES, notebook_name=NOTEBOOK_NAME, options={\"block_until_min_cluster_size\": 2})"
   ],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f5a23617-d3ba-4bf6-af3e-56a38c31f747",
   "metadata": {
    "language": "python",
    "name": "async_many_model_train"
   },
   "outputs": [],
   "source": [
    "from snowflake.ml.modeling.distributors.many_model import ManyModelTraining\n",
    "from snowflake.ml.modeling.distributors.distributed_partition_function.entities import (\n",
    "    ExecutionOptions,\n",
    "    RunStatus,\n",
    ")\n",
    "\n",
    "trainer = ManyModelTraining(\n",
    "    user_training_func,    \n",
    "    stage_name=stage_name,    \n",
    ")\n",
    "\n",
    "run_id=\"my_mmt_model_v1\"\n",
    "training_run = trainer.run(\n",
    "    snowpark_dataframe=snowpark_df,\n",
    "    partition_by=\"LOCATION_ID\",\n",
    "    run_id=run_id,\n",
    "    on_existing_artifacts=\"overwrite\", # or \"error\"\n",
    "    # execution_options is optional. When running in a multi-node setting, it's recommended setting use_head_node=False to exclude head node from doing actual training, this improves overall MMT training reliability.\n",
    "    # execution_options=ExecutionOptions(use_head_node=False)\n",
    ")"
   ],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "46da52a6-40e5-415c-86d3-3a27c82ef9be",
   "metadata": {
    "language": "python",
    "name": "cell1"
   },
   "outputs": [],
   "source": [
    "assert training_run.wait() == RunStatus.SUCCESS"
   ],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "23240e44-dc8a-4330-8784-fe818b852bb0",
   "metadata": {
    "language": "python",
    "name": "inspect_result"
   },
   "outputs": [],
   "source": [
    "training_run.get_progress()[\"DONE\"][0].logs # inspect result\n",
    "\n",
    "# To inspect failures\n",
    "# training_run.get_progress()[\"FAILED\"][0].logs "
   ],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b343930b-720c-443b-8ccd-64b67064548b",
   "metadata": {
    "language": "python",
    "name": "cell3"
   },
   "outputs": [],
   "source": [
    "# To obtain models trained with each partition\n",
    "for partition_id in training_run.partition_details.keys():\n",
    "    model = training_run.get_model(partition_id)\n",
    "    assert isinstance(model, xgb.XGBRegressor)"
   ],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "72091481-5ce5-48e0-a2aa-565b5fe9a08c",
   "metadata": {
    "language": "python",
    "name": "inspect_stage"
   },
   "outputs": [],
   "source": [
    "# Ideally you will not need to interact with the stage at all. This is more of a FYI how your stage is being used\n",
    "# to persist the model and other artifacts.\n",
    "session.sql(f\"ls @{stage_name}\").collect()"
   ],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "fcd69cfe-e38e-45af-ac2c-22f2ea3920f4",
   "metadata": {
    "name": "cell17",
    "collapsed": false
   },
   "source": [
    "## Step 3: Running Inference on Trained Models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f389e4f8-2411-4f3d-b699-b6490a69d7e7",
   "metadata": {
    "name": "cell2",
    "collapsed": false
   },
   "source": [
    "### Step 3.1: Register Models in the Snowflake Model Registry and Run Inference (Warehouse Execution) — GA Feature\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "541d6c63-62da-41c6-8d14-6b418e7c55c8",
   "metadata": {
    "language": "python",
    "name": "cell14"
   },
   "outputs": [],
   "source": [
    "models = {\n",
    "    partition_id: training_run.get_model(partition_id)\n",
    "    for partition_id in training_run.partition_details\n",
    "}"
   ],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "6d9c2a04-9c6e-42ee-95ec-10b2e64fea11",
   "metadata": {
    "language": "python",
    "name": "cell13"
   },
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "from snowflake.ml.model import custom_model\n",
    "from snowflake.ml.registry import registry\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Log model to model registry\n",
    "class PartitionedModel(custom_model.CustomModel):\n",
    "    def __init__(self, context: Optional[custom_model.ModelContext] = None) -> None:\n",
    "        super().__init__(context)\n",
    "        self.partition_id = None\n",
    "        self.model = None\n",
    "\n",
    "    @custom_model.partitioned_api\n",
    "    def predict(self, input: pd.DataFrame) -> pd.DataFrame:\n",
    "        NUMERICAL_COLUMNS = [\"X1\", \"X2\", \"X3\"]\n",
    "\n",
    "        model_id = str(input[\"LOCATION_ID\"][0])\n",
    "        model = self.context.model_ref(model_id)\n",
    "\n",
    "        model_output = model.predict(input[NUMERICAL_COLUMNS])\n",
    "        res = pd.DataFrame(model_output)\n",
    "        return res\n",
    "\n"
   ],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "92627231-4445-4612-927b-d5e52e7cb300",
   "metadata": {
    "language": "python",
    "name": "cell16"
   },
   "outputs": [],
   "source": [
    "from snowflake.ml.model import custom_model\n",
    "\n",
    "# Models have been fit, and they can now be retrieved and registered to the model registry.\n",
    "model_context = custom_model.ModelContext(\n",
    "    models=models\n",
    ")\n",
    "\n",
    "my_stateful_model = PartitionedModel(context=model_context)\n",
    "reg = registry.Registry(session=session)\n",
    "options = {\n",
    "    \"function_type\": \"TABLE_FUNCTION\",\n",
    "    \"relax_version\": False\n",
    "}\n",
    "NUMERICAL_COLUMNS = [\"X1\", \"X2\", \"X3\"]\n",
    "mv = reg.log_model(\n",
    "    my_stateful_model,\n",
    "    model_name=\"partitioned_model\",\n",
    "    options=options,\n",
    "    conda_dependencies=[\"pandas\", \"xgboost\"],\n",
    "    sample_input_data=snowpark_df.limit(1).to_pandas()[NUMERICAL_COLUMNS + [\"LOCATION_ID\"]],    \n",
    ")"
   ],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "5624a55d-bebd-4698-97d0-c03528330c6b",
   "metadata": {
    "language": "python",
    "name": "cell12"
   },
   "outputs": [],
   "source": [
    "service_prediction = mv.run(\n",
    "    snowpark_df,\n",
    "    partition_column=\"LOCATION_ID\",\n",
    ")"
   ],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "cdf74ec0-4cd5-402b-93d2-9bb2476e0467",
   "metadata": {
    "name": "cell15",
    "collapsed": false
   },
   "source": [
    "### Step 3.2: Alternative ManyModelInference Method (Container Execution) — Preview Feature\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "4a0efb12-5d54-449d-9a13-d74e1dde1534",
   "metadata": {
    "language": "python",
    "name": "cell18"
   },
   "outputs": [],
   "source": [
    "from snowflake.ml.modeling.distributors.many_model import ManyModelInference\n",
    "\n",
    "def xgb_inference_func(data_connector: DataConnector, model, context: PartitionContext):\n",
    "    \"\"\"Simple inference function.\"\"\"\n",
    "    df = data_connector.to_pandas()\n",
    "    X = df[[\"X1\", \"X2\", \"X3\"]].values\n",
    "    predictions = model.predict(X)\n",
    "\n",
    "    # Write prediction results to persistent storage\n",
    "    results = df.copy()\n",
    "    results['predictions'] = predictions\n",
    "    \n",
    "    # Two persistence strategies (choose one or both based on your needs):\n",
    "\n",
    "    # Strategy 1: Stage artifacts - for framework management and debugging\n",
    "    # context.upload_to_stage(results, \"predictions.csv\",\n",
    "    #     write_function=lambda df, path: df.to_csv(path, index=False))\n",
    "\n",
    "    # Strategy 2: Snowflake table - for immediate downstream consumption\n",
    "    # predictions_df = context.session.create_dataframe(results)\n",
    "    # predictions_df.write.mode(\"append\").save_as_table(\"sales_predictions\")\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "mmi = ManyModelInference(\n",
    "    inference_func=xgb_inference_func,\n",
    "    stage_name=stage_name,\n",
    "    training_run_id=run_id, # run_id from previous many model training run at step 2\n",
    ")\n",
    "\n",
    "\n",
    "inference_run = mmi.run(\n",
    "    partition_by=\"LOCATION_ID\",\n",
    "    snowpark_dataframe=snowpark_df, # running inference on the same training data mainly for illustration purposes.\n",
    "    run_id=\"basic_inference_run\",\n",
    "    on_existing_artifacts=\"overwrite\",\n",
    ")"
   ],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "9f5bc12e-2b0c-46a0-93c6-a3d231104d34",
   "metadata": {
    "language": "python",
    "name": "cell19"
   },
   "outputs": [],
   "source": [
    "assert inference_run.wait() == RunStatus.SUCCESS"
   ],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "52a1c2cf-be75-443e-b08e-92fcaa86fc88",
   "metadata": {
    "language": "python",
    "name": "cell20"
   },
   "outputs": [],
   "source": [
    "inference_run.get_progress()\n",
    "# inference_run.get_progress()[\"FAILED\"][0].logs"
   ],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "bd08539f-53b7-4906-aaf5-30e5432e9b7a",
   "metadata": {
    "name": "cell5",
    "collapsed": false
   },
   "source": [
    "## Step 4: Troubleshooting Failed Runs\n",
    "\n",
    "Training functions can fail for various reasons. Below are some common causes:\n",
    "\n",
    "- **User Code Errors**  \n",
    "  Bugs or issues in the user-defined training function can cause failures.\n",
    "\n",
    "- **Infrastructure Issues**  \n",
    "  An *Out-of-Memory (OOM)* error occurs when the training function consumes more memory than the node can provide.\n",
    "\n",
    "- **Unexpected Node Failures**  \n",
    "  In some cases, a node might crash unexpectedly.\n",
    "\n",
    "---\n",
    "\n",
    "### Handling OOM and Node Failures\n",
    "\n",
    "When an OOM error or fatal node failure occurs, the **MMT API will not automatically retry** the training function. Instead, it will mark the corresponding partition ID run as **`INTERNAL_ERROR`**. If a worker node crashes, logs might not be captured, making debugging more difficult.\n",
    "\n",
    "For all other failure scenarios (including OOM errors), MMT provides:\n",
    "- A **detailed error message**  \n",
    "- A **stack trace** to help diagnose and fix the issue\n",
    "\n",
    "---\n",
    "\n",
    "### Retry Logic for Non-Fatal Errors\n",
    "\n",
    "If the failure is not considered fatal (e.g., transient issues), MMT will automatically retry the training function with **exponential backoff**. This mechanism allows transient issues to resolve before the function ultimately fails.\n",
    "\n",
    "**Retry Attempts:**\n",
    "1. **First retry**: Wait for 2 seconds (`initial_delay`)\n",
    "2. **Second retry**: Wait for 4 seconds (2 * `initial_delay`)\n",
    "3. **Third retry**: Wait for 8 seconds (2^2 * `initial_delay`)\n",
    "4. **Fourth retry**: Wait for 16 seconds (2^3 * `initial_delay`)\n",
    "5. **Final retry**: No delay — if it fails again, an exception is raised\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "a2bfe1a6-79f5-40c1-b758-4c056d48a381",
   "metadata": {
    "language": "python",
    "name": "cell4"
   },
   "outputs": [],
   "source": [
    "def user_func_error(data_connector: DataConnector, context: PartitionContext):\n",
    "    pandas_df = data_connector.to_pandas()\n",
    "\n",
    "    NUMERICAL_COLUMNS = [\"X1\", \"X2\", \"X3\"]\n",
    "    LABEL_COLUMNS = [\"X4\"]\n",
    "    model = xgb.XGBRegressor()\n",
    "\n",
    "    # INTENTIONAL USER-CODE FAILURE: fitss function does not exist\n",
    "    model.fitss(pandas_df[NUMERICAL_COLUMNS], pandas_df[LABEL_COLUMNS])    \n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "model_name=\"my_mmt_model\"\n",
    "model_version = \"v2\"\n",
    "run_id=f\"{model_name}_{model_version}\"\n",
    "\n",
    "trainer = ManyModelTraining(\n",
    "    user_func_error,    \n",
    "    stage_name=stage_name,\n",
    ")\n",
    "\n",
    "failed_trainer_run = trainer.run(\n",
    "    snowpark_dataframe=snowpark_df,\n",
    "    partition_by=\"LOCATION_ID\",    \n",
    "    run_id=run_id,\n",
    "    on_existing_artifacts=\"overwrite\", # or \"error\"\n",
    ")\n"
   ],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "dbb4dcaf-1184-4775-95d8-7be33fa585ae",
   "metadata": {
    "language": "python",
    "name": "cell9"
   },
   "outputs": [],
   "source": [
    "# MMT will retry the user-function up to five times and then fail.\n",
    "\n",
    "# Helper function for illustartion purposes of getting failed logs.\n",
    "import time\n",
    "while True:\n",
    "    if \"FAILED\" in failed_trainer_run.get_progress():\n",
    "        break\n",
    "    time.sleep(1)\n",
    "\n",
    "# User can optionally choose to cancel the entire MMT run when at least one failed run is detected.\n",
    "failed_trainer_run.cancel()\n",
    "\n",
    "# Show first failed partition logs\n",
    "failed_trainer_run.get_progress()[\"FAILED\"][0].logs"
   ],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "48583e54-0b89-4598-b599-c301f29373d5",
   "metadata": {
    "name": "cell10",
    "collapsed": false
   },
   "source": [
    "## Step 5: Inspecting Models and Logs After Notebook/Session Closure\n",
    "\n",
    "\n",
    "After investing considerable time training multiple models, you might want to shut down your notebook temporarily to save resources. But how can you recover your trained models and review logs later?\n",
    "\n",
    "Snowflake offers an API designed precisely for this use case.\n",
    "\n",
    "Using `DPFRun`, you can easily restore previously trained models and access their logs and metadata—even after your notebook session has ended.\n",
    "\n",
    "Below, we demonstrate how to retrieve and inspect artifacts from a previous run.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "94810768-6cba-440c-b521-d88227531226",
   "metadata": {
    "language": "python",
    "name": "cell11"
   },
   "outputs": [],
   "source": [
    "from snowflake.ml.modeling.distributors.distributed_partition_function.dpf_run import (\n",
    "    DPFRun,\n",
    ")\n",
    "\n",
    "restored_run = DPFRun.restore_from(\n",
    "    run_id=run_id,\n",
    "    stage_name=stage_name,\n",
    ")\n",
    "\n",
    "# Check the status of the trained model\n",
    "model_status = restored_run.status\n",
    "print(model_status)\n",
    "\n",
    "# You can also access other APIs to inspect logs and metadata\n",
    "restored_run.get_progress()\n"
   ],
   "execution_count": null
  }
 ]
}
