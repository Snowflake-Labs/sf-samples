{
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  },
  "lastEditStatus": {
   "notebookId": "jobukmxnx2hlc57l2aro",
   "authorId": "4914184347444",
   "authorName": "ADMIN",
   "authorEmail": "",
   "sessionId": "b6e0ca98-7629-4295-a565-3110c90f9173",
   "lastEditTime": 1748905981827
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c8dc41e-3636-465a-a317-4ead3c143ba6",
   "metadata": {
    "name": "cell6",
    "collapsed": false
   },
   "source": [
    "## Many Model Training (MMT) – Example Walkthrough\n",
    "\n",
    "This notebook demonstrates how to use Snowflake's Many Model Training (MMT) API to train multiple models in parallel.\n",
    "\n",
    "We’ll:\n",
    "- Define a custom training function\n",
    "- Run MMT on a synthetic dataset\n",
    "- Optionally scale with multiple nodes to speed up MMT.\n",
    "- Monitor training progress and debug failures\n",
    "- Inspect model logs and metadata from previous runs (even after the notebook/session is closed)\n",
    "\n",
    "\n",
    "🛠️ **Callout**:  \n",
    "There are two ways to invoke the MMT API:\n",
    "1. **Inside the Snowflake Notebook environment**.\n",
    "\n",
    "2. **Outside the notebook via ML Jobs** — This option allows you to run the MMT API in a headless setup. Please refer to the [headless setup guide](../ml_jobs)\n",
    "for details. Note that the latest headless version does not include the MMT feature, so adjustments may be required. For assistance, reach out to Snowflake support.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9dab66e-d310-4fbc-93a8-5ed8946b805e",
   "metadata": {
    "name": "cell7",
    "collapsed": false
   },
   "source": [
    "## Step 1: Basic Setup\n",
    "Set up training function and generate synthetic training data."
   ]
  },
  {
   "cell_type": "code",
   "id": "3775908f-ca36-4846-8f38-5adca39217f2",
   "metadata": {
    "language": "python",
    "name": "session_setup"
   },
   "source": "from snowflake.snowpark.context import get_active_session\nsession = get_active_session()\n\n# Create a stage that will be used to store various training artifacts, including models, logs and etc.\nstage_name = \"MY_STAGE\" \nsession.sql(f\"CREATE STAGE IF NOT EXISTS {stage_name}\").collect()\n",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "65d6ef5a-ac5c-4c66-9dec-cac15b94db20",
   "metadata": {
    "language": "python",
    "name": "define_training_func"
   },
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "def user_training_func(data_connector, **kwargs) -> Any:\n",
    "    \"\"\"\n",
    "    User-defined function that takes in a DataConnector object and returns a trained model.\n",
    "        \n",
    "    Args:\n",
    "        data_connector: A Snowflake DataConnector object containing partitioned training data. This is passed in by the \n",
    "        Snowflake data ingestion framework, which handles extracting data from the warehouse  and converting it into a \n",
    "        DataConnector object which contains only the partitioned data.\n",
    "            \n",
    "    Returns:\n",
    "        Trained model object.\n",
    "    \"\"\"\n",
    "    partition_id = kwargs.get(\"partition_id\", None)\n",
    "    assert partition_id is not None\n",
    "\n",
    "    # Load partitioned data.\n",
    "    pandas_df: pd.DataFrame = data_connector.to_pandas()\n",
    "    \n",
    "    # Define feature and label columns\n",
    "    NUMERICAL_COLUMNS = [\"X1\", \"X2\", \"X3\"]\n",
    "    LABEL_COLUMNS = \"X4\"\n",
    "    \n",
    "    # Train the model\n",
    "    model = xgb.XGBRegressor()\n",
    "    model.fit(pandas_df[NUMERICAL_COLUMNS], pandas_df[LABEL_COLUMNS])\n",
    "    \n",
    "    # Evaluate on training data\n",
    "    preds = model.predict(pandas_df[NUMERICAL_COLUMNS])\n",
    "    mse = mean_squared_error(pandas_df[LABEL_COLUMNS], preds)\n",
    "    r2 = r2_score(pandas_df[LABEL_COLUMNS], preds)\n",
    "    \n",
    "    # Print metrics\n",
    "    print(f\"[Partition {partition_id}] Training MSE: {mse:.4f}, R²: {r2:.4f}\")\n",
    "    \n",
    "    return model\n"
   ],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "32d9082a-1d5a-4800-a83e-0f768844d58f",
   "metadata": {
    "language": "python",
    "name": "define_training_data"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import numpy as np\n",
    "import uuid\n",
    "\n",
    "def _init_snowpark_df(curr_session, partition_counts=2):\n",
    "    \"\"\"\n",
    "    Initializes and returns a Snowpark DataFrame containing synthetic regression data.\n",
    "\n",
    "    This function generates a dataset with 4 numerical features using `make_regression`,\n",
    "    where the first three columns (\"X1\", \"X2\", \"X3\") are treated as input features and \n",
    "    the fourth column (\"X4\") as the target variable. Each row is also assigned:\n",
    "      - A LOCATION_ID (Partition key based on modulo of total rows and `partition_counts`)\n",
    "      - A randomly selected date between 2020-01-01 and 2023-01-01\n",
    "\n",
    "    The resulting DataFrame is uploaded to Snowflake as a permanent table with a \n",
    "    unique name in the current database and schema. The table name is returned as a\n",
    "    Snowpark DataFrame object.\n",
    "\n",
    "    Args:\n",
    "        curr_session: A valid Snowpark session object.\n",
    "        partition_counts (int, optional): Number of unique partition values \n",
    "            for the LOCATION_ID column. Defaults to 2.\n",
    "\n",
    "    Returns:\n",
    "        Snowpark DataFrame: A reference to the saved table in Snowflake.\n",
    "    \n",
    "    \"\"\"\n",
    "    # Generate synthetic data\n",
    "    cols = [\"X1\", \"X2\", \"X3\", \"X4\"]\n",
    "    X, _ = make_regression(n_samples=1000, n_features=4, noise=0.1, random_state=0)\n",
    "    df = pd.DataFrame(X, columns=cols)\n",
    "    df[\"LOCATION_ID\"] = np.arange(len(df)) % partition_counts\n",
    "\n",
    "    # Add random dates between 2020-01-01 and 2023-01-01\n",
    "    date_range = pd.date_range(\"2020-01-01\", \"2023-01-01\", freq=\"D\")\n",
    "    df[\"DATE\"] = np.random.choice(date_range, size=len(df))\n",
    "\n",
    "    # Create Snowpark DataFrame and save to a uniquely named table\n",
    "    snowpark_df = curr_session.create_dataframe(df)\n",
    "    table_name = f\"{curr_session.get_current_database()}.{curr_session.get_current_schema()}.mmt_test_{uuid.uuid4().hex.upper()}\"\n",
    "    snowpark_df.write.mode(\"overwrite\").save_as_table(table_name)\n",
    "\n",
    "    return curr_session.table(table_name)\n"
   ],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "62be28ec-bd49-4dfd-ae5d-b11df1b64d5b",
   "metadata": {
    "name": "cell8",
    "collapsed": false
   },
   "source": [
    "## Step 2: Invoke MMT API & Monitor MMT Training Run\n",
    "\n",
    "User can optionally choose to scale up the cluster to multi-nodes prior to run the many model training. "
   ]
  },
  {
   "cell_type": "code",
   "id": "c3100c37-2f17-4348-a884-09bfdae0666d",
   "metadata": {
    "language": "python",
    "name": "scale_to_multi_nodes"
   },
   "outputs": [],
   "source": [
    "# Optional step to scale to multiple nodes for speed up overall many model trainings.\n",
    "# from snowflake.ml.runtime_cluster import cluster_manager\n",
    "# TOTAL_NODES=5\n",
    "# cluster_manager.scale_cluster(expected_cluster_size=TOTAL_NODES, notebook_name=NOTEBOOK_NAME, options={\"block_until_min_cluster_size\": 2})"
   ],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f5a23617-d3ba-4bf6-af3e-56a38c31f747",
   "metadata": {
    "language": "python",
    "name": "async_many_model_train"
   },
   "outputs": [],
   "source": "from snowflake.ml.modeling.distributors.many_model_training.many_model_trainer import (\n    ManyModelTrainer,\n)\nfrom snowflake.ml.modeling.distributors.many_model_training.entities import (\n    ExecutionOptions,\n)\n\nsnowpark_df = _init_snowpark_df(session)\nmodel_name=\"my_mmt_model\"\nmodel_version_v1 = \"v1\"\ntrainer = ManyModelTrainer(\n    training_func=user_training_func,\n    model_name=model_name,\n    model_version=model_version_v1,\n    stage_name=stage_name,\n    # execution_options is optional. When running in a multi-node setting, it's recommended setting use_head_node=False to exclude head node from doing actual training, this improves overall MMT training reliability.\n    # execution_options=ExecutionOptions(use_head_node=False)\n)\n\ntrainer.run(snowpark_dataframe=snowpark_df, partition_by=\"LOCATION_ID\")\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "46da52a6-40e5-415c-86d3-3a27c82ef9be",
   "metadata": {
    "language": "python",
    "name": "cell1"
   },
   "outputs": [],
   "source": [
    "# Depending on the workload size, MMT can take an arbitrarily long time to complete. This call is \n",
    "# interruptible—you can cancel the cell, run other commands, and return later. Interrupting this call \n",
    "# does not affect the actual MMT run. The show_progress function will automatically reflect the current \n",
    "# status of the MMT run.\n",
    "trainer.show_progress()"
   ],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "23240e44-dc8a-4330-8784-fe818b852bb0",
   "metadata": {
    "language": "python",
    "name": "inspect_result"
   },
   "outputs": [],
   "source": [
    "# Mapping between training status and corresponding SingleModelTrainingDetails objects.\n",
    "# trainer.get_progress() returns a dictionary where the keys are training statuses\n",
    "# and the values are lists of SingleModelTrainingDetails objects associated with that status.\n",
    "#\n",
    "# Example output:\n",
    "# {\n",
    "#     \"PENDING\": [SingleModelTrainingDetails],\n",
    "#     \"RUNNING\": [SingleModelTrainingDetails],\n",
    "#     \"FAILED\": [SingleModelTrainingDetails],\n",
    "#     \"DONE\": [SingleModelTrainingDetails, SingleModelTrainingDetails],\n",
    "#     \"INTERNAL_ERROR\": [SingleModelTrainingDetails]\n",
    "# }\n",
    "\n",
    "trainer.get_progress()"
   ],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "dca32f98-eb9f-4ead-9cdb-5b738bb31735",
   "metadata": {
    "language": "python",
    "name": "cell2"
   },
   "outputs": [],
   "source": [
    "# Mapping between partition_id and corresponding SingleModelTrainingDetails object.\n",
    "# trainer.model_trainings returns a dictionary where the keys are partition IDs (strings),\n",
    "# and the values are SingleModelTrainingDetails objects representing the training detail for each partition.\n",
    "#\n",
    "# Example output:\n",
    "# {\n",
    "#     \"partition_id1\": SingleModelTrainingDetails,\n",
    "#     \"partition_id2\": SingleModelTrainingDetails,\n",
    "#     \"partition_id3\": SingleModelTrainingDetails,\n",
    "#     ...\n",
    "# }\n",
    "\n",
    "trainer.model_trainings"
   ],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b343930b-720c-443b-8ccd-64b67064548b",
   "metadata": {
    "language": "python",
    "name": "cell3"
   },
   "outputs": [],
   "source": [
    "for partition_id, training_detail in trainer.model_trainings.items():\n",
    "    print(trainer.model_trainings[partition_id].logs)\n",
    "    assert isinstance(trainer.model_trainings[partition_id].model, xgb.XGBRegressor)"
   ],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "72091481-5ce5-48e0-a2aa-565b5fe9a08c",
   "metadata": {
    "language": "python",
    "name": "inspect_stage"
   },
   "outputs": [],
   "source": [
    "# Ideally you will not need to interact with the stage at all. This is more of a FYI how your stage is being used\n",
    "# to persist the model and other artifacts.\n",
    "session.sql(f\"ls @{stage_name}\").collect()"
   ],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "bd08539f-53b7-4906-aaf5-30e5432e9b7a",
   "metadata": {
    "name": "cell5",
    "collapsed": false
   },
   "source": [
    "## Step 3: Troubleshooting Failed Runs\n",
    "\n",
    "Training functions can fail for various reasons. Below are some common causes:\n",
    "\n",
    "- **User Code Errors**  \n",
    "  Bugs or issues in the user-defined training function can cause failures.\n",
    "\n",
    "- **Infrastructure Issues**  \n",
    "  An *Out-of-Memory (OOM)* error occurs when the training function consumes more memory than the node can provide.\n",
    "\n",
    "- **Unexpected Node Failures**  \n",
    "  In some cases, a node might crash unexpectedly.\n",
    "\n",
    "---\n",
    "\n",
    "### Handling OOM and Node Failures\n",
    "\n",
    "When an OOM error or fatal node failure occurs, the **MMT API will not automatically retry** the training function. Instead, it will mark the corresponding partition ID run as **`INTERNAL_ERROR`**. If a worker node crashes, logs might not be captured, making debugging more difficult.\n",
    "\n",
    "For all other failure scenarios (including OOM errors), MMT provides:\n",
    "- A **detailed error message**  \n",
    "- A **stack trace** to help diagnose and fix the issue\n",
    "\n",
    "---\n",
    "\n",
    "### Retry Logic for Non-Fatal Errors\n",
    "\n",
    "If the failure is not considered fatal (e.g., transient issues), MMT will automatically retry the training function with **exponential backoff**. This mechanism allows transient issues to resolve before the function ultimately fails.\n",
    "\n",
    "**Retry Attempts:**\n",
    "1. **First retry**: Wait for 2 seconds (`initial_delay`)\n",
    "2. **Second retry**: Wait for 4 seconds (2 * `initial_delay`)\n",
    "3. **Third retry**: Wait for 8 seconds (2^2 * `initial_delay`)\n",
    "4. **Fourth retry**: Wait for 16 seconds (2^3 * `initial_delay`)\n",
    "5. **Final retry**: No delay — if it fails again, an exception is raised\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "a2bfe1a6-79f5-40c1-b758-4c056d48a381",
   "metadata": {
    "language": "python",
    "name": "cell4"
   },
   "outputs": [],
   "source": "def user_func_error(data_connector, **kwargs):\n    pandas_df = data_connector.to_pandas()\n\n    NUMERICAL_COLUMNS = [\"X1\", \"X2\", \"X3\"]\n    LABEL_COLUMNS = [\"X4\"]\n    model = xgb.XGBRegressor()\n\n    # INTENTIONAL USER-CODE FAILURE: fitss function does not exist\n    model.fitss(pandas_df[NUMERICAL_COLUMNS], pandas_df[LABEL_COLUMNS])    \n    \n    return model\n\n\nmodel_name=\"my_mmt_model\"\nmodel_version = \"v2\"\n\ntrainer = ManyModelTrainer(\n    training_func=user_func_error,\n    model_name=model_name,\n    model_version=model_version,\n    stage_name=stage_name,\n)\n\ntrainer.run(\n    snowpark_dataframe=snowpark_df,\n    partition_by=\"LOCATION_ID\",    \n)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "dbb4dcaf-1184-4775-95d8-7be33fa585ae",
   "metadata": {
    "language": "python",
    "name": "cell9"
   },
   "outputs": [],
   "source": [
    "# MMT will retry the user-function up to five times and then fail.\n",
    "\n",
    "# Helper function for illustartion purposes of getting failed logs.\n",
    "import time\n",
    "while True:\n",
    "    if \"FAILED\" in trainer.get_progress():\n",
    "        break\n",
    "    time.sleep(1)\n",
    "\n",
    "# User can optionally choose to cancel the entire MMT run when at least one failed run is detected.\n",
    "# trainer.cancel()\n",
    "\n",
    "# Show first failed partition logs\n",
    "trainer.get_progress()[\"FAILED\"][0].logs"
   ],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "48583e54-0b89-4598-b599-c301f29373d5",
   "metadata": {
    "name": "cell10",
    "collapsed": false
   },
   "source": [
    "## Step 4: Inspecting Models and Logs After Notebook/Session Closure\n",
    "\n",
    "After spending significant time training multiple models, you may want to temporarily shut down your notebook to save costs. However, you might wonder how to recover trained models and logs later. Snowflake provides an API specifically for this purpose.\n",
    "\n",
    "With the `ReadOnlyManyModelTrainer`, you can restore previously trained models and access their logs and metadata, even after the notebook session has ended. While you can interact with most APIs, note that the `.run()` method is not available because model names and versions are immutable — once a model has been trained, you cannot re-run the same training job for that model/version.\n",
    "\n",
    "Below we show how you can retrieve and inspect a previously trained model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "94810768-6cba-440c-b521-d88227531226",
   "metadata": {
    "language": "python",
    "name": "cell11"
   },
   "outputs": [],
   "source": "\nfrom snowflake.ml.modeling.distributors.many_model_training.read_only_many_model_trainer import (\n    ReadOnlyManyModelTrainer,\n)\n\n# Restore the trained model using the model name, version, and stage name\nread_only_trainer = ReadOnlyManyModelTrainer.restore_from(\n    model_name=model_name, \n    model_version=\"v1\", \n    stage_name=stage_name\n)\n\n# Check the status of the trained model\nmodel_status = read_only_trainer.status\nprint(model_status)\n\n# You can also access other APIs to inspect logs and metadata (except for .run())\nread_only_trainer.get_progress()\n",
   "execution_count": null
  }
 ]
}