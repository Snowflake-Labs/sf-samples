{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a6496bf-b928-4069-adfa-083299a21a13",
   "metadata": {},
   "source": [
    "### Set up Snowpark Session\n",
    "\n",
    "See [Configure Connections](https://docs.snowflake.com/developer-guide/snowflake-cli/connecting/configure-connections#define-connections)\n",
    "for information on how to define default Snowflake connection(s) in a config.toml\n",
    "file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f9880be-f3c2-4100-a35c-0e72595f3bdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<snowflake.snowpark.session.Session: account=\"mlplatformtest\", role=\"ENGINEER\", database=\"DHUNG_DEV\", schema=\"DHUNG_DEV\", warehouse=\"DHUNG_WH\">\n"
     ]
    }
   ],
   "source": [
    "from snowflake.snowpark import Session, Row\n",
    "\n",
    "# Requires valid ~/.snowflake/config.toml file\n",
    "session = Session.builder.getOrCreate()\n",
    "print(session)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa9a253-38c5-4641-a01a-eaf899d24199",
   "metadata": {},
   "source": [
    "#### Set up Snowflake resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "46e5fad4-748b-4390-b4fa-748a10547835",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_name = \"HEADLESS_DEMO\"\n",
    "session.sql(f\"CREATE SCHEMA IF NOT EXISTS {schema_name}\").collect()\n",
    "session.use_schema(schema_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7fb47093-9b23-492d-a135-5722729a0c7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(status='DEMO_POOL_GPU already exists, statement succeeded.')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create compute pool\n",
    "def create_compute_pool(name: str, instance_family: str, min_nodes: int = 1, max_nodes: int = 10) -> list[Row]:\n",
    "    query = f\"\"\"\n",
    "        CREATE COMPUTE POOL IF NOT EXISTS {name}\n",
    "            MIN_NODES = {min_nodes}\n",
    "            MAX_NODES = {max_nodes}\n",
    "            INSTANCE_FAMILY = {instance_family}\n",
    "    \"\"\"\n",
    "    return session.sql(query).collect()\n",
    "\n",
    "compute_pool = \"DEMO_POOL_GPU\"\n",
    "create_compute_pool(compute_pool, \"GPU_NV_S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "84804c16-a359-4e5b-9037-207cc9675b75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(status='LOAN_APPLICATIONS already exists, statement succeeded.')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate synthetic data\n",
    "def generate_data(table_name: str, num_rows: int, replace: bool = False) -> list[Row]:\n",
    "    query = f\"\"\"\n",
    "        CREATE{\" OR REPLACE\" if replace else \"\"} TABLE{\"\" if replace else \" IF NOT EXISTS\"} {table_name} AS\n",
    "        SELECT \n",
    "            ROW_NUMBER() OVER (ORDER BY RANDOM()) as application_id,\n",
    "            ROUND(NORMAL(40, 10, RANDOM())) as age,\n",
    "            ROUND(NORMAL(65000, 20000, RANDOM())) as income,\n",
    "            ROUND(NORMAL(680, 50, RANDOM())) as credit_score,\n",
    "            ROUND(NORMAL(5, 2, RANDOM())) as employment_length,\n",
    "            ROUND(NORMAL(25000, 8000, RANDOM())) as loan_amount,\n",
    "            ROUND(NORMAL(35, 10, RANDOM()), 2) as debt_to_income,\n",
    "            ROUND(NORMAL(5, 2, RANDOM())) as number_of_credit_lines,\n",
    "            GREATEST(0, ROUND(NORMAL(1, 1, RANDOM()))) as previous_defaults,\n",
    "            ARRAY_CONSTRUCT(\n",
    "                'home_improvement', 'debt_consolidation', 'business', 'education',\n",
    "                'major_purchase', 'medical', 'vehicle', 'other'\n",
    "            )[UNIFORM(1, 8, RANDOM())] as loan_purpose,\n",
    "            RANDOM() < 0.15 as is_default,\n",
    "            TIMEADD(\"MINUTE\", UNIFORM(-525600, 0, RANDOM()), CURRENT_TIMESTAMP()) as created_at\n",
    "        FROM TABLE(GENERATOR(rowcount => {num_rows}))\n",
    "        ORDER BY created_at;\n",
    "    \"\"\"\n",
    "    return session.sql(query).collect()\n",
    "\n",
    "table_name = \"loan_applications\"\n",
    "generate_data(table_name, 1e5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6783667-50b2-4f41-a72c-3969e431f858",
   "metadata": {},
   "source": [
    "### Prepare Model Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f6ba7a8b-a944-4149-a46e-48196350b70e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pickle\n",
    "from time import perf_counter\n",
    "from typing import Literal, Optional\n",
    "\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import accuracy_score, classification_report, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "\n",
    "\n",
    "from snowflake.ml.data.data_connector import DataConnector\n",
    "from snowflake.ml.registry import Registry as ModelRegistry\n",
    "from snowflake.ml.utils.connection_params import SnowflakeLoginOptions\n",
    "from snowflake.snowpark import Session\n",
    "\n",
    "\n",
    "def create_data_connector(session, table_name: str) -> DataConnector:\n",
    "    \"\"\"Load data from Snowflake table\"\"\"\n",
    "    # Example query - modify according to your schema\n",
    "    query = f\"\"\"\n",
    "    SELECT\n",
    "        age,\n",
    "        income,\n",
    "        credit_score,\n",
    "        employment_length,\n",
    "        loan_amount,\n",
    "        debt_to_income,\n",
    "        number_of_credit_lines,\n",
    "        previous_defaults,\n",
    "        loan_purpose,\n",
    "        is_default\n",
    "    FROM {table_name}\n",
    "    \"\"\"\n",
    "    sp_df = session.sql(query)\n",
    "    return DataConnector.from_dataframe(sp_df)\n",
    "\n",
    "\n",
    "def build_pipeline(**model_params) -> Pipeline:\n",
    "    \"\"\"Create pipeline with preprocessors and model\"\"\"\n",
    "    # Define column types\n",
    "    categorical_cols = [\"LOAN_PURPOSE\"]\n",
    "    numerical_cols = [\n",
    "        \"AGE\",\n",
    "        \"INCOME\",\n",
    "        \"CREDIT_SCORE\",\n",
    "        \"EMPLOYMENT_LENGTH\",\n",
    "        \"LOAN_AMOUNT\",\n",
    "        \"DEBT_TO_INCOME\",\n",
    "        \"NUMBER_OF_CREDIT_LINES\",\n",
    "        \"PREVIOUS_DEFAULTS\",\n",
    "    ]\n",
    "\n",
    "    # Numerical preprocessing pipeline\n",
    "    numeric_transformer = Pipeline(\n",
    "        steps=[\n",
    "            (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "            (\"scaler\", StandardScaler()),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Categorical preprocessing pipeline\n",
    "    categorical_transformer = Pipeline(\n",
    "        steps=[\n",
    "            (\"imputer\", SimpleImputer(strategy=\"constant\", fill_value=\"missing\")),\n",
    "            (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Combine transformers\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"num\", numeric_transformer, numerical_cols),\n",
    "            (\"cat\", categorical_transformer, categorical_cols),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Define model parameters\n",
    "    default_params = {\n",
    "        \"objective\": \"binary:logistic\",\n",
    "        \"eval_metric\": \"auc\",\n",
    "        \"max_depth\": 6,\n",
    "        \"learning_rate\": 0.1,\n",
    "        \"n_estimators\": 100,\n",
    "        \"subsample\": 0.8,\n",
    "        \"colsample_bytree\": 0.8,\n",
    "        \"random_state\": 42,\n",
    "    }\n",
    "    model = xgb.XGBClassifier(**(model_params or default_params))\n",
    "\n",
    "    return Pipeline([(\"preprocessor\", preprocessor), (\"classifier\", model)])\n",
    "\n",
    "\n",
    "def evaluate_model(model: Pipeline, X_test: pd.DataFrame, y_test: pd.DataFrame):\n",
    "    \"\"\"Evaluate model performance\"\"\"\n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    # Calculate metrics\n",
    "    metrics = {\n",
    "        \"accuracy\": accuracy_score(y_test, y_pred),\n",
    "        \"roc_auc\": roc_auc_score(y_test, y_pred_proba),\n",
    "        \"classification_report\": classification_report(y_test, y_pred),\n",
    "    }\n",
    "\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def save_to_registry(\n",
    "    session: Session,\n",
    "    model: Pipeline,\n",
    "    model_name: str,\n",
    "    metrics: dict,\n",
    "    sample_input_data: pd.DataFrame,\n",
    "):\n",
    "    \"\"\"Save model and artifacts to Snowflake Model Registry\"\"\"\n",
    "    # Initialize model registry\n",
    "    registry = ModelRegistry(session)\n",
    "\n",
    "    # Save to registry\n",
    "    registry.log_model(\n",
    "        model=model,\n",
    "        model_name=model_name,\n",
    "        metrics=metrics,\n",
    "        sample_input_data=sample_input_data[:5],\n",
    "        conda_dependencies=[\"xgboost\"],\n",
    "    )\n",
    "\n",
    "\n",
    "def train(session: Session, source_data: str, save_mode: Literal[\"local\", \"registry\"] = \"local\", output_dir: Optional[str] = None, **kwargs):\n",
    "    # Load data\n",
    "    dc = create_data_connector(session, table_name=source_data)\n",
    "    print(\"Loading data...\", end=\"\", flush=True)\n",
    "    start = perf_counter()\n",
    "    df = dc.to_pandas()\n",
    "    elapsed = perf_counter() - start\n",
    "    print(f\" done! Loaded {len(df)} rows, elapsed={elapsed:.3f}s\")\n",
    "\n",
    "    # Split data\n",
    "    X = df.drop(\"IS_DEFAULT\", axis=1)\n",
    "    y = df[\"IS_DEFAULT\"]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42\n",
    "    )\n",
    "\n",
    "    # Train model\n",
    "    model = build_pipeline()\n",
    "    print(\"Training model...\", end=\"\")\n",
    "    start = perf_counter()\n",
    "    model.fit(X_train, y_train)\n",
    "    elapsed = perf_counter() - start\n",
    "    print(f\" done! Elapsed={elapsed:.3f}s\")\n",
    "\n",
    "    # Evaluate model\n",
    "    print(\"Evaluating model...\", end=\"\")\n",
    "    start = perf_counter()\n",
    "    metrics = evaluate_model(\n",
    "        model,\n",
    "        X_test,\n",
    "        y_test,\n",
    "    )\n",
    "    elapsed = perf_counter() - start\n",
    "    print(f\" done! Elapsed={elapsed:.3f}s\")\n",
    "\n",
    "    # Print evaluation results\n",
    "    print(\"\\nModel Performance Metrics:\")\n",
    "    print(f\"Accuracy: {metrics['accuracy']:.4f}\")\n",
    "    print(f\"ROC AUC: {metrics['roc_auc']:.4f}\")\n",
    "    # Uncomment below for full classification report\n",
    "    # print(\"\\nClassification Report:\")\n",
    "    # print(metrics[\"classification_report\"])\n",
    "\n",
    "    start = perf_counter()\n",
    "    if save_mode == \"local\":\n",
    "        # Save model locally\n",
    "        print(\"Saving model to disk...\", end=\"\")\n",
    "        output_dir = output_dir or '.'\n",
    "        model_subdir = os.environ.get(\"SNOWFLAKE_SERVICE_NAME\", \"output\")\n",
    "        model_dir = os.path.join(output_dir, model_subdir) if not output_dir.endswith(model_subdir) else output_dir\n",
    "        os.makedirs(model_dir, exist_ok=True)\n",
    "        with open(os.path.join(model_dir, \"model.pkl\"), \"wb\") as f:\n",
    "            pickle.dump(model, f)\n",
    "        with open(os.path.join(model_dir, \"metrics.json\"), \"w\") as f:\n",
    "            json.dump(metrics, f, indent=2)\n",
    "    elif save_mode == \"registry\":\n",
    "        # Save model to registry\n",
    "        print(\"Logging model to Model Registry...\", end=\"\")\n",
    "        save_to_registry(\n",
    "            session,\n",
    "            model=model,\n",
    "            model_name=\"loan_default_predictor\",\n",
    "            metrics=metrics,\n",
    "            sample_input_data=X_train,\n",
    "        )\n",
    "    elapsed = perf_counter() - start\n",
    "    print(f\" done! Elapsed={elapsed:.3f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "897e6b37-0cbc-4878-8e06-a6dae60b6249",
   "metadata": {},
   "source": [
    "### Run training locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ef1c3106-7e4a-4c7c-82e1-0fd1f6de417f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DataConnector.from_dataframe() is in private preview since 1.6.0. Do not use it in production. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data... done! Loaded 100000 rows, elapsed=1.212s\n",
      "Training model... done! Elapsed=1.475s\n",
      "Evaluating model... done! Elapsed=0.044s\n",
      "\n",
      "Model Performance Metrics:\n",
      "Accuracy: 0.4955\n",
      "ROC AUC: 0.4961\n",
      "Saving model to disk... done! Elapsed=0.003s\n"
     ]
    }
   ],
   "source": [
    "train(session, table_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e84a9f41-1113-47f5-9041-a8cf1f904a9f",
   "metadata": {},
   "source": [
    "### Train with GPU on remote SPCS instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c2940e5a-4f85-46d9-9b49-aed6ac79fb53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from snowflake.ml.jobs import decorators as jd, manager as jm\n",
    "\n",
    "@jd.remote_ml_job(compute_pool, stage_name=\"payload_stage\")\n",
    "def train_remote(source_data: str, save_mode: str = \"local\", output_dir: str = None):\n",
    "    # Retrieve session from SPCS service context\n",
    "    from snowflake.ml.utils import connection_params\n",
    "    session = Session.builder.configs(connection_params.SnowflakeLoginOptions()).create()\n",
    "\n",
    "    # Run training script\n",
    "    train(session, source_data, save_mode, output_dir)\n",
    "\n",
    "train_job = train_remote(table_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3eb1ad66-851f-4fdb-887c-4dcaaf5bf2c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ML_JOB_065D9277_5300_4251_8766_1561775DD403\n",
      "PENDING\n"
     ]
    }
   ],
   "source": [
    "print(train_job.id)\n",
    "print(train_job.status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1909e552-d289-4bf4-8840-926d99295acb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SnowflakeLoginOptions() is in private preview since 0.2.0. Do not use it in production. \n",
      "DataConnector.from_dataframe() is in private preview since 1.6.0. Do not use it in production. \n",
      "2024-12-10 16:45:12,750\tINFO worker.py:1777 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "2024-12-10 16:45:14,407\tINFO streaming_executor.py:108 -- Starting execution of Dataset. Full logs are in /tmp/ray/session_2024-12-10_16-45-11_553859_1/logs/ray-data\n",
      "2024-12-10 16:45:14,407\tINFO streaming_executor.py:109 -- Execution plan of Dataset: InputDataBuffer[Input] -> TaskPoolMapOperator[ReadResultSetDataSource]\n",
      "Running 0: 0.00 row [00:00, ? row/s]\n",
      "Running. Resources: 6/6 CPU, 0/1 GPU, 1.5GB/4.0GB object_store_memory (pending: 0 CPU, 0 GPU): : 0.00 row [00:01, ? row/s]\n",
      "- ReadResultSetDataSource: 6 active, 194 queued ðŸš§, [cpu: 6.0, objects: 1.5GB]: : 0.00 row [00:01, ? row/s]\u001b[A\n",
      "                                                                                                                          \n",
      "\u001b[A\u001b[36m(ReadResultSetDataSource pid=179)\u001b[0m SnowflakeLoginOptions() is in private preview since 0.2.0. Do not use it in production. \n",
      "Running. Resources: 6/6 CPU, 0/1 GPU, 1.5GB/4.0GB object_store_memory (pending: 0 CPU, 0 GPU): : 0.00 row [00:01, ? row/s]\n",
      "Running. Resources: 5/6 CPU, 0/1 GPU, 640.0MB/4.0GB object_store_memory (pending: 0 CPU, 0 GPU): : 0.00 row [00:02, ? row/s]\n",
      "- ReadResultSetDataSource: 6 active, 192 queued ðŸš§, [cpu: 6.0, objects: 768.0MB]: : 0.00 row [00:02, ? row/s]\u001b[A\n",
      "- ReadResultSetDataSource: 6 active, 192 queued ðŸš§, [cpu: 6.0, objects: 768.0MB]: : 0.00 row [00:02, ? row/s]\u001b[A\n",
      "                                                                                                                            \n",
      "âœ”ï¸  Dataset execution finished in 2.18 seconds: : 0.00 row [00:02, ? row/s]                                  \n",
      "\n",
      "- ReadResultSetDataSource: 6 active, 192 queued ðŸš§, [cpu: 6.0, objects: 768.0MB]: : 0.00 row [00:02, ? row/s]\u001b[A\n",
      "- ReadResultSetDataSource: 6 active, 192 queued ðŸš§, [cpu: 6.0, objects: 768.0MB]: : 0.00 row [00:02, ? row/s]\u001b[A\n",
      "- ReadResultSetDataSource: 6 active, 192 queued ðŸš§, [cpu: 6.0, objects: 768.0MB]: : 0.00 row [00:02, ? row/s]\n",
      "2024-12-10 16:45:16,596\tINFO streaming_executor.py:108 -- Starting execution of Dataset. Full logs are in /tmp/ray/session_2024-12-10_16-45-11_553859_1/logs/ray-data\n",
      "2024-12-10 16:45:16,596\tINFO streaming_executor.py:109 -- Execution plan of Dataset: InputDataBuffer[Input] -> TaskPoolMapOperator[ReadResultSetDataSource]\n",
      "Running 0: 0.00 row [00:00, ? row/s]\n",
      "Running. Resources: 1/6 CPU, 0/1 GPU, 75.7KB/4.0GB object_store_memory (pending: 0 CPU, 0 GPU):  19%|â–ˆâ–‰        | 70.2k/369k [00:01<00:04, 65.5k row/s]\n",
      "- ReadResultSetDataSource: 6 active, 151 queued ðŸš§, [cpu: 6.0, objects: 454.2KB]: : 0.00 row [00:01, ? row/s]\u001b[A\n",
      "- ReadResultSetDataSource: 6 active, 151 queued ðŸš§, [cpu: 6.0, objects: 454.2KB]:   0%|          | 0.00/292k [00:01<?, ? row/s]\u001b[A\n",
      "- ReadResultSetDataSource: 6 active, 151 queued ðŸš§, [cpu: 6.0, objects: 454.2KB]:  24%|â–ˆâ–ˆâ–       | 70.2k/292k [00:01<00:03, 66.1k row/s]\u001b[A\n",
      "                                                                                                                                                      \n",
      "âœ”ï¸  Dataset execution finished in 1.59 seconds: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100k/100k [00:01<00:00, 62.9k row/s]                                   \n",
      "\n",
      "- ReadResultSetDataSource: 6 active, 151 queued ðŸš§, [cpu: 6.0, objects: 454.2KB]:  24%|â–ˆâ–ˆâ–       | 70.2k/292k [00:01<00:03, 66.1k row/s]\u001b[A\n",
      "- ReadResultSetDataSource: 0 active, 0 queued, [cpu: 0.0, objects: 0.0B]:  24%|â–ˆâ–ˆâ–       | 70.2k/292k [00:01<00:03, 66.1k row/s]        \u001b[A\n",
      "- ReadResultSetDataSource: 0 active, 0 queued, [cpu: 0.0, objects: 0.0B]:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 70.2k/100k [00:01<00:00, 66.1k row/s]\u001b[A\n",
      "- ReadResultSetDataSource: 0 active, 0 queued, [cpu: 0.0, objects: 0.0B]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100k/100k [00:01<00:00, 68.3k row/s] \u001b[A\n",
      "- ReadResultSetDataSource: 0 active, 0 queued, [cpu: 0.0, objects: 0.0B]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100k/100k [00:01<00:00, 68.3k row/s]\u001b[A\n",
      "- ReadResultSetDataSource: 0 active, 0 queued, [cpu: 0.0, objects: 0.0B]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100k/100k [00:01<00:00, 67.8k row/s]\n",
      "Loading data... done! Loaded 100000 rows, elapsed=7.412s\n",
      "Training model... done! Elapsed=2.241s\n",
      "Evaluating model... done! Elapsed=0.084s\n",
      "\n",
      "Model Performance Metrics:\n",
      "Accuracy: 0.4940\n",
      "ROC AUC: 0.4957\n",
      "Saving model to disk... done! Elapsed=0.002s\n",
      "\u001b[36m(ReadResultSetDataSource pid=183)\u001b[0m SnowflakeLoginOptions() is in private preview since 0.2.0. Do not use it in production. \u001b[32m [repeated 5x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_job.wait()\n",
    "train_job.show_logs()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe67e0b-e73f-4c90-8a22-f7b7fce0bfe2",
   "metadata": {},
   "source": [
    "### Run concurrent training jobs on SPCS\n",
    "\n",
    "Suppose we want to train multiple models on different datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5c1be7c9-1512-48f5-adc0-630102e4e41e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating datasets\n",
      "Generated datasets: ['loan_applications_0', 'loan_applications_1', 'loan_applications_2', 'loan_applications_3', 'loan_applications_4', 'loan_applications_5', 'loan_applications_6', 'loan_applications_7', 'loan_applications_8', 'loan_applications_9']\n",
      "Starting training jobs\n",
      "Started 10 training jobs\n"
     ]
    }
   ],
   "source": [
    "datasets = []\n",
    "print(\"Generating datasets\")\n",
    "for i in range(10):\n",
    "    dataset = f\"loan_applications_{i}\"\n",
    "    generate_data(dataset, 1e6)\n",
    "    datasets.append(dataset)\n",
    "print(f\"Generated datasets: {datasets}\")\n",
    "    \n",
    "print(\"Starting training jobs\")\n",
    "train_jobs = []\n",
    "for ds in datasets:\n",
    "    train_jobs.append(train_remote(ds))\n",
    "print(f\"Started {len(train_jobs)} training jobs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "75619ff2-70fb-4237-b183-25d6627b902a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------------------------------------------------\n",
      "|\"name\"                                       |\"owner\"   |\"status\"  |\"created_on\"                      |\"compute_pool\"  |\n",
      "-------------------------------------------------------------------------------------------------------------------------\n",
      "|ML_JOB_2B0A59CE_66D6_4302_9A98_5E69091BA566  |ENGINEER  |PENDING   |2024-12-09 19:36:45.405000-08:00  |DEMO_POOL_GPU   |\n",
      "|ML_JOB_3E9E0205_FF7D_4FFA_A476_20867AEC71CE  |ENGINEER  |PENDING   |2024-12-09 19:36:33.113000-08:00  |DEMO_POOL_GPU   |\n",
      "|ML_JOB_60D354BB_0682_45D9_9A10_85B55B1D3020  |ENGINEER  |PENDING   |2024-12-09 19:36:30.166000-08:00  |DEMO_POOL_GPU   |\n",
      "|ML_JOB_415C98D8_F06D_4974_AC66_19DEC78C70E0  |ENGINEER  |RUNNING   |2024-12-09 19:36:27.502000-08:00  |DEMO_POOL_GPU   |\n",
      "|ML_JOB_54184E49_FDFA_4B8E_9035_EB550756F616  |ENGINEER  |RUNNING   |2024-12-09 19:36:21.060000-08:00  |DEMO_POOL_GPU   |\n",
      "|ML_JOB_19425B9F_FF41_4BFC_BF32_C3399CD80700  |ENGINEER  |RUNNING   |2024-12-09 19:36:18.271000-08:00  |DEMO_POOL_GPU   |\n",
      "|ML_JOB_5404E140_4AD3_4B16_9556_FA608C7795A6  |ENGINEER  |DONE      |2024-12-09 19:35:14.897000-08:00  |DEMO_POOL_GPU   |\n",
      "|ML_JOB_6D26D65F_BD88_40FA_90FB_D0E7FC2F8772  |ENGINEER  |DONE      |2024-12-09 19:23:32.010000-08:00  |DEMO_POOL_GPU   |\n",
      "|ML_JOB_12FE03F1_5177_4E06_ACC3_A357A26D6DB5  |ENGINEER  |DONE      |2024-12-09 19:23:17.799000-08:00  |DEMO_POOL_GPU   |\n",
      "|ML_JOB_34A6D11E_20E0_4F35_BD02_C096378E4C1C  |ENGINEER  |DONE      |2024-12-09 19:23:12.247000-08:00  |DEMO_POOL_GPU   |\n",
      "-------------------------------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "jm.list_jobs().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "76d8d926-6277-42a0-b8c4-db7bb225c4b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Local: Loaded (100000000, 12) in 163.431s\n",
      "Remote: Loaded (100000000, 12) in 88.938s\n"
     ]
    }
   ],
   "source": [
    "from time import perf_counter\n",
    "\n",
    "big_table_name = \"demo_data_100m\"\n",
    "generate_data(big_table_name, 1e8)\n",
    "\n",
    "@jd.remote_ml_job(compute_pool, stage_name=\"payload_stage\")\n",
    "def load_data(table_name: str):\n",
    "    from snowflake.ml.utils import connection_params\n",
    "    session = Session.builder.configs(connection_params.SnowflakeLoginOptions()).create()\n",
    "    \n",
    "    start = perf_counter()\n",
    "    dc = DataConnector.from_dataframe(session.table(table_name))\n",
    "    pd_df = dc.to_pandas()\n",
    "    elapsed = perf_counter() - start\n",
    "    print(f\"Remote: Loaded {pd_df.shape} in {elapsed:.3f}s\")\n",
    "\n",
    "load_data_job = load_data(big_table_name)\n",
    "\n",
    "start = perf_counter()\n",
    "dc = DataConnector.from_dataframe(session.table(big_table_name))\n",
    "pd_df = dc.to_pandas()\n",
    "elapsed = perf_counter() - start\n",
    "print(f\"Local: Loaded {pd_df.shape} in {elapsed:.3f}s\")\n",
    "\n",
    "load_data_job.wait()\n",
    "print(load_data_job.get_logs().split(\"\\n\")[-3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "21e6f178-e80d-4101-82f1-a4287aac4021",
   "metadata": {},
   "outputs": [],
   "source": [
    "# session.sql(f\"DROP SCHEMA {schema_name}\").collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
