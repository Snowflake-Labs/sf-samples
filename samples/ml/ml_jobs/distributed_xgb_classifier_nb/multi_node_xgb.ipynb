{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a6496bf-b928-4069-adfa-083299a21a13",
   "metadata": {},
   "source": [
    "### Set up Snowpark Session\n",
    "\n",
    "See [Configure Connections](https://docs.snowflake.com/developer-guide/snowflake-cli/connecting/configure-connections#define-connections)\n",
    "for information on how to define default Snowflake connection(s) in a config.toml\n",
    "file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f9880be-f3c2-4100-a35c-0e72595f3bdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<snowflake.snowpark.session.Session: account=\"NOTEBOOK_MLTEST\", role=\"SYSADMIN\", database=\"HEADLESS_STARTER_DB\", schema=\"HEADLESS_STARTER_SCHEMA\", warehouse=\"ST_WH\">\n"
     ]
    }
   ],
   "source": [
    "from snowflake.snowpark import Session, Row\n",
    "\n",
    "# Requires valid ~/.snowflake/config.toml file\n",
    "session = Session.builder.getOrCreate()\n",
    "print(session)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa9a253-38c5-4641-a01a-eaf899d24199",
   "metadata": {},
   "source": [
    "#### Set up Snowflake resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "46e5fad4-748b-4390-b4fa-748a10547835",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIONAL: Uncomment below to select a database and schema to use\n",
    "# session.use_database(\"temp\")\n",
    "# session.use_schema(\"public\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7fb47093-9b23-492d-a135-5722729a0c7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(status='DEMO_POOL_CPU already exists, statement succeeded.')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create compute pool if not exists\n",
    "def create_compute_pool(name: str, instance_family: str, min_nodes: int = 1, max_nodes: int = 10):\n",
    "    query = f\"\"\"\n",
    "        CREATE COMPUTE POOL IF NOT EXISTS {name}\n",
    "            MIN_NODES = {min_nodes}\n",
    "            MAX_NODES = {max_nodes}\n",
    "            INSTANCE_FAMILY = {instance_family}\n",
    "    \"\"\"\n",
    "    return session.sql(query).collect()\n",
    "\n",
    "compute_pool = \"DEMO_POOL_CPU\"\n",
    "create_compute_pool(compute_pool, \"CPU_X64_S\", 1, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5f328bfd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(status='Statement executed successfully.')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Enable multi node ML jobs\n",
    "# Note: If the parameter is invisible to you, contact the Snowflake account admin to enable the parameter for your account.\n",
    "session.sql(\"alter session set ENABLE_BATCH_JOB_SERVICES = true\").collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da4833f",
   "metadata": {},
   "source": [
    "### Approach 1: Train with function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "84804c16-a359-4e5b-9037-207cc9675b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a arbitary dataset\n",
    "def generate_dataset_sql(db, schema, table_name, num_rows, num_cols) -> str:\n",
    "    sql_script = f\"CREATE TABLE IF NOT EXISTS {db}.{schema}.{table_name} AS \\n\"\n",
    "    sql_script += f\"SELECT \\n\"\n",
    "    for i in range(1, num_cols):\n",
    "        sql_script += f\"uniform(0::FLOAT, 10::FLOAT, random()) AS FEATURE_{i}, \\n\"\n",
    "    sql_script += f\"FEATURE_1 + FEATURE_1 AS TARGET_1, \\n\"\n",
    "    sql_script += f\"FROM TABLE(generator(rowcount=>({num_rows})));\"\n",
    "    return sql_script\n",
    "num_rows = 1000 * 1000\n",
    "num_cols = 100\n",
    "table_name = \"MULTINODE_CPU_TRAIN_DS\"\n",
    "session.sql(generate_dataset_sql(session.get_current_database(), session.get_current_schema(), \n",
    "                                table_name, num_rows, num_cols)).collect()\n",
    "feature_list = [f'FEATURE_{num}' for num in range(1, num_cols)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "efd72bf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'num_instances' is deprecated and will be removed in a future release. Use 'target_instances' instead.\n"
     ]
    }
   ],
   "source": [
    "from snowflake.ml.jobs import remote\n",
    "\n",
    "@remote(compute_pool, stage_name=\"payload_stage\", num_instances=3)\n",
    "def xgb(table_name, input_cols, label_col):\n",
    "    from snowflake.snowpark import Session\n",
    "    from snowflake.ml.modeling.distributors.xgboost import XGBEstimator, XGBScalingConfig\n",
    "    from snowflake.ml.data.data_connector import DataConnector\n",
    "\n",
    "    session = Session.builder.getOrCreate()\n",
    "    cpu_train_df = session.table(table_name)\n",
    "    \n",
    "    params = {\n",
    "        \"tree_method\": \"hist\",\n",
    "        \"objective\": \"reg:pseudohubererror\",\n",
    "        \"eta\": 1e-4,\n",
    "        \"subsample\": 0.5,\n",
    "        \"max_depth\": 50,\n",
    "        \"max_leaves\": 1000,\n",
    "        \"max_bin\":63,\n",
    "    }\n",
    "    scaling_config = XGBScalingConfig(use_gpu=False)\n",
    "    estimator = XGBEstimator(\n",
    "        n_estimators=100,\n",
    "        params=params,\n",
    "        scaling_config=scaling_config,\n",
    "    )\n",
    "    data_connector = DataConnector.from_dataframe(cpu_train_df)\n",
    "    xgb_model = estimator.fit(\n",
    "        data_connector, input_cols=input_cols, label_col=label_col\n",
    "    )\n",
    "    return xgb_model\n",
    "\n",
    "# Function invocation returns a job handle (snowflake.ml.jobs.MLJob)\n",
    "job = xgb(table_name, feature_list, \"TARGET_1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3eb1ad66-851f-4fdb-887c-4dcaaf5bf2c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HEADLESS_STARTER_DB.HEADLESS_DEMO.MLJOB_3152B8EE_A391_4340_9152_D54A58365C1B\n",
      "PENDING\n"
     ]
    }
   ],
   "source": [
    "print(job.id)\n",
    "print(job.status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1909e552-d289-4bf4-8840-926d99295acb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-24 21:08:17,038\tINFO job_manager.py:528 -- Runtime env is setting up.\n",
      "/opt/conda/lib/python3.10/site-packages/snowflake/snowpark/session.py:38: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\n",
      "\n",
      "2025-06-24 21:08:20,186\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n",
      "\n",
      "83f38934a778457abfaff63217cfc17e: Received raw arguments: XGBTrainArgs(model_type=<BoostingModelTypes.XGBOOST: 'xgboost'>, dataset=<snowflake.ml.data.data_connector.DataConnector object at 0x7fd3b8f30eb0>, input_cols=['FEATURE_1', 'FEATURE_2', 'FEATURE_3', 'FEATURE_4', 'FEATURE_5', 'FEATURE_6', 'FEATURE_7', 'FEATURE_8', 'FEATURE_9', 'FEATURE_10', 'FEATURE_11', 'FEATURE_12', 'FEATURE_13', 'FEATURE_14', 'FEATURE_15', 'FEATURE_16', 'FEATURE_17', 'FEATURE_18', 'FEATURE_19', 'FEATURE_20', 'FEATURE_21', 'FEATURE_22', 'FEATURE_23', 'FEATURE_24', 'FEATURE_25', 'FEATURE_26', 'FEATURE_27', 'FEATURE_28', 'FEATURE_29', 'FEATURE_30', 'FEATURE_31', 'FEATURE_32', 'FEATURE_33', 'FEATURE_34', 'FEATURE_35', 'FEATURE_36', 'FEATURE_37', 'FEATURE_38', 'FEATURE_39', 'FEATURE_40', 'FEATURE_41', 'FEATURE_42', 'FEATURE_43', 'FEATURE_44', 'FEATURE_45', 'FEATURE_46', 'FEATURE_47', 'FEATURE_48', 'FEATURE_49', 'FEATURE_50', 'FEATURE_51', 'FEATURE_52', 'FEATURE_53', 'FEATURE_54', 'FEATURE_55', 'FEATURE_56', 'FEATURE_57', 'FEATURE_58', 'FEATURE_59', 'FEATURE_60', 'FEATURE_61', 'FEATURE_62', 'FEATURE_63', 'FEATURE_64', 'FEATURE_65', 'FEATURE_66', 'FEATURE_67', 'FEATURE_68', 'FEATURE_69', 'FEATURE_70', 'FEATURE_71', 'FEATURE_72', 'FEATURE_73', 'FEATURE_74', 'FEATURE_75', 'FEATURE_76', 'FEATURE_77', 'FEATURE_78', 'FEATURE_79', 'FEATURE_80', 'FEATURE_81', 'FEATURE_82', 'FEATURE_83', 'FEATURE_84', 'FEATURE_85', 'FEATURE_86', 'FEATURE_87', 'FEATURE_88', 'FEATURE_89', 'FEATURE_90', 'FEATURE_91', 'FEATURE_92', 'FEATURE_93', 'FEATURE_94', 'FEATURE_95', 'FEATURE_96', 'FEATURE_97', 'FEATURE_98', 'FEATURE_99'], label_col='TARGET_1', params={'n_estimators': 100, 'objective': 'reg:pseudohubererror', 'tree_method': 'hist', 'eta': 0.0001, 'subsample': 0.5, 'max_depth': 50, 'max_leaves': 1000, 'max_bin': 63}, eval_set=None, num_workers=-1, num_cpu_per_worker=-1, use_gpu=False, output_model_path='/tmp/ray-jobs/83f38934a778457abfaff63217cfc17e.pkl', eval_results_path='/tmp/ray-jobs/83f38934a778457abfaff63217cfc17e_eval_results.pkl', verbose_eval=None, model=None)\n",
      "Info - 2025-06-24 21:08:20.411685 - Number of nodes active for training: 3 nodes\n",
      "Info - 2025-06-24 21:08:20.413140 - Scaling Config: {'num_workers': 3, 'resources_per_worker': {'CPU': 2.0}, 'use_gpu': False}\n",
      "Info - 2025-06-24 21:08:20.413212 - Materializing Data ...\n",
      "Info - 2025-06-24 21:08:20.546457 - Loading data from Snowpark Dataframe from query id 01bd4054-0004-bd5b-0001-3b8704ce79f6\n",
      "\n",
      "Info - 2025-06-24 21:08:24.398563 - Finished executing data load query.\n",
      "Info - 2025-06-24 21:08:25.764243 - Loaded data into ray dataset.\n",
      "Info - 2025-06-24 21:08:25.769470 - Starting training job...\n",
      "\n",
      "\u001b[36m(TrainTrainable pid=622)\u001b[0m /opt/conda/lib/python3.10/site-packages/snowflake/snowpark/session.py:38: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "\u001b[36m(TrainTrainable pid=622)\u001b[0m   import pkg_resources\n",
      "\n",
      "\u001b[36m(DistributedXGBoostTrainer pid=622)\u001b[0m Started distributed worker processes: \n",
      "\u001b[36m(DistributedXGBoostTrainer pid=622)\u001b[0m - (node_id=fd022abd4d64d5018f9a6df9f04e7670e378af9728c8d44aba1c8c6a, ip=10.244.12.203, pid=664) world_rank=0, local_rank=0, node_rank=0\n",
      "\u001b[36m(DistributedXGBoostTrainer pid=622)\u001b[0m - (node_id=98eb04ceb0a5ca449b11bd090039b3d9d855eea6b0f6d6d34c620430, ip=10.244.12.75, pid=348) world_rank=1, local_rank=0, node_rank=1\n",
      "\u001b[36m(DistributedXGBoostTrainer pid=622)\u001b[0m - (node_id=5d49574e0383c7f66c78ad4cf45588b5ac6c347e82a29203a474cf3e, ip=10.244.13.75, pid=309) world_rank=2, local_rank=0, node_rank=2\n",
      "\n",
      "\u001b[36m(SplitCoordinator pid=702)\u001b[0m /opt/conda/lib/python3.10/site-packages/snowflake/snowpark/session.py:38: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "\u001b[36m(SplitCoordinator pid=702)\u001b[0m   import pkg_resources\n",
      "\n",
      "\u001b[36m(ReadResultSetDataSource->SplitBlocks(2) pid=387, ip=10.244.12.75)\u001b[0m /opt/conda/lib/python3.10/site-packages/snowflake/snowpark/session.py:38: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "\u001b[36m(ReadResultSetDataSource->SplitBlocks(2) pid=387, ip=10.244.12.75)\u001b[0m   import pkg_resources\n",
      "\n",
      "\u001b[36m(ReadResultSetDataSource->SplitBlocks(2) pid=805)\u001b[0m /opt/conda/lib/python3.10/site-packages/snowflake/snowpark/session.py:38: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "\u001b[36m(ReadResultSetDataSource->SplitBlocks(2) pid=805)\u001b[0m   import pkg_resources\n",
      "\n",
      "\u001b[36m(ReadResultSetDataSource->SplitBlocks(2) pid=383, ip=10.244.13.75)\u001b[0m /opt/conda/lib/python3.10/site-packages/snowflake/snowpark/session.py:38: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "\u001b[36m(ReadResultSetDataSource->SplitBlocks(2) pid=383, ip=10.244.13.75)\u001b[0m   import pkg_resources\n",
      "\n",
      "Info - 2025-06-24 21:10:09.168513 - Training job completed\n",
      "UserWarning: [21:10:09] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1738395417126/work/src/c_api/c_api.cc:1374: Saving model in the UBJSON format as default.  You can use file extension: `json`, `ubj` or `deprecated` to choose between formats.\n"
     ]
    }
   ],
   "source": [
    "job.wait()\n",
    "job.show_logs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6c46b2f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:snowflake.snowpark:MLJob.result() is in private preview since 1.8.2. Do not use it in production. \n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/xgboost/core.py:158: UserWarning: [14:10:35] WARNING: /Users/runner/work/xgboost/xgboost/src/gbm/../common/error_msg.h:80: If you are loading a serialized model (like pickle in Python, RDS in R) or\n",
      "configuration generated by an older version of XGBoost, please export the model by calling\n",
      "`Booster.save_model` from that version first, then load it back in current version. See:\n",
      "\n",
      "    https://xgboost.readthedocs.io/en/stable/tutorials/saving_model.html\n",
      "\n",
      "for more details about differences between saving model and serializing.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([11.8159   , 10.611345 ,  9.717881 , 18.790493 ,  7.9805217,\n",
       "       16.480486 , 15.571457 , 14.789684 , 12.37405  , 12.086709 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import xgboost\n",
    "\n",
    "# Retrieve trained model from job execution and use it for prediction\n",
    "xgb_model = job.result()\n",
    "\n",
    "# Predict on a sample of the dataset\n",
    "# Note: This is just a demonstration, in practice you would want to predict on a different dataset\n",
    "dataset = session.table(table_name).drop(\"TARGET_1\").limit(10).to_pandas()\n",
    "xgb_model.predict(xgboost.DMatrix(dataset))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10",
   "language": "python",
   "name": "py310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
