{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a6496bf-b928-4069-adfa-083299a21a13",
   "metadata": {},
   "source": [
    "### Set up Snowpark Session\n",
    "\n",
    "See [Configure Connections](https://docs.snowflake.com/developer-guide/snowflake-cli/connecting/configure-connections#define-connections)\n",
    "for information on how to define default Snowflake connection(s) in a config.toml\n",
    "file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f9880be-f3c2-4100-a35c-0e72595f3bdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<snowflake.snowpark.session.Session: account=\"DEMO_ACCOUNT\", role=\"DEMO_RL\", database=\"DEMO_DB\", schema=\"DEMO_SCHEMA\", warehouse=\"DEMO_WH\">\n"
     ]
    }
   ],
   "source": [
    "from snowflake.snowpark import Session, Row\n",
    "\n",
    "# Requires valid ~/.snowflake/config.toml file\n",
    "session = Session.builder.getOrCreate()\n",
    "print(session)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa9a253-38c5-4641-a01a-eaf899d24199",
   "metadata": {},
   "source": [
    "#### Set up Snowflake resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "46e5fad4-748b-4390-b4fa-748a10547835",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIONAL: Uncomment below to select a database and schema to use\n",
    "# session.use_database(\"temp\")\n",
    "# session.use_schema(\"public\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7fb47093-9b23-492d-a135-5722729a0c7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(status='DEMO_POOL_CPU already exists, statement succeeded.')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create compute pool if not exists\n",
    "def create_compute_pool(name: str, instance_family: str, min_nodes: int = 1, max_nodes: int = 10):\n",
    "    query = f\"\"\"\n",
    "        CREATE COMPUTE POOL IF NOT EXISTS {name}\n",
    "            MIN_NODES = {min_nodes}\n",
    "            MAX_NODES = {max_nodes}\n",
    "            INSTANCE_FAMILY = {instance_family}\n",
    "    \"\"\"\n",
    "    return session.sql(query).collect()\n",
    "\n",
    "compute_pool = \"DEMO_POOL_CPU\"\n",
    "create_compute_pool(compute_pool, \"CPU_X64_S\", 1, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da4833f",
   "metadata": {},
   "source": [
    "### Approach 1: Train with function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "84804c16-a359-4e5b-9037-207cc9675b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a arbitary dataset\n",
    "def generate_dataset_sql(db, schema, table_name, num_rows, num_cols) -> str:\n",
    "    sql_script = f\"CREATE TABLE IF NOT EXISTS {db}.{schema}.{table_name} AS \\n\"\n",
    "    sql_script += f\"SELECT \\n\"\n",
    "    for i in range(1, num_cols):\n",
    "        sql_script += f\"uniform(0::FLOAT, 10::FLOAT, random()) AS FEATURE_{i}, \\n\"\n",
    "    sql_script += f\"FEATURE_1 + FEATURE_1 AS TARGET_1, \\n\"\n",
    "    sql_script += f\"FROM TABLE(generator(rowcount=>({num_rows})));\"\n",
    "    return sql_script\n",
    "num_rows = 1000 * 1000\n",
    "num_cols = 100\n",
    "table_name = \"MULTINODE_CPU_TRAIN_DS\"\n",
    "session.sql(generate_dataset_sql(session.get_current_database(), session.get_current_schema(), \n",
    "                                table_name, num_rows, num_cols)).collect()\n",
    "feature_list = [f'FEATURE_{num}' for num in range(1, num_cols)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd72bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from snowflake.ml.jobs import remote\n",
    "\n",
    "@remote(compute_pool, stage_name=\"payload_stage\", target_instances=3)\n",
    "def xgb(table_name, input_cols, label_col):\n",
    "    from snowflake.snowpark import Session\n",
    "    from snowflake.ml.modeling.distributors.xgboost import XGBEstimator, XGBScalingConfig\n",
    "    from snowflake.ml.data.data_connector import DataConnector\n",
    "\n",
    "    session = Session.builder.getOrCreate()\n",
    "    cpu_train_df = session.table(table_name)\n",
    "    \n",
    "    params = {\n",
    "        \"tree_method\": \"hist\",\n",
    "        \"objective\": \"reg:pseudohubererror\",\n",
    "        \"eta\": 1e-4,\n",
    "        \"subsample\": 0.5,\n",
    "        \"max_depth\": 50,\n",
    "        \"max_leaves\": 1000,\n",
    "        \"max_bin\":63,\n",
    "    }\n",
    "    scaling_config = XGBScalingConfig(use_gpu=False)\n",
    "    estimator = XGBEstimator(\n",
    "        n_estimators=100,\n",
    "        params=params,\n",
    "        scaling_config=scaling_config,\n",
    "    )\n",
    "    data_connector = DataConnector.from_dataframe(cpu_train_df)\n",
    "    xgb_model = estimator.fit(\n",
    "        data_connector, input_cols=input_cols, label_col=label_col\n",
    "    )\n",
    "    return xgb_model\n",
    "\n",
    "# Function invocation returns a job handle (snowflake.ml.jobs.MLJob)\n",
    "job = xgb(table_name, feature_list, \"TARGET_1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3eb1ad66-851f-4fdb-887c-4dcaaf5bf2c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLJOB_99440CD7_F620_468E_B52D_B2872C52BAFE\n",
      "PENDING\n"
     ]
    }
   ],
   "source": [
    "print(job.id)\n",
    "print(job.status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1909e552-d289-4bf4-8840-926d99295acb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "'micromamba' is running as a subprocess and can't modify the parent shell.\n",
      "Thus you must initialize your shell before using activate and deactivate.\n",
      "\n",
      "To initialize the current bash shell, run:\n",
      "    $ eval \"$(micromamba shell hook --shell bash)\"\n",
      "and then activate or deactivate with:\n",
      "    $ micromamba activate\n",
      "To automatically initialize all future (bash) shells, run:\n",
      "    $ micromamba shell init --shell bash --root-prefix=~/micromamba\n",
      "If your shell was already initialized, reinitialize your shell with:\n",
      "    $ micromamba shell reinit --shell bash\n",
      "Otherwise, this may be an issue. In the meantime you can run commands. See:\n",
      "    $ micromamba run --help\n",
      "\n",
      "Supported shells are {bash, zsh, csh, xonsh, cmd.exe, powershell, fish}.\n",
      "Creating log directories...\n",
      " * Starting periodic command scheduler cron\n",
      "   ...done.\n",
      "2025-04-24 23:51:15,079 - WARNING - SnowflakeLoginOptions() is in private preview since 0.2.0. Do not use it in production. \n",
      "2025-04-24 23:51:15,080 - INFO - Snowflake Connector for Python Version: 3.13.2, Python Version: 3.10.15, Platform: Linux-5.4.181-99.354.amzn2.x86_64-x86_64-with-glibc2.31\n",
      "2025-04-24 23:51:15,080 - INFO - Connecting to GLOBAL Snowflake domain\n",
      "2025-04-24 23:51:15,081 - INFO - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.\n",
      "2025-04-24 23:51:16,443 - INFO - Snowpark Session information: \n",
      "\"version\" : 1.25.0,\n",
      "\"python.version\" : 3.10.15,\n",
      "\"python.connector.version\" : 3.13.2,\n",
      "\"python.connector.session.id\" : 239723776519818,\n",
      "\"os.name\" : Linux\n",
      "\n",
      "2025-04-24 23:51:59,122 - INFO - Closing session: 239723776519818\n",
      "2025-04-24 23:51:59,122 - INFO - Canceling all running queries\n",
      "2025-04-24 23:51:59,179 - INFO - closed\n",
      "2025-04-24 23:51:59,207 - INFO - No async queries seem to be running, deleting session\n",
      "2025-04-24 23:51:59,236 - INFO - Closed session: 239723776519818\n",
      "Head Instance Index: 0\n",
      "Head Instance IP: 10.244.63.201\n",
      "2025-04-24 23:52:00,795\tINFO usage_lib.py:441 -- Usage stats collection is disabled.\n",
      "2025-04-24 23:52:00,795\tINFO scripts.py:767 -- \u001b[37mLocal node IP\u001b[39m: \u001b[1m10.244.63.201\u001b[22m\n",
      "2025-04-24 23:52:02,233\tSUCC scripts.py:804 -- \u001b[32m--------------------\u001b[39m\n",
      "2025-04-24 23:52:02,234\tSUCC scripts.py:805 -- \u001b[32mRay runtime started.\u001b[39m\n",
      "2025-04-24 23:52:02,234\tSUCC scripts.py:806 -- \u001b[32m--------------------\u001b[39m\n",
      "2025-04-24 23:52:02,234\tINFO scripts.py:808 -- \u001b[36mNext steps\u001b[39m\n",
      "2025-04-24 23:52:02,234\tINFO scripts.py:811 -- To add another node to this Ray cluster, run\n",
      "2025-04-24 23:52:02,234\tINFO scripts.py:814 -- \u001b[1m  ray start --address='10.244.63.201:12001'\u001b[22m\n",
      "2025-04-24 23:52:02,234\tINFO scripts.py:823 -- To connect to this Ray cluster:\n",
      "2025-04-24 23:52:02,234\tINFO scripts.py:825 -- \u001b[35mimport\u001b[39m\u001b[26m ray\n",
      "2025-04-24 23:52:02,234\tINFO scripts.py:826 -- ray\u001b[35m.\u001b[39m\u001b[26minit(_node_ip_address\u001b[35m=\u001b[39m\u001b[26m\u001b[33m'10.244.63.201'\u001b[39m\u001b[26m)\n",
      "2025-04-24 23:52:02,234\tINFO scripts.py:838 -- To submit a Ray job using the Ray Jobs CLI:\n",
      "2025-04-24 23:52:02,234\tINFO scripts.py:839 -- \u001b[1m  RAY_ADDRESS='http://10.244.63.201:12003' ray job submit --working-dir . -- python my_script.py\u001b[22m\n",
      "2025-04-24 23:52:02,234\tINFO scripts.py:848 -- See https://docs.ray.io/en/latest/cluster/running-applications/job-submission/index.html \n",
      "2025-04-24 23:52:02,234\tINFO scripts.py:852 -- for more information on submitting Ray jobs to the Ray cluster.\n",
      "2025-04-24 23:52:02,234\tINFO scripts.py:857 -- To terminate the Ray runtime, run\n",
      "2025-04-24 23:52:02,234\tINFO scripts.py:858 -- \u001b[1m  ray stop\u001b[22m\n",
      "2025-04-24 23:52:02,234\tINFO scripts.py:861 -- To view the status of the cluster, use\n",
      "2025-04-24 23:52:02,234\tINFO scripts.py:862 --   \u001b[1mray status\u001b[22m\u001b[26m\n",
      "2025-04-24 23:52:02,234\tINFO scripts.py:866 -- To monitor and debug Ray, view the dashboard at \n",
      "2025-04-24 23:52:02,234\tINFO scripts.py:867 --   \u001b[1m10.244.63.201:12003\u001b[22m\u001b[26m\n",
      "2025-04-24 23:52:02,235\tINFO scripts.py:874 -- \u001b[4mIf connection to the dashboard fails, check your firewall settings and network configuration.\u001b[24m\n",
      "Running command: python /mnt/app/mljob_launcher.py /mnt/app/func.py --script_main_func func\n",
      "SnowflakeLoginOptions() is in private preview since 0.2.0. Do not use it in production. \n",
      "DataConnector.from_dataframe() is in private preview since 1.6.0. Do not use it in production. \n",
      "DataConnector.from_sql() is in private preview since 1.7.3. Do not use it in production. \n",
      "2025-04-24 23:52:06,935\tINFO worker.py:1601 -- Connecting to existing Ray cluster at address: 10.244.63.201:12001...\n",
      "2025-04-24 23:52:06,953\tINFO worker.py:1777 -- Connected to Ray cluster. View the dashboard at \u001b[1m\u001b[32m10.244.63.201:12003 \u001b[39m\u001b[22m\n",
      "2025-04-24 23:52:07,627\tINFO job_manager.py:528 -- Runtime env is setting up.\n",
      "\n",
      "2025-04-24 23:52:11,193\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n",
      "e03fba9706734fa68123110586f16f19: Received raw arguments: XGBTrainArgs(model_type=<BoostingModelTypes.XGBOOST: 'xgboost'>, dataset=<snowflake.ml.data.data_connector.DataConnector object at 0x7fdc77acb8b0>, input_cols=['FEATURE_1', 'FEATURE_2', 'FEATURE_3', 'FEATURE_4', 'FEATURE_5', 'FEATURE_6', 'FEATURE_7', 'FEATURE_8', 'FEATURE_9', 'FEATURE_10', 'FEATURE_11', 'FEATURE_12', 'FEATURE_13', 'FEATURE_14', 'FEATURE_15', 'FEATURE_16', 'FEATURE_17', 'FEATURE_18', 'FEATURE_19', 'FEATURE_20', 'FEATURE_21', 'FEATURE_22', 'FEATURE_23', 'FEATURE_24', 'FEATURE_25', 'FEATURE_26', 'FEATURE_27', 'FEATURE_28', 'FEATURE_29', 'FEATURE_30', 'FEATURE_31', 'FEATURE_32', 'FEATURE_33', 'FEATURE_34', 'FEATURE_35', 'FEATURE_36', 'FEATURE_37', 'FEATURE_38', 'FEATURE_39', 'FEATURE_40', 'FEATURE_41', 'FEATURE_42', 'FEATURE_43', 'FEATURE_44', 'FEATURE_45', 'FEATURE_46', 'FEATURE_47', 'FEATURE_48', 'FEATURE_49', 'FEATURE_50', 'FEATURE_51', 'FEATURE_52', 'FEATURE_53', 'FEATURE_54', 'FEATURE_55', 'FEATURE_56', 'FEATURE_57', 'FEATURE_58', 'FEATURE_59', 'FEATURE_60', 'FEATURE_61', 'FEATURE_62', 'FEATURE_63', 'FEATURE_64', 'FEATURE_65', 'FEATURE_66', 'FEATURE_67', 'FEATURE_68', 'FEATURE_69', 'FEATURE_70', 'FEATURE_71', 'FEATURE_72', 'FEATURE_73', 'FEATURE_74', 'FEATURE_75', 'FEATURE_76', 'FEATURE_77', 'FEATURE_78', 'FEATURE_79', 'FEATURE_80', 'FEATURE_81', 'FEATURE_82', 'FEATURE_83', 'FEATURE_84', 'FEATURE_85', 'FEATURE_86', 'FEATURE_87', 'FEATURE_88', 'FEATURE_89', 'FEATURE_90', 'FEATURE_91', 'FEATURE_92', 'FEATURE_93', 'FEATURE_94', 'FEATURE_95', 'FEATURE_96', 'FEATURE_97', 'FEATURE_98', 'FEATURE_99'], label_col='TARGET_1', params={'n_estimators': 100, 'objective': 'reg:pseudohubererror', 'tree_method': 'hist', 'eta': 0.0001, 'subsample': 0.5, 'max_depth': 50, 'max_leaves': 1000, 'max_bin': 63}, eval_set=None, num_workers=-1, num_cpu_per_worker=-1, use_gpu=False, output_model_path='/tmp/ray-jobs/e03fba9706734fa68123110586f16f19.pkl', eval_results_path='/tmp/ray-jobs/e03fba9706734fa68123110586f16f19_eval_results.pkl', verbose_eval=None, model=None)\n",
      "Info - Training XGBoost using CPU single node\n",
      "\n",
      "2025-04-24 23:52:14,987\tINFO streaming_executor.py:108 -- Starting execution of Dataset. Full logs are in /tmp/ray/session_2025-04-24_23-52-00_796201_46/logs/ray-data\n",
      "2025-04-24 23:52:14,987\tINFO streaming_executor.py:109 -- Execution plan of Dataset: InputDataBuffer[Input] -> TaskPoolMapOperator[ReadResultSetDataSource]\n",
      "\n",
      "✔️  Dataset execution finished in 5.70 seconds: : 0.00 row [00:05, ? row/s]                                               \n",
      "✔️  Dataset execution finished in 5.70 seconds: : 0.00 row [00:05, ? row/s]\n",
      "✔️  Dataset execution finished in 5.70 seconds: : 0.00 row [00:05, ? row/s]\n",
      "✔️  Dataset execution finished in 5.70 seconds: : 0.00 row [00:05, ? row/s]\n",
      "\n",
      "2025-04-24 23:52:20,693\tINFO streaming_executor.py:108 -- Starting execution of Dataset. Full logs are in /tmp/ray/session_2025-04-24_23-52-00_796201_46/logs/ray-data\n",
      "2025-04-24 23:52:20,693\tINFO streaming_executor.py:109 -- Execution plan of Dataset: InputDataBuffer[Input] -> TaskPoolMapOperator[ReadResultSetDataSource]\n",
      "\n",
      "✔️  Dataset execution finished in 6.10 seconds:  25%|██▌       | 251k/1.00M [00:06<00:19, 38.2k row/s]                                                 \n",
      "✔️  Dataset execution finished in 6.10 seconds: 100%|██████████| 1.00M/1.00M [00:06<00:00, 287k row/s]\n",
      "✔️  Dataset execution finished in 6.10 seconds: 100%|██████████| 1.00M/1.00M [00:06<00:00, 287k row/s]\n",
      "\n",
      "✔️  Dataset execution finished in 6.10 seconds: 100%|██████████| 1.00M/1.00M [00:06<00:00, 164k row/s]\n",
      "\n",
      "Training completed\n",
      "User job completed. Signaling workers to shut down...\n",
      "2025-04-24 23:53:08,545\tINFO worker.py:1601 -- Connecting to existing Ray cluster at address: 10.244.63.201:12001...\n",
      "2025-04-24 23:53:08,555\tINFO worker.py:1777 -- Connected to Ray cluster. View the dashboard at \u001b[1m\u001b[32m10.244.63.201:12003 \u001b[39m\u001b[22m\n",
      "2025-04-24 23:53:08,573 - INFO - Found 2 worker nodes\n",
      "2025-04-24 23:53:08,573 - INFO - Creating new shutdown signal actor: Failed to look up actor with name 'ShutdownSignal'. This could because 1. You are trying to look up a named actor you didn't create. 2. The named actor died. 3. You did not use a namespace matching the namespace of the actor.\n",
      "2025-04-24 23:53:09,179 - INFO - Shutdown requested: {'status': 'shutdown_requested', 'timestamp': 1745538789.1788423, 'host': 'job-0'}\n",
      "2025-04-24 23:53:09,180 - INFO - Waiting up to 15s for workers to acknowledge shutdown signal...\n",
      "2025-04-24 23:53:14,207 - INFO - All 2 workers acknowledged shutdown. Completed in 5.03s\n",
      "Head node job completed. Exiting.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "job.wait()\n",
    "job.show_logs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6c46b2f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/snowml310/lib/python3.10/site-packages/xgboost/core.py:158: UserWarning: [16:53:20] WARNING: /Users/runner/work/xgboost/xgboost/src/gbm/../common/error_msg.h:80: If you are loading a serialized model (like pickle in Python, RDS in R) or\n",
      "configuration generated by an older version of XGBoost, please export the model by calling\n",
      "`Booster.save_model` from that version first, then load it back in current version. See:\n",
      "\n",
      "    https://xgboost.readthedocs.io/en/stable/tutorials/saving_model.html\n",
      "\n",
      "for more details about differences between saving model and serializing.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 7.242013,  9.110336, 18.514343, 14.750366, 18.905142, 11.804218,\n",
       "       17.774406, 17.400677,  7.676889, 14.249159], dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import xgboost\n",
    "\n",
    "# Retrieve trained model from job execution and use it for prediction\n",
    "xgb_model = job.result()\n",
    "\n",
    "# Predict on a sample of the dataset\n",
    "# Note: This is just a demonstration, in practice you would want to predict on a different dataset\n",
    "dataset = session.table(table_name).drop(\"TARGET_1\").limit(10).to_pandas()\n",
    "xgb_model.predict(xgboost.DMatrix(dataset))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
