{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a6496bf-b928-4069-adfa-083299a21a13",
   "metadata": {},
   "source": [
    "### Set up Snowpark Session\n",
    "\n",
    "See [Configure Connections](https://docs.snowflake.com/developer-guide/snowflake-cli/connecting/configure-connections#define-connections)\n",
    "for information on how to define default Snowflake connection(s) in a config.toml\n",
    "file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f9880be-f3c2-4100-a35c-0e72595f3bdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<snowflake.snowpark.session.Session: account=\"NOTEBOOK_MLTEST\", role=\"SYSADMIN\", database=\"HEADLESS_STARTER_DB\", schema=\"HEADLESS_STARTER_SCHEMA\", warehouse=\"ST_WH\">\n"
     ]
    }
   ],
   "source": [
    "from snowflake.snowpark import Session, Row\n",
    "\n",
    "# Requires valid ~/.snowflake/config.toml file\n",
    "session = Session.builder.getOrCreate()\n",
    "print(session)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa9a253-38c5-4641-a01a-eaf899d24199",
   "metadata": {},
   "source": [
    "#### Set up Snowflake resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "46e5fad4-748b-4390-b4fa-748a10547835",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_name = \"HEADLESS_DEMO\"\n",
    "session.sql(f\"CREATE SCHEMA IF NOT EXISTS {schema_name}\").collect()\n",
    "session.use_schema(schema_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7fb47093-9b23-492d-a135-5722729a0c7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(status='DEMO_POOL_CPU already exists, statement succeeded.')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create compute pool\n",
    "def create_compute_pool(name: str, instance_family: str, min_nodes: int = 1, max_nodes: int = 10) -> list[Row]:\n",
    "    query = f\"\"\"\n",
    "        CREATE COMPUTE POOL IF NOT EXISTS {name}\n",
    "            MIN_NODES = {min_nodes}\n",
    "            MAX_NODES = {max_nodes}\n",
    "            INSTANCE_FAMILY = {instance_family}\n",
    "    \"\"\"\n",
    "    return session.sql(query).collect()\n",
    "\n",
    "compute_pool = \"DEMO_POOL_CPU\"\n",
    "create_compute_pool(compute_pool, \"CPU_X64_S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "84804c16-a359-4e5b-9037-207cc9675b75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(status='LOAN_APPLICATIONS already exists, statement succeeded.')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate synthetic data\n",
    "def generate_data(table_name: str, num_rows: int, replace: bool = False) -> list[Row]:\n",
    "    query = f\"\"\"\n",
    "        CREATE{\" OR REPLACE\" if replace else \"\"} TABLE{\"\" if replace else \" IF NOT EXISTS\"} {table_name} AS\n",
    "        SELECT \n",
    "            ROW_NUMBER() OVER (ORDER BY RANDOM()) as application_id,\n",
    "            ROUND(NORMAL(40, 10, RANDOM())) as age,\n",
    "            ROUND(NORMAL(65000, 20000, RANDOM())) as income,\n",
    "            ROUND(NORMAL(680, 50, RANDOM())) as credit_score,\n",
    "            ROUND(NORMAL(5, 2, RANDOM())) as employment_length,\n",
    "            ROUND(NORMAL(25000, 8000, RANDOM())) as loan_amount,\n",
    "            ROUND(NORMAL(35, 10, RANDOM()), 2) as debt_to_income,\n",
    "            ROUND(NORMAL(5, 2, RANDOM())) as number_of_credit_lines,\n",
    "            GREATEST(0, ROUND(NORMAL(1, 1, RANDOM()))) as previous_defaults,\n",
    "            ARRAY_CONSTRUCT(\n",
    "                'home_improvement', 'debt_consolidation', 'business', 'education',\n",
    "                'major_purchase', 'medical', 'vehicle', 'other'\n",
    "            )[UNIFORM(1, 8, RANDOM())] as loan_purpose,\n",
    "            RANDOM() < 0.15 as is_default,\n",
    "            TIMEADD(\"MINUTE\", UNIFORM(-525600, 0, RANDOM()), CURRENT_TIMESTAMP()) as created_at\n",
    "        FROM TABLE(GENERATOR(rowcount => {num_rows}))\n",
    "        ORDER BY created_at;\n",
    "    \"\"\"\n",
    "    return session.sql(query).collect()\n",
    "\n",
    "table_name = \"loan_applications\"\n",
    "generate_data(table_name, 1e5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6783667-50b2-4f41-a72c-3969e431f858",
   "metadata": {},
   "source": [
    "### Prepare Model Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f6ba7a8b-a944-4149-a46e-48196350b70e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import pickle\n",
    "from time import perf_counter\n",
    "from typing import Literal, Optional\n",
    "\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import accuracy_score, classification_report, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "\n",
    "\n",
    "from snowflake.ml.data.data_connector import DataConnector\n",
    "from snowflake.ml.registry import Registry as ModelRegistry\n",
    "from snowflake.snowpark import Session\n",
    "\n",
    "\n",
    "def create_data_connector(session, table_name: str) -> DataConnector:\n",
    "    \"\"\"Load data from Snowflake table\"\"\"\n",
    "    # Example query - modify according to your schema\n",
    "    query = f\"\"\"\n",
    "    SELECT\n",
    "        age,\n",
    "        income,\n",
    "        credit_score,\n",
    "        employment_length,\n",
    "        loan_amount,\n",
    "        debt_to_income,\n",
    "        number_of_credit_lines,\n",
    "        previous_defaults,\n",
    "        loan_purpose,\n",
    "        is_default\n",
    "    FROM {table_name}\n",
    "    \"\"\"\n",
    "    sp_df = session.sql(query)\n",
    "    return DataConnector.from_dataframe(sp_df)\n",
    "\n",
    "\n",
    "def build_pipeline(**model_params) -> Pipeline:\n",
    "    \"\"\"Create pipeline with preprocessors and model\"\"\"\n",
    "    # Define column types\n",
    "    categorical_cols = [\"LOAN_PURPOSE\"]\n",
    "    numerical_cols = [\n",
    "        \"AGE\",\n",
    "        \"INCOME\",\n",
    "        \"CREDIT_SCORE\",\n",
    "        \"EMPLOYMENT_LENGTH\",\n",
    "        \"LOAN_AMOUNT\",\n",
    "        \"DEBT_TO_INCOME\",\n",
    "        \"NUMBER_OF_CREDIT_LINES\",\n",
    "        \"PREVIOUS_DEFAULTS\",\n",
    "    ]\n",
    "\n",
    "    # Numerical preprocessing pipeline\n",
    "    numeric_transformer = Pipeline(\n",
    "        steps=[\n",
    "            (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "            (\"scaler\", StandardScaler()),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Categorical preprocessing pipeline\n",
    "    categorical_transformer = Pipeline(\n",
    "        steps=[\n",
    "            (\"imputer\", SimpleImputer(strategy=\"constant\", fill_value=\"missing\")),\n",
    "            (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Combine transformers\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"num\", numeric_transformer, numerical_cols),\n",
    "            (\"cat\", categorical_transformer, categorical_cols),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Define model parameters\n",
    "    default_params = {\n",
    "        \"objective\": \"binary:logistic\",\n",
    "        \"eval_metric\": \"auc\",\n",
    "        \"max_depth\": 6,\n",
    "        \"learning_rate\": 0.1,\n",
    "        \"n_estimators\": 100,\n",
    "        \"subsample\": 0.8,\n",
    "        \"colsample_bytree\": 0.8,\n",
    "        \"random_state\": 42,\n",
    "    }\n",
    "    model = xgb.XGBClassifier(**(model_params or default_params))\n",
    "\n",
    "    return Pipeline([(\"preprocessor\", preprocessor), (\"classifier\", model)])\n",
    "\n",
    "\n",
    "def evaluate_model(model: Pipeline, X_test: pd.DataFrame, y_test: pd.DataFrame):\n",
    "    \"\"\"Evaluate model performance\"\"\"\n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    # Calculate metrics\n",
    "    metrics = {\n",
    "        \"accuracy\": accuracy_score(y_test, y_pred),\n",
    "        \"roc_auc\": roc_auc_score(y_test, y_pred_proba),\n",
    "        \"classification_report\": classification_report(y_test, y_pred),\n",
    "    }\n",
    "\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def save_to_registry(\n",
    "    session: Session,\n",
    "    model: Pipeline,\n",
    "    model_name: str,\n",
    "    metrics: dict,\n",
    "    sample_input_data: pd.DataFrame,\n",
    "):\n",
    "    \"\"\"Save model and artifacts to Snowflake Model Registry\"\"\"\n",
    "    # Initialize model registry\n",
    "    registry = ModelRegistry(session)\n",
    "\n",
    "    # Save to registry\n",
    "    registry.log_model(\n",
    "        model=model,\n",
    "        model_name=model_name,\n",
    "        metrics=metrics,\n",
    "        sample_input_data=sample_input_data[:5],\n",
    "        conda_dependencies=[\"xgboost\"],\n",
    "    )\n",
    "\n",
    "\n",
    "def train(session: Session, source_data: str, save_mode: Literal[\"local\", \"registry\"] = \"local\", output_dir: Optional[str] = None, **kwargs):\n",
    "    # Load data\n",
    "    dc = create_data_connector(session, table_name=source_data)\n",
    "    print(\"Loading data...\", end=\"\", flush=True)\n",
    "    start = perf_counter()\n",
    "    df = dc.to_pandas()\n",
    "    elapsed = perf_counter() - start\n",
    "    print(f\" done! Loaded {len(df)} rows, elapsed={elapsed:.3f}s\")\n",
    "\n",
    "    # Split data\n",
    "    X = df.drop(\"IS_DEFAULT\", axis=1)\n",
    "    y = df[\"IS_DEFAULT\"]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42\n",
    "    )\n",
    "\n",
    "    # Train model\n",
    "    model = build_pipeline()\n",
    "    print(\"Training model...\", end=\"\")\n",
    "    start = perf_counter()\n",
    "    model.fit(X_train, y_train)\n",
    "    elapsed = perf_counter() - start\n",
    "    print(f\" done! Elapsed={elapsed:.3f}s\")\n",
    "\n",
    "    # Evaluate model\n",
    "    print(\"Evaluating model...\", end=\"\")\n",
    "    start = perf_counter()\n",
    "    metrics = evaluate_model(\n",
    "        model,\n",
    "        X_test,\n",
    "        y_test,\n",
    "    )\n",
    "    elapsed = perf_counter() - start\n",
    "    print(f\" done! Elapsed={elapsed:.3f}s\")\n",
    "\n",
    "    # Print evaluation results\n",
    "    print(\"\\nModel Performance Metrics:\")\n",
    "    print(f\"Accuracy: {metrics['accuracy']:.4f}\")\n",
    "    print(f\"ROC AUC: {metrics['roc_auc']:.4f}\")\n",
    "    # Uncomment below for full classification report\n",
    "    # print(\"\\nClassification Report:\")\n",
    "    # print(metrics[\"classification_report\"])\n",
    "\n",
    "    start = perf_counter()\n",
    "    if save_mode == \"local\":\n",
    "        # Save model locally\n",
    "        print(\"Saving model to disk...\", end=\"\")\n",
    "        output_dir = output_dir or '.'\n",
    "        model_subdir = os.environ.get(\"SNOWFLAKE_SERVICE_NAME\", \"output\")\n",
    "        model_dir = os.path.join(output_dir, model_subdir) if not output_dir.endswith(model_subdir) else output_dir\n",
    "        os.makedirs(model_dir, exist_ok=True)\n",
    "        with open(os.path.join(model_dir, \"model.pkl\"), \"wb\") as f:\n",
    "            pickle.dump(model, f)\n",
    "        with open(os.path.join(model_dir, \"metrics.json\"), \"w\") as f:\n",
    "            json.dump(metrics, f, indent=2)\n",
    "    elif save_mode == \"registry\":\n",
    "        # Save model to registry\n",
    "        print(\"Logging model to Model Registry...\", end=\"\")\n",
    "        save_to_registry(\n",
    "            session,\n",
    "            model=model,\n",
    "            model_name=\"loan_default_predictor\",\n",
    "            metrics=metrics,\n",
    "            sample_input_data=X_train,\n",
    "        )\n",
    "    elapsed = perf_counter() - start\n",
    "    print(f\" done! Elapsed={elapsed:.3f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "897e6b37-0cbc-4878-8e06-a6dae60b6249",
   "metadata": {},
   "source": [
    "### Run training locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ef1c3106-7e4a-4c7c-82e1-0fd1f6de417f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data... done! Loaded 100000 rows, elapsed=3.174s\n",
      "Training model... done! Elapsed=0.227s\n",
      "Evaluating model... done! Elapsed=0.035s\n",
      "\n",
      "Model Performance Metrics:\n",
      "Accuracy: 0.5014\n",
      "ROC AUC: 0.5027\n",
      "Saving model to disk... done! Elapsed=0.002s\n"
     ]
    }
   ],
   "source": [
    "train(session, table_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e84a9f41-1113-47f5-9041-a8cf1f904a9f",
   "metadata": {},
   "source": [
    "### Train with remote SPCS instance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c2940e5a-4f85-46d9-9b49-aed6ac79fb53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from snowflake.ml.jobs import remote\n",
    "\n",
    "@remote(compute_pool, stage_name=\"payload_stage\")\n",
    "def train_remote(source_data: str, save_mode: str = \"local\", output_dir: str = None):\n",
    "    # Retrieve session from SPCS service context\n",
    "    session = Session.builder.getOrCreate()\n",
    "\n",
    "    # Run training script\n",
    "    train(session, source_data, save_mode, output_dir)\n",
    "\n",
    "train_job = train_remote(table_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3eb1ad66-851f-4fdb-887c-4dcaaf5bf2c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HEADLESS_STARTER_DB.HEADLESS_DEMO.MLJOB_1A0AE3B9_4A61_463F_948C_0005D4879C46\n",
      "PENDING\n"
     ]
    }
   ],
   "source": [
    "print(train_job.id)\n",
    "print(train_job.status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1909e552-d289-4bf4-8840-926d99295acb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-24 21:02:22,384 - INFO - Snowflake Connector for Python Version: 3.15.0, Python Version: 3.10.17, Platform: Linux-5.15.180-14.2025053011g5f85d0d+snow+aws+5.15+amd64.x86_64-x86_64-with-glibc2.31\n",
      "2025-06-24 21:02:22,384 - INFO - Connecting to GLOBAL Snowflake domain\n",
      "2025-06-24 21:02:22,690\tINFO worker.py:1601 -- Connecting to existing Ray cluster at address: 10.244.5.152:12001...\n",
      "2025-06-24 21:02:22,705\tINFO worker.py:1777 -- Connected to Ray cluster. View the dashboard at \u001b[1m\u001b[32m10.244.5.152:12003 \u001b[39m\u001b[22m\n",
      "2025-06-24 21:02:24,132\tINFO streaming_executor.py:108 -- Starting execution of Dataset. Full logs are in /tmp/ray/session_2025-06-24_21-02-13_815546_47/logs/ray-data\n",
      "2025-06-24 21:02:24,132\tINFO streaming_executor.py:109 -- Execution plan of Dataset: InputDataBuffer[Input] -> TaskPoolMapOperator[ReadResultSetDataSource]\n",
      "\u001b[36m(ReadResultSetDataSource->SplitBlocks(34) pid=315)\u001b[0m /opt/conda/lib/python3.10/site-packages/snowflake/snowpark/session.py:38: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "\u001b[36m(ReadResultSetDataSource->SplitBlocks(34) pid=315)\u001b[0m   import pkg_resources\n",
      "\u001b[36m(ReadResultSetDataSource->SplitBlocks(34) pid=316)\u001b[0m SnowflakeLoginOptions() is in private preview since 0.2.0. Do not use it in production. \n",
      "Loading data...Info - 2025-06-24 21:02:22.501542 - Loading data from Snowpark Dataframe from query id 01bd404e-0004-bd5b-0001-3b8704ce776a\n",
      "Info - 2025-06-24 21:02:22.687591 - Finished executing data load query.\n",
      "Info - 2025-06-24 21:02:24.128130 - Loaded data into ray dataset.\n",
      "Info - 2025-06-24 21:02:24.129586 - Loading data into a pandas dataframe.\n",
      " done! Loaded 100000 rows, elapsed=6.918s\n",
      "Training model... done! Elapsed=0.501s\n",
      "Evaluating model... done! Elapsed=0.075s\n",
      "\n",
      "Model Performance Metrics:\n",
      "Accuracy: 0.5016\n",
      "ROC AUC: 0.4986\n",
      "Saving model to disk... done! Elapsed=0.339s\n"
     ]
    }
   ],
   "source": [
    "train_job.wait()\n",
    "train_job.show_logs()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe67e0b-e73f-4c90-8a22-f7b7fce0bfe2",
   "metadata": {},
   "source": [
    "### Run concurrent training jobs on SPCS\n",
    "\n",
    "Suppose we want to train multiple models on different datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5c1be7c9-1512-48f5-adc0-630102e4e41e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating datasets\n",
      "Generated datasets: ['loan_applications_0', 'loan_applications_1', 'loan_applications_2', 'loan_applications_3', 'loan_applications_4', 'loan_applications_5', 'loan_applications_6', 'loan_applications_7', 'loan_applications_8', 'loan_applications_9']\n",
      "Starting training jobs\n",
      "Started 10 training jobs\n"
     ]
    }
   ],
   "source": [
    "datasets = []\n",
    "print(\"Generating datasets\")\n",
    "for i in range(10):\n",
    "    dataset = f\"loan_applications_{i}\"\n",
    "    generate_data(dataset, 1e6)\n",
    "    datasets.append(dataset)\n",
    "print(f\"Generated datasets: {datasets}\")\n",
    "    \n",
    "print(\"Starting training jobs\")\n",
    "train_jobs = []\n",
    "for ds in datasets:\n",
    "    train_jobs.append(train_remote(ds))\n",
    "print(f\"Started {len(train_jobs)} training jobs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "75619ff2-70fb-4237-b183-25d6627b902a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>status</th>\n",
       "      <th>message</th>\n",
       "      <th>database_name</th>\n",
       "      <th>schema_name</th>\n",
       "      <th>owner</th>\n",
       "      <th>compute_pool</th>\n",
       "      <th>target_instances</th>\n",
       "      <th>created_time</th>\n",
       "      <th>completed_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MLJOB_09C3A7E9_531D_4706_8DEB_7F628935174D</td>\n",
       "      <td>PENDING</td>\n",
       "      <td></td>\n",
       "      <td>HEADLESS_STARTER_DB</td>\n",
       "      <td>HEADLESS_DEMO</td>\n",
       "      <td>SYSADMIN</td>\n",
       "      <td>DEMO_POOL_CPU</td>\n",
       "      <td>1</td>\n",
       "      <td>2025-06-24 14:03:40.897000-07:00</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MLJOB_E16CE37E_7DD4_4BDC_96F8_A27BBD2F694E</td>\n",
       "      <td>PENDING</td>\n",
       "      <td></td>\n",
       "      <td>HEADLESS_STARTER_DB</td>\n",
       "      <td>HEADLESS_DEMO</td>\n",
       "      <td>SYSADMIN</td>\n",
       "      <td>DEMO_POOL_CPU</td>\n",
       "      <td>1</td>\n",
       "      <td>2025-06-24 14:03:35.318000-07:00</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MLJOB_BA598B1E_779A_432E_940A_9537546473A7</td>\n",
       "      <td>PENDING</td>\n",
       "      <td></td>\n",
       "      <td>HEADLESS_STARTER_DB</td>\n",
       "      <td>HEADLESS_DEMO</td>\n",
       "      <td>SYSADMIN</td>\n",
       "      <td>DEMO_POOL_CPU</td>\n",
       "      <td>1</td>\n",
       "      <td>2025-06-24 14:03:29.767000-07:00</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MLJOB_7114313F_2712_413F_B2C6_B24412F808BD</td>\n",
       "      <td>PENDING</td>\n",
       "      <td></td>\n",
       "      <td>HEADLESS_STARTER_DB</td>\n",
       "      <td>HEADLESS_DEMO</td>\n",
       "      <td>SYSADMIN</td>\n",
       "      <td>DEMO_POOL_CPU</td>\n",
       "      <td>1</td>\n",
       "      <td>2025-06-24 14:03:24.145000-07:00</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MLJOB_19749338_0A76_4BF7_9DBA_7CB4C95A8E8B</td>\n",
       "      <td>RUNNING</td>\n",
       "      <td></td>\n",
       "      <td>HEADLESS_STARTER_DB</td>\n",
       "      <td>HEADLESS_DEMO</td>\n",
       "      <td>SYSADMIN</td>\n",
       "      <td>DEMO_POOL_CPU</td>\n",
       "      <td>1</td>\n",
       "      <td>2025-06-24 14:03:18.936000-07:00</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>MLJOB_4954F057_5D52_474A_8618_FDD7D5AEDF25</td>\n",
       "      <td>PENDING</td>\n",
       "      <td></td>\n",
       "      <td>HEADLESS_STARTER_DB</td>\n",
       "      <td>HEADLESS_DEMO</td>\n",
       "      <td>SYSADMIN</td>\n",
       "      <td>DEMO_POOL_CPU</td>\n",
       "      <td>1</td>\n",
       "      <td>2025-06-24 14:03:09.669000-07:00</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>MLJOB_E4F1FF2F_90A4_4C6F_B8F3_EDE0C71A0F05</td>\n",
       "      <td>PENDING</td>\n",
       "      <td></td>\n",
       "      <td>HEADLESS_STARTER_DB</td>\n",
       "      <td>HEADLESS_DEMO</td>\n",
       "      <td>SYSADMIN</td>\n",
       "      <td>DEMO_POOL_CPU</td>\n",
       "      <td>1</td>\n",
       "      <td>2025-06-24 14:03:04.633000-07:00</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>MLJOB_58BC4035_78BB_448F_AE03_38CFBFB1D734</td>\n",
       "      <td>PENDING</td>\n",
       "      <td></td>\n",
       "      <td>HEADLESS_STARTER_DB</td>\n",
       "      <td>HEADLESS_DEMO</td>\n",
       "      <td>SYSADMIN</td>\n",
       "      <td>DEMO_POOL_CPU</td>\n",
       "      <td>1</td>\n",
       "      <td>2025-06-24 14:02:59.467000-07:00</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>MLJOB_4BC196AD_D99B_49A3_98EB_13D55A174F95</td>\n",
       "      <td>DONE</td>\n",
       "      <td>Job completed successfully.</td>\n",
       "      <td>HEADLESS_STARTER_DB</td>\n",
       "      <td>HEADLESS_DEMO</td>\n",
       "      <td>SYSADMIN</td>\n",
       "      <td>DEMO_POOL_CPU</td>\n",
       "      <td>1</td>\n",
       "      <td>2025-06-24 14:02:53.670000-07:00</td>\n",
       "      <td>2025-06-24 14:03:35.249000-07:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>MLJOB_837589B6_6D0F_4AC2_89B5_97622C8CCA14</td>\n",
       "      <td>DONE</td>\n",
       "      <td>Job completed successfully.</td>\n",
       "      <td>HEADLESS_STARTER_DB</td>\n",
       "      <td>HEADLESS_DEMO</td>\n",
       "      <td>SYSADMIN</td>\n",
       "      <td>DEMO_POOL_CPU</td>\n",
       "      <td>1</td>\n",
       "      <td>2025-06-24 14:02:48.254000-07:00</td>\n",
       "      <td>2025-06-24 14:03:31.750000-07:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         name   status  \\\n",
       "0  MLJOB_09C3A7E9_531D_4706_8DEB_7F628935174D  PENDING   \n",
       "1  MLJOB_E16CE37E_7DD4_4BDC_96F8_A27BBD2F694E  PENDING   \n",
       "2  MLJOB_BA598B1E_779A_432E_940A_9537546473A7  PENDING   \n",
       "3  MLJOB_7114313F_2712_413F_B2C6_B24412F808BD  PENDING   \n",
       "4  MLJOB_19749338_0A76_4BF7_9DBA_7CB4C95A8E8B  RUNNING   \n",
       "5  MLJOB_4954F057_5D52_474A_8618_FDD7D5AEDF25  PENDING   \n",
       "6  MLJOB_E4F1FF2F_90A4_4C6F_B8F3_EDE0C71A0F05  PENDING   \n",
       "7  MLJOB_58BC4035_78BB_448F_AE03_38CFBFB1D734  PENDING   \n",
       "8  MLJOB_4BC196AD_D99B_49A3_98EB_13D55A174F95     DONE   \n",
       "9  MLJOB_837589B6_6D0F_4AC2_89B5_97622C8CCA14     DONE   \n",
       "\n",
       "                       message        database_name    schema_name     owner  \\\n",
       "0                               HEADLESS_STARTER_DB  HEADLESS_DEMO  SYSADMIN   \n",
       "1                               HEADLESS_STARTER_DB  HEADLESS_DEMO  SYSADMIN   \n",
       "2                               HEADLESS_STARTER_DB  HEADLESS_DEMO  SYSADMIN   \n",
       "3                               HEADLESS_STARTER_DB  HEADLESS_DEMO  SYSADMIN   \n",
       "4                               HEADLESS_STARTER_DB  HEADLESS_DEMO  SYSADMIN   \n",
       "5                               HEADLESS_STARTER_DB  HEADLESS_DEMO  SYSADMIN   \n",
       "6                               HEADLESS_STARTER_DB  HEADLESS_DEMO  SYSADMIN   \n",
       "7                               HEADLESS_STARTER_DB  HEADLESS_DEMO  SYSADMIN   \n",
       "8  Job completed successfully.  HEADLESS_STARTER_DB  HEADLESS_DEMO  SYSADMIN   \n",
       "9  Job completed successfully.  HEADLESS_STARTER_DB  HEADLESS_DEMO  SYSADMIN   \n",
       "\n",
       "    compute_pool target_instances                     created_time  \\\n",
       "0  DEMO_POOL_CPU                1 2025-06-24 14:03:40.897000-07:00   \n",
       "1  DEMO_POOL_CPU                1 2025-06-24 14:03:35.318000-07:00   \n",
       "2  DEMO_POOL_CPU                1 2025-06-24 14:03:29.767000-07:00   \n",
       "3  DEMO_POOL_CPU                1 2025-06-24 14:03:24.145000-07:00   \n",
       "4  DEMO_POOL_CPU                1 2025-06-24 14:03:18.936000-07:00   \n",
       "5  DEMO_POOL_CPU                1 2025-06-24 14:03:09.669000-07:00   \n",
       "6  DEMO_POOL_CPU                1 2025-06-24 14:03:04.633000-07:00   \n",
       "7  DEMO_POOL_CPU                1 2025-06-24 14:02:59.467000-07:00   \n",
       "8  DEMO_POOL_CPU                1 2025-06-24 14:02:53.670000-07:00   \n",
       "9  DEMO_POOL_CPU                1 2025-06-24 14:02:48.254000-07:00   \n",
       "\n",
       "                    completed_time  \n",
       "0                              NaT  \n",
       "1                              NaT  \n",
       "2                              NaT  \n",
       "3                              NaT  \n",
       "4                              NaT  \n",
       "5                              NaT  \n",
       "6                              NaT  \n",
       "7                              NaT  \n",
       "8 2025-06-24 14:03:35.249000-07:00  \n",
       "9 2025-06-24 14:03:31.750000-07:00  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from snowflake.ml.jobs import list_jobs\n",
    "\n",
    "list_jobs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e6f178-e80d-4101-82f1-a4287aac4021",
   "metadata": {},
   "outputs": [],
   "source": [
    "# session.sql(f\"DROP SCHEMA {schema_name}\").collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10",
   "language": "python",
   "name": "py310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
