{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a6496bf-b928-4069-adfa-083299a21a13",
   "metadata": {},
   "source": [
    "### Set up Snowpark Session\n",
    "\n",
    "See [Configure Connections](https://docs.snowflake.com/developer-guide/snowflake-cli/connecting/configure-connections#define-connections)\n",
    "for information on how to define default Snowflake connection(s) in a config.toml\n",
    "file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f9880be-f3c2-4100-a35c-0e72595f3bdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<snowflake.snowpark.session.Session: account=\"DEMO_ACCOUNT\", role=\"DEMO_RL\", database=\"DEMO_DEV\", schema=\"DEMO_DEV\", warehouse=\"DEMO_WH\">\n"
     ]
    }
   ],
   "source": [
    "from snowflake.snowpark import Session, Row\n",
    "\n",
    "# Requires valid ~/.snowflake/config.toml file\n",
    "session = Session.builder.getOrCreate()\n",
    "print(session)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa9a253-38c5-4641-a01a-eaf899d24199",
   "metadata": {},
   "source": [
    "#### Set up Snowflake resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "46e5fad4-748b-4390-b4fa-748a10547835",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_name = \"HEADLESS_DEMO\"\n",
    "session.sql(f\"CREATE SCHEMA IF NOT EXISTS {schema_name}\").collect()\n",
    "session.use_schema(schema_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7fb47093-9b23-492d-a135-5722729a0c7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(status='DEMO_POOL_CPU already exists, statement succeeded.')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create compute pool\n",
    "def create_compute_pool(name: str, instance_family: str, min_nodes: int = 1, max_nodes: int = 10) -> list[Row]:\n",
    "    query = f\"\"\"\n",
    "        CREATE COMPUTE POOL IF NOT EXISTS {name}\n",
    "            MIN_NODES = {min_nodes}\n",
    "            MAX_NODES = {max_nodes}\n",
    "            INSTANCE_FAMILY = {instance_family}\n",
    "    \"\"\"\n",
    "    return session.sql(query).collect()\n",
    "\n",
    "compute_pool = \"DEMO_POOL_CPU\"\n",
    "create_compute_pool(compute_pool, \"CPU_X64_S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "84804c16-a359-4e5b-9037-207cc9675b75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(status='LOAN_APPLICATIONS already exists, statement succeeded.')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate synthetic data\n",
    "def generate_data(table_name: str, num_rows: int, replace: bool = False) -> list[Row]:\n",
    "    query = f\"\"\"\n",
    "        CREATE{\" OR REPLACE\" if replace else \"\"} TABLE{\"\" if replace else \" IF NOT EXISTS\"} {table_name} AS\n",
    "        SELECT \n",
    "            ROW_NUMBER() OVER (ORDER BY RANDOM()) as application_id,\n",
    "            ROUND(NORMAL(40, 10, RANDOM())) as age,\n",
    "            ROUND(NORMAL(65000, 20000, RANDOM())) as income,\n",
    "            ROUND(NORMAL(680, 50, RANDOM())) as credit_score,\n",
    "            ROUND(NORMAL(5, 2, RANDOM())) as employment_length,\n",
    "            ROUND(NORMAL(25000, 8000, RANDOM())) as loan_amount,\n",
    "            ROUND(NORMAL(35, 10, RANDOM()), 2) as debt_to_income,\n",
    "            ROUND(NORMAL(5, 2, RANDOM())) as number_of_credit_lines,\n",
    "            GREATEST(0, ROUND(NORMAL(1, 1, RANDOM()))) as previous_defaults,\n",
    "            ARRAY_CONSTRUCT(\n",
    "                'home_improvement', 'debt_consolidation', 'business', 'education',\n",
    "                'major_purchase', 'medical', 'vehicle', 'other'\n",
    "            )[UNIFORM(1, 8, RANDOM())] as loan_purpose,\n",
    "            RANDOM() < 0.15 as is_default,\n",
    "            TIMEADD(\"MINUTE\", UNIFORM(-525600, 0, RANDOM()), CURRENT_TIMESTAMP()) as created_at\n",
    "        FROM TABLE(GENERATOR(rowcount => {num_rows}))\n",
    "        ORDER BY created_at;\n",
    "    \"\"\"\n",
    "    return session.sql(query).collect()\n",
    "\n",
    "table_name = \"loan_applications\"\n",
    "generate_data(table_name, 1e5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6783667-50b2-4f41-a72c-3969e431f858",
   "metadata": {},
   "source": [
    "### Prepare Model Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f6ba7a8b-a944-4149-a46e-48196350b70e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pickle\n",
    "from time import perf_counter\n",
    "from typing import Literal, Optional\n",
    "\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import accuracy_score, classification_report, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "\n",
    "\n",
    "from snowflake.ml.data.data_connector import DataConnector\n",
    "from snowflake.ml.registry import Registry as ModelRegistry\n",
    "from snowflake.snowpark import Session\n",
    "\n",
    "\n",
    "def create_data_connector(session, table_name: str) -> DataConnector:\n",
    "    \"\"\"Load data from Snowflake table\"\"\"\n",
    "    # Example query - modify according to your schema\n",
    "    query = f\"\"\"\n",
    "    SELECT\n",
    "        age,\n",
    "        income,\n",
    "        credit_score,\n",
    "        employment_length,\n",
    "        loan_amount,\n",
    "        debt_to_income,\n",
    "        number_of_credit_lines,\n",
    "        previous_defaults,\n",
    "        loan_purpose,\n",
    "        is_default\n",
    "    FROM {table_name}\n",
    "    \"\"\"\n",
    "    sp_df = session.sql(query)\n",
    "    return DataConnector.from_dataframe(sp_df)\n",
    "\n",
    "\n",
    "def build_pipeline(**model_params) -> Pipeline:\n",
    "    \"\"\"Create pipeline with preprocessors and model\"\"\"\n",
    "    # Define column types\n",
    "    categorical_cols = [\"LOAN_PURPOSE\"]\n",
    "    numerical_cols = [\n",
    "        \"AGE\",\n",
    "        \"INCOME\",\n",
    "        \"CREDIT_SCORE\",\n",
    "        \"EMPLOYMENT_LENGTH\",\n",
    "        \"LOAN_AMOUNT\",\n",
    "        \"DEBT_TO_INCOME\",\n",
    "        \"NUMBER_OF_CREDIT_LINES\",\n",
    "        \"PREVIOUS_DEFAULTS\",\n",
    "    ]\n",
    "\n",
    "    # Numerical preprocessing pipeline\n",
    "    numeric_transformer = Pipeline(\n",
    "        steps=[\n",
    "            (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "            (\"scaler\", StandardScaler()),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Categorical preprocessing pipeline\n",
    "    categorical_transformer = Pipeline(\n",
    "        steps=[\n",
    "            (\"imputer\", SimpleImputer(strategy=\"constant\", fill_value=\"missing\")),\n",
    "            (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Combine transformers\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"num\", numeric_transformer, numerical_cols),\n",
    "            (\"cat\", categorical_transformer, categorical_cols),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Define model parameters\n",
    "    default_params = {\n",
    "        \"objective\": \"binary:logistic\",\n",
    "        \"eval_metric\": \"auc\",\n",
    "        \"max_depth\": 6,\n",
    "        \"learning_rate\": 0.1,\n",
    "        \"n_estimators\": 100,\n",
    "        \"subsample\": 0.8,\n",
    "        \"colsample_bytree\": 0.8,\n",
    "        \"random_state\": 42,\n",
    "    }\n",
    "    model = xgb.XGBClassifier(**(model_params or default_params))\n",
    "\n",
    "    return Pipeline([(\"preprocessor\", preprocessor), (\"classifier\", model)])\n",
    "\n",
    "\n",
    "def evaluate_model(model: Pipeline, X_test: pd.DataFrame, y_test: pd.DataFrame):\n",
    "    \"\"\"Evaluate model performance\"\"\"\n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    # Calculate metrics\n",
    "    metrics = {\n",
    "        \"accuracy\": accuracy_score(y_test, y_pred),\n",
    "        \"roc_auc\": roc_auc_score(y_test, y_pred_proba),\n",
    "        \"classification_report\": classification_report(y_test, y_pred),\n",
    "    }\n",
    "\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def save_to_registry(\n",
    "    session: Session,\n",
    "    model: Pipeline,\n",
    "    model_name: str,\n",
    "    metrics: dict,\n",
    "    sample_input_data: pd.DataFrame,\n",
    "):\n",
    "    \"\"\"Save model and artifacts to Snowflake Model Registry\"\"\"\n",
    "    # Initialize model registry\n",
    "    registry = ModelRegistry(session)\n",
    "\n",
    "    # Save to registry\n",
    "    registry.log_model(\n",
    "        model=model,\n",
    "        model_name=model_name,\n",
    "        metrics=metrics,\n",
    "        sample_input_data=sample_input_data[:5],\n",
    "        conda_dependencies=[\"xgboost\"],\n",
    "    )\n",
    "\n",
    "\n",
    "def train(session: Session, source_data: str, save_mode: Literal[\"local\", \"registry\"] = \"local\", output_dir: Optional[str] = None, **kwargs):\n",
    "    # Load data\n",
    "    dc = create_data_connector(session, table_name=source_data)\n",
    "    print(\"Loading data...\", end=\"\", flush=True)\n",
    "    start = perf_counter()\n",
    "    df = dc.to_pandas()\n",
    "    elapsed = perf_counter() - start\n",
    "    print(f\" done! Loaded {len(df)} rows, elapsed={elapsed:.3f}s\")\n",
    "\n",
    "    # Split data\n",
    "    X = df.drop(\"IS_DEFAULT\", axis=1)\n",
    "    y = df[\"IS_DEFAULT\"]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42\n",
    "    )\n",
    "\n",
    "    # Train model\n",
    "    model = build_pipeline()\n",
    "    print(\"Training model...\", end=\"\")\n",
    "    start = perf_counter()\n",
    "    model.fit(X_train, y_train)\n",
    "    elapsed = perf_counter() - start\n",
    "    print(f\" done! Elapsed={elapsed:.3f}s\")\n",
    "\n",
    "    # Evaluate model\n",
    "    print(\"Evaluating model...\", end=\"\")\n",
    "    start = perf_counter()\n",
    "    metrics = evaluate_model(\n",
    "        model,\n",
    "        X_test,\n",
    "        y_test,\n",
    "    )\n",
    "    elapsed = perf_counter() - start\n",
    "    print(f\" done! Elapsed={elapsed:.3f}s\")\n",
    "\n",
    "    # Print evaluation results\n",
    "    print(\"\\nModel Performance Metrics:\")\n",
    "    print(f\"Accuracy: {metrics['accuracy']:.4f}\")\n",
    "    print(f\"ROC AUC: {metrics['roc_auc']:.4f}\")\n",
    "    # Uncomment below for full classification report\n",
    "    # print(\"\\nClassification Report:\")\n",
    "    # print(metrics[\"classification_report\"])\n",
    "\n",
    "    start = perf_counter()\n",
    "    if save_mode == \"local\":\n",
    "        # Save model locally\n",
    "        print(\"Saving model to disk...\", end=\"\")\n",
    "        output_dir = output_dir or '.'\n",
    "        model_subdir = os.environ.get(\"SNOWFLAKE_SERVICE_NAME\", \"output\")\n",
    "        model_dir = os.path.join(output_dir, model_subdir) if not output_dir.endswith(model_subdir) else output_dir\n",
    "        os.makedirs(model_dir, exist_ok=True)\n",
    "        with open(os.path.join(model_dir, \"model.pkl\"), \"wb\") as f:\n",
    "            pickle.dump(model, f)\n",
    "        with open(os.path.join(model_dir, \"metrics.json\"), \"w\") as f:\n",
    "            json.dump(metrics, f, indent=2)\n",
    "    elif save_mode == \"registry\":\n",
    "        # Save model to registry\n",
    "        print(\"Logging model to Model Registry...\", end=\"\")\n",
    "        save_to_registry(\n",
    "            session,\n",
    "            model=model,\n",
    "            model_name=\"loan_default_predictor\",\n",
    "            metrics=metrics,\n",
    "            sample_input_data=X_train,\n",
    "        )\n",
    "    elapsed = perf_counter() - start\n",
    "    print(f\" done! Elapsed={elapsed:.3f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "897e6b37-0cbc-4878-8e06-a6dae60b6249",
   "metadata": {},
   "source": [
    "### Run training locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ef1c3106-7e4a-4c7c-82e1-0fd1f6de417f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DataConnector.from_dataframe() is in private preview since 1.6.0. Do not use it in production. \n",
      "DataConnector.from_sql() is in private preview since 1.7.3. Do not use it in production. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data... done! Loaded 10000000 rows, elapsed=21.933s\n",
      "Training model... done! Elapsed=16.225s\n",
      "Evaluating model... done! Elapsed=3.842s\n",
      "\n",
      "Model Performance Metrics:\n",
      "Accuracy: 0.5003\n",
      "ROC AUC: 0.5000\n",
      "Saving model to disk... done! Elapsed=0.004s\n"
     ]
    }
   ],
   "source": [
    "train(session, table_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e84a9f41-1113-47f5-9041-a8cf1f904a9f",
   "metadata": {},
   "source": [
    "### Train with remote SPCS instance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c2940e5a-4f85-46d9-9b49-aed6ac79fb53",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "remote() is in private preview since 1.7.4. Do not use it in production. \n"
     ]
    }
   ],
   "source": [
    "from snowflake.ml.jobs import remote\n",
    "\n",
    "@remote(compute_pool, stage_name=\"payload_stage\")\n",
    "def train_remote(source_data: str, save_mode: str = \"local\", output_dir: str = None):\n",
    "    # Retrieve session from SPCS service context\n",
    "    session = Session.builder.getOrCreate()\n",
    "\n",
    "    # Run training script\n",
    "    train(session, source_data, save_mode, output_dir)\n",
    "\n",
    "train_job = train_remote(table_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3eb1ad66-851f-4fdb-887c-4dcaaf5bf2c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLJOB_D686BEB4_91ED_4DE7_BFD6_3FB38DCAF972\n",
      "PENDING\n"
     ]
    }
   ],
   "source": [
    "print(train_job.id)\n",
    "print(train_job.status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1909e552-d289-4bf4-8840-926d99295acb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MLJob.wait() is in private preview since 1.7.4. Do not use it in production. \n",
      "MLJob.show_logs() is in private preview since 1.7.4. Do not use it in production. \n",
      "MLJob.get_logs() is in private preview since 1.7.4. Do not use it in production. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "'micromamba' is running as a subprocess and can't modify the parent shell.\n",
      "Thus you must initialize your shell before using activate and deactivate.\n",
      "\n",
      "To initialize the current bash shell, run:\n",
      "    $ eval \"$(micromamba shell hook --shell bash)\"\n",
      "and then activate or deactivate with:\n",
      "    $ micromamba activate\n",
      "To automatically initialize all future (bash) shells, run:\n",
      "    $ micromamba shell init --shell bash --root-prefix=~/micromamba\n",
      "If your shell was already initialized, reinitialize your shell with:\n",
      "    $ micromamba shell reinit --shell bash\n",
      "Otherwise, this may be an issue. In the meantime you can run commands. See:\n",
      "    $ micromamba run --help\n",
      "\n",
      "Supported shells are {bash, zsh, csh, xonsh, cmd.exe, powershell, fish}.\n",
      "Creating log directories...\n",
      " * Starting periodic command scheduler cron\n",
      "   ...done.\n",
      "2025-04-24 23:53:18,741\tINFO usage_lib.py:441 -- Usage stats collection is disabled.\n",
      "2025-04-24 23:53:18,742\tINFO scripts.py:767 -- \u001b[37mLocal node IP\u001b[39m: \u001b[1m10.244.64.74\u001b[22m\n",
      "2025-04-24 23:53:20,014\tSUCC scripts.py:804 -- \u001b[32m--------------------\u001b[39m\n",
      "2025-04-24 23:53:20,015\tSUCC scripts.py:805 -- \u001b[32mRay runtime started.\u001b[39m\n",
      "2025-04-24 23:53:20,015\tSUCC scripts.py:806 -- \u001b[32m--------------------\u001b[39m\n",
      "2025-04-24 23:53:20,015\tINFO scripts.py:808 -- \u001b[36mNext steps\u001b[39m\n",
      "2025-04-24 23:53:20,015\tINFO scripts.py:811 -- To add another node to this Ray cluster, run\n",
      "2025-04-24 23:53:20,015\tINFO scripts.py:814 -- \u001b[1m  ray start --address='10.244.64.74:12001'\u001b[22m\n",
      "2025-04-24 23:53:20,015\tINFO scripts.py:823 -- To connect to this Ray cluster:\n",
      "2025-04-24 23:53:20,015\tINFO scripts.py:825 -- \u001b[35mimport\u001b[39m\u001b[26m ray\n",
      "2025-04-24 23:53:20,015\tINFO scripts.py:826 -- ray\u001b[35m.\u001b[39m\u001b[26minit(_node_ip_address\u001b[35m=\u001b[39m\u001b[26m\u001b[33m'10.244.64.74'\u001b[39m\u001b[26m)\n",
      "2025-04-24 23:53:20,015\tINFO scripts.py:838 -- To submit a Ray job using the Ray Jobs CLI:\n",
      "2025-04-24 23:53:20,015\tINFO scripts.py:839 -- \u001b[1m  RAY_ADDRESS='http://10.244.64.74:12003' ray job submit --working-dir . -- python my_script.py\u001b[22m\n",
      "2025-04-24 23:53:20,015\tINFO scripts.py:848 -- See https://docs.ray.io/en/latest/cluster/running-applications/job-submission/index.html \n",
      "2025-04-24 23:53:20,015\tINFO scripts.py:852 -- for more information on submitting Ray jobs to the Ray cluster.\n",
      "2025-04-24 23:53:20,015\tINFO scripts.py:857 -- To terminate the Ray runtime, run\n",
      "2025-04-24 23:53:20,015\tINFO scripts.py:858 -- \u001b[1m  ray stop\u001b[22m\n",
      "2025-04-24 23:53:20,015\tINFO scripts.py:861 -- To view the status of the cluster, use\n",
      "2025-04-24 23:53:20,015\tINFO scripts.py:862 --   \u001b[1mray status\u001b[22m\u001b[26m\n",
      "2025-04-24 23:53:20,015\tINFO scripts.py:866 -- To monitor and debug Ray, view the dashboard at \n",
      "2025-04-24 23:53:20,015\tINFO scripts.py:867 --   \u001b[1m10.244.64.74:12003\u001b[22m\u001b[26m\n",
      "2025-04-24 23:53:20,015\tINFO scripts.py:874 -- \u001b[4mIf connection to the dashboard fails, check your firewall settings and network configuration.\u001b[24m\n",
      "Running command: python /mnt/app/mljob_launcher.py /mnt/app/func.py --script_main_func func\n",
      "SnowflakeLoginOptions() is in private preview since 0.2.0. Do not use it in production. \n",
      "DataConnector.from_dataframe() is in private preview since 1.6.0. Do not use it in production. \n",
      "DataConnector.from_sql() is in private preview since 1.7.3. Do not use it in production. \n",
      "2025-04-24 23:53:25,253\tINFO worker.py:1601 -- Connecting to existing Ray cluster at address: 10.244.64.74:12001...\n",
      "2025-04-24 23:53:25,263\tINFO worker.py:1777 -- Connected to Ray cluster. View the dashboard at \u001b[1m\u001b[32m10.244.64.74:12003 \u001b[39m\u001b[22m\n",
      "2025-04-24 23:53:26,768\tINFO streaming_executor.py:108 -- Starting execution of Dataset. Full logs are in /tmp/ray/session_2025-04-24_23-53-18_742719_43/logs/ray-data\n",
      "2025-04-24 23:53:26,768\tINFO streaming_executor.py:109 -- Execution plan of Dataset: InputDataBuffer[Input] -> TaskPoolMapOperator[ReadResultSetDataSource]\n",
      "Running 0: 0.00 row [00:00, ? row/s]\n",
      "Running. Resources: 3/3 CPU, 0/0 GPU, 768.0MB/1.9GB object_store_memory (pending: 0 CPU, 0 GPU): : 0.00 row [00:01, ? row/s]\n",
      "- ReadResultSetDataSource: 3 active, 197 queued 🚧, [cpu: 3.0, objects: 768.0MB]: : 0.00 row [00:01, ? row/s]\u001b[A\n",
      "Running. Resources: 3/3 CPU, 0/0 GPU, 768.0MB/1.9GB object_store_memory (pending: 0 CPU, 0 GPU): : 0.00 row [00:02, ? row/s]\n",
      "                                                                                                                            \n",
      "\u001b[A\u001b[36m(ReadResultSetDataSource pid=299)\u001b[0m SnowflakeLoginOptions() is in private preview since 0.2.0. Do not use it in production. \n",
      "Running. Resources: 3/3 CPU, 0/0 GPU, 768.0MB/1.9GB object_store_memory (pending: 0 CPU, 0 GPU): : 0.00 row [00:03, ? row/s]\n",
      "- ReadResultSetDataSource: 3 active, 197 queued 🚧, [cpu: 3.0, objects: 768.0MB]: : 0.00 row [00:03, ? row/s]\u001b[A\n",
      "                                                                                                                            \n",
      "✔️  Dataset execution finished in 3.40 seconds: : 0.00 row [00:03, ? row/s]                                  \n",
      "\n",
      "- ReadResultSetDataSource: 3 active, 197 queued 🚧, [cpu: 3.0, objects: 768.0MB]: : 0.00 row [00:03, ? row/s]\u001b[A\n",
      "- ReadResultSetDataSource: 3 active, 196 queued 🚧, [cpu: 3.0, objects: 384.0MB]: : 0.00 row [00:03, ? row/s]\u001b[A\n",
      "- ReadResultSetDataSource: 3 active, 196 queued 🚧, [cpu: 3.0, objects: 384.0MB]: : 0.00 row [00:03, ? row/s]\u001b[A\n",
      "- ReadResultSetDataSource: 3 active, 196 queued 🚧, [cpu: 3.0, objects: 384.0MB]: : 0.00 row [00:03, ? row/s]\u001b[A\n",
      "- ReadResultSetDataSource: 3 active, 196 queued 🚧, [cpu: 3.0, objects: 384.0MB]: : 0.00 row [00:03, ? row/s]\n",
      "2025-04-24 23:53:30,177\tINFO streaming_executor.py:108 -- Starting execution of Dataset. Full logs are in /tmp/ray/session_2025-04-24_23-53-18_742719_43/logs/ray-data\n",
      "2025-04-24 23:53:30,177\tINFO streaming_executor.py:109 -- Execution plan of Dataset: InputDataBuffer[Input] -> TaskPoolMapOperator[ReadResultSetDataSource]\n",
      "Running 0: 0.00 row [00:00, ? row/s]\n",
      "Running. Resources: 2/3 CPU, 0/0 GPU, 7.9MB/1.9GB object_store_memory (pending: 0 CPU, 0 GPU):   4%|▍         | 774k/19.3M [00:01<00:25, 729k row/s]\n",
      "- ReadResultSetDataSource: 3 active, 188 queued 🚧, [cpu: 3.0, objects: 11.8MB]: : 0.00 row [00:01, ? row/s]\u001b[A\n",
      "- ReadResultSetDataSource: 3 active, 188 queued 🚧, [cpu: 3.0, objects: 11.8MB]:   0%|          | 0.00/22.7M [00:01<?, ? row/s]\u001b[A\n",
      "- ReadResultSetDataSource: 3 active, 188 queued 🚧, [cpu: 3.0, objects: 11.8MB]:   5%|▌         | 1.24M/22.7M [00:01<00:17, 1.21M row/s]\u001b[A\n",
      "Running. Resources: 1/3 CPU, 0/0 GPU, 13.0MB/1.9GB object_store_memory (pending: 0 CPU, 0 GPU):  11%|█         | 2.48M/22.5M [00:02<00:15, 1.28M row/s]\n",
      "- ReadResultSetDataSource: 3 active, 172 queued 🚧, [cpu: 3.0, objects: 14.5MB]:   5%|▌         | 1.24M/22.7M [00:02<00:17, 1.21M row/s]\u001b[A\n",
      "- ReadResultSetDataSource: 3 active, 172 queued 🚧, [cpu: 3.0, objects: 14.5MB]:   6%|▌         | 1.24M/21.4M [00:02<00:16, 1.21M row/s]\u001b[A\n",
      "- ReadResultSetDataSource: 3 active, 172 queued 🚧, [cpu: 3.0, objects: 14.5MB]:  12%|█▏        | 2.66M/21.4M [00:02<00:14, 1.32M row/s]\u001b[A\n",
      "Running. Resources: 1/3 CPU, 0/0 GPU, 28.7MB/1.9GB object_store_memory (pending: 0 CPU, 0 GPU):  19%|█▉        | 4.52M/23.2M [00:03<00:11, 1.60M row/s]\n",
      "- ReadResultSetDataSource: 3 active, 156 queued 🚧, [cpu: 3.0, objects: 39.8MB]:  12%|█▏        | 2.66M/21.4M [00:03<00:14, 1.32M row/s]\u001b[A\n",
      "- ReadResultSetDataSource: 3 active, 156 queued 🚧, [cpu: 3.0, objects: 39.8MB]:  11%|█         | 2.66M/24.2M [00:03<00:16, 1.32M row/s]\u001b[A\n",
      "- ReadResultSetDataSource: 3 active, 156 queued 🚧, [cpu: 3.0, objects: 39.8MB]:  21%|██        | 5.08M/24.2M [00:03<00:10, 1.80M row/s]\u001b[A\n",
      "Running. Resources: 1/3 CPU, 0/0 GPU, 6.6MB/1.9GB object_store_memory (pending: 0 CPU, 0 GPU):  28%|██▊       | 6.99M/25.4M [00:04<00:09, 1.92M row/s] \n",
      "- ReadResultSetDataSource: 3 active, 140 queued 🚧, [cpu: 3.0, objects: 17.8MB]:  21%|██        | 5.08M/24.2M [00:04<00:10, 1.80M row/s]\u001b[A\n",
      "- ReadResultSetDataSource: 3 active, 140 queued 🚧, [cpu: 3.0, objects: 17.8MB]:  20%|█▉        | 5.08M/25.6M [00:04<00:11, 1.80M row/s]\u001b[A\n",
      "- ReadResultSetDataSource: 3 active, 140 queued 🚧, [cpu: 3.0, objects: 17.8MB]:  29%|██▉       | 7.52M/25.6M [00:04<00:08, 2.02M row/s]\u001b[A\n",
      "Running. Resources: 2/3 CPU, 0/0 GPU, 11.5MB/1.9GB object_store_memory (pending: 0 CPU, 0 GPU):  35%|███▌      | 9.14M/25.7M [00:05<00:08, 1.98M row/s]\n",
      "- ReadResultSetDataSource: 3 active, 125 queued 🚧, [cpu: 3.0, objects: 17.2MB]:  29%|██▉       | 7.52M/25.6M [00:05<00:08, 2.02M row/s]\u001b[A\n",
      "- ReadResultSetDataSource: 3 active, 125 queued 🚧, [cpu: 3.0, objects: 17.2MB]:  29%|██▉       | 7.52M/25.7M [00:05<00:09, 2.02M row/s]\u001b[A\n",
      "- ReadResultSetDataSource: 3 active, 125 queued 🚧, [cpu: 3.0, objects: 17.2MB]:  37%|███▋      | 9.53M/25.7M [00:05<00:08, 2.00M row/s]\u001b[A\n",
      "                                                                                                                                                       \n",
      "✔️  Dataset execution finished in 5.63 seconds: 100%|██████████| 10.0M/10.0M [00:05<00:00, 1.78M row/s]                                 \n",
      "\n",
      "- ReadResultSetDataSource: 3 active, 125 queued 🚧, [cpu: 3.0, objects: 17.2MB]:  37%|███▋      | 9.53M/25.7M [00:05<00:08, 2.00M row/s]\u001b[A\n",
      "- ReadResultSetDataSource: 0 active, 0 queued, [cpu: 0.0, objects: 0.0B]:  37%|███▋      | 9.53M/25.7M [00:05<00:08, 2.00M row/s]       \u001b[A\n",
      "- ReadResultSetDataSource: 0 active, 0 queued, [cpu: 0.0, objects: 0.0B]:  95%|█████████▌| 9.53M/10.0M [00:05<00:00, 2.00M row/s]\u001b[A\n",
      "- ReadResultSetDataSource: 0 active, 0 queued, [cpu: 0.0, objects: 0.0B]: 100%|██████████| 10.0M/10.0M [00:05<00:00, 1.88M row/s]\u001b[A\n",
      "- ReadResultSetDataSource: 0 active, 0 queued, [cpu: 0.0, objects: 0.0B]: 100%|██████████| 10.0M/10.0M [00:05<00:00, 1.88M row/s]\u001b[A\n",
      "- ReadResultSetDataSource: 0 active, 0 queued, [cpu: 0.0, objects: 0.0B]: 100%|██████████| 10.0M/10.0M [00:05<00:00, 1.82M row/s]\n",
      "Loading data... done! Loaded 10000000 rows, elapsed=11.149s\n",
      "Training model... done! Elapsed=45.842s\n",
      "Evaluating model... done! Elapsed=6.274s\n",
      "\n",
      "Model Performance Metrics:\n",
      "Accuracy: 0.4999\n",
      "ROC AUC: 0.5002\n",
      "Saving model to disk... done! Elapsed=0.526s\n",
      "\u001b[36m(ReadResultSetDataSource pid=300)\u001b[0m SnowflakeLoginOptions() is in private preview since 0.2.0. Do not use it in production. \u001b[32m [repeated 2x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)\u001b[0m\n",
      "User job completed. Signaling workers to shut down...\n",
      "2025-04-24 23:54:32,173\tINFO worker.py:1601 -- Connecting to existing Ray cluster at address: 10.244.64.74:12001...\n",
      "2025-04-24 23:54:32,182\tINFO worker.py:1777 -- Connected to Ray cluster. View the dashboard at \u001b[1m\u001b[32m10.244.64.74:12003 \u001b[39m\u001b[22m\n",
      "2025-04-24 23:54:32,201 - INFO - No active worker nodes found\n",
      "2025-04-24 23:54:32,201 - INFO - No active worker nodes found to signal.\n",
      "Head node job completed. Exiting.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_job.wait()\n",
    "train_job.show_logs()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe67e0b-e73f-4c90-8a22-f7b7fce0bfe2",
   "metadata": {},
   "source": [
    "### Run concurrent training jobs on SPCS\n",
    "\n",
    "Suppose we want to train multiple models on different datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5c1be7c9-1512-48f5-adc0-630102e4e41e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating datasets\n",
      "Generated datasets: ['loan_applications_0', 'loan_applications_1', 'loan_applications_2', 'loan_applications_3', 'loan_applications_4', 'loan_applications_5', 'loan_applications_6', 'loan_applications_7', 'loan_applications_8', 'loan_applications_9']\n",
      "Starting training jobs\n",
      "Started 10 training jobs\n"
     ]
    }
   ],
   "source": [
    "datasets = []\n",
    "print(\"Generating datasets\")\n",
    "for i in range(10):\n",
    "    dataset = f\"loan_applications_{i}\"\n",
    "    generate_data(dataset, 1e6)\n",
    "    datasets.append(dataset)\n",
    "print(f\"Generated datasets: {datasets}\")\n",
    "    \n",
    "print(\"Starting training jobs\")\n",
    "train_jobs = []\n",
    "for ds in datasets:\n",
    "    train_jobs.append(train_remote(ds))\n",
    "print(f\"Started {len(train_jobs)} training jobs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "75619ff2-70fb-4237-b183-25d6627b902a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------------------------------------------------------\n",
      "|\"id\"                                        |\"owner\"   |\"status\"  |\"created_on\"                      |\"compute_pool\"  |\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "|MLJOB_2B691E83_2D52_49D1_ACC9_DB0C951017FB  |ENGINEER  |RUNNING   |2025-04-24 16:55:48.851000-07:00  |DEMO_POOL_CPU   |\n",
      "|MLJOB_B6B11A30_EAC9_4BBB_A41F_B47827C8013A  |ENGINEER  |RUNNING   |2025-04-24 16:55:43.606000-07:00  |DEMO_POOL_CPU   |\n",
      "|MLJOB_75B596F9_5674_4636_B9F9_67E036E67DEA  |ENGINEER  |RUNNING   |2025-04-24 16:55:32.928000-07:00  |DEMO_POOL_CPU   |\n",
      "|MLJOB_56239D4C_F6FA_4691_8ABC_FCA5BA8E58B6  |ENGINEER  |DONE      |2025-04-24 16:55:27.958000-07:00  |DEMO_POOL_CPU   |\n",
      "|MLJOB_6A0DB4D4_29F8_4CFB_BD46_9FB0966CBC5F  |ENGINEER  |DONE      |2025-04-24 16:55:17.525000-07:00  |DEMO_POOL_CPU   |\n",
      "|MLJOB_BF15DA6D_8192_4831_A92F_A33483A0E8FB  |ENGINEER  |DONE      |2025-04-08 17:19:39.188000-07:00  |DEMO_POOL_CPU   |\n",
      "|MLJOB_11E30859_AC22_4F1B_A474_0354370924EF  |ENGINEER  |DONE      |2025-04-08 17:14:12.860000-07:00  |DEMO_POOL_CPU   |\n",
      "|MLJOB_BD8AC0A7_5417_4CCB_A8F7_0755D8E8181D  |ENGINEER  |DONE      |2025-04-08 17:10:24.612000-07:00  |DEMO_POOL_CPU   |\n",
      "|MLJOB_8C8CA499_1608_4DA2_8540_A2E2F6B2D8EC  |ENGINEER  |DONE      |2025-04-08 17:00:21.249000-07:00  |DEMO_POOL_CPU   |\n",
      "|MLJOB_86ADF76C_5797_4EDC_9130_B5167FD5537B  |ENGINEER  |DONE      |2025-04-08 16:54:16.596000-07:00  |DEMO_POOL_CPU   |\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from snowflake.ml.jobs import list_jobs\n",
    "\n",
    "list_jobs().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "21e6f178-e80d-4101-82f1-a4287aac4021",
   "metadata": {},
   "outputs": [],
   "source": [
    "# session.sql(f\"DROP SCHEMA {schema_name}\").collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
