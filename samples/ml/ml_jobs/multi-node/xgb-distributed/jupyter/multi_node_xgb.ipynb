{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a6496bf-b928-4069-adfa-083299a21a13",
   "metadata": {},
   "source": [
    "### Set up Snowpark Session\n",
    "\n",
    "See [Configure Connections](https://docs.snowflake.com/developer-guide/snowflake-cli/connecting/configure-connections#define-connections)\n",
    "for information on how to define default Snowflake connection(s) in a config.toml\n",
    "file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f9880be-f3c2-4100-a35c-0e72595f3bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from snowflake.snowpark import Session, Row\n",
    "\n",
    "# # Requires valid ~/.snowflake/config.toml file\n",
    "# session = Session.builder.getOrCreate()\n",
    "# print(session)\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from snowflake.ml.utils.connection_params import SnowflakeLoginOptions\n",
    "from snowflake.snowpark import Session\n",
    "\n",
    "session = Session.builder.configs(SnowflakeLoginOptions(\"preprod8\")).create()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa9a253-38c5-4641-a01a-eaf899d24199",
   "metadata": {},
   "source": [
    "#### Set up Snowflake resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e5fad4-748b-4390-b4fa-748a10547835",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use any database and schema you want\n",
    "session.sql(\"use database temp\").collect()\n",
    "session.sql(\"use schema public\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb47093-9b23-492d-a135-5722729a0c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create compute pool if not exists\n",
    "def create_compute_pool(name: str, instance_family: str, min_nodes: int = 1, max_nodes: int = 10):\n",
    "    query = f\"\"\"\n",
    "        CREATE COMPUTE POOL IF NOT EXISTS {name}\n",
    "            MIN_NODES = {min_nodes}\n",
    "            MAX_NODES = {max_nodes}\n",
    "            INSTANCE_FAMILY = {instance_family}\n",
    "    \"\"\"\n",
    "    return session.sql(query).collect()\n",
    "\n",
    "compute_pool = \"E2E_CPU_POOL\"\n",
    "create_compute_pool(compute_pool, \"CPU_X64_S\", 1, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da4833f",
   "metadata": {},
   "source": [
    "### Approach 1: Train with function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84804c16-a359-4e5b-9037-207cc9675b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a arbitary dataset\n",
    "def generate_dataset_sql(db, schema, table_name, num_rows, num_cols) -> str:\n",
    "    sql_script = f\"CREATE TABLE IF NOT EXISTS {db}.{schema}.{table_name} AS \\n\"\n",
    "    sql_script += f\"SELECT \\n\"\n",
    "    for i in range(1, num_cols):\n",
    "        sql_script += f\"uniform(0::FLOAT, 10::FLOAT, random()) AS FEATURE_{i}, \\n\"\n",
    "    sql_script += f\"FEATURE_1 + FEATURE_1 AS TARGET_1, \\n\"\n",
    "    sql_script += f\"FROM TABLE(generator(rowcount=>({num_rows})));\"\n",
    "    return sql_script\n",
    "num_rows = 1000 * 1000\n",
    "num_cols = 100\n",
    "table_name = \"MULTINODE_CPU_TRAIN_DS\"\n",
    "session.sql(generate_dataset_sql(session.get_current_database(), session.get_current_schema(), \n",
    "                                table_name, num_rows, num_cols)).collect()\n",
    "feature_list = [f'FEATURE_{num}' for num in range(1, num_cols)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2940e5a-4f85-46d9-9b49-aed6ac79fb53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from snowflake.ml.jobs import remote\n",
    "\n",
    "@remote(compute_pool, stage_name=\"payload_stage\", num_instances=3)\n",
    "def xgb(table_name, input_cols, label_col):\n",
    "    from snowflake.snowpark.context import get_active_session\n",
    "    from snowflake.ml.modeling.distributors.xgboost import XGBEstimator, XGBScalingConfig\n",
    "    from snowflake.ml.data.data_connector import DataConnector\n",
    "    from implementations.ray_data_ingester import RayDataIngester\n",
    "\n",
    "    def xgb_train(num_workers, num_cpu_per_worker):\n",
    "        session = get_active_session()\n",
    "        cpu_train_df = session.table(table_name)\n",
    "        \n",
    "        params = {\n",
    "            \"tree_method\": \"hist\",\n",
    "            \"objective\": \"reg:pseudohubererror\",\n",
    "            \"eta\": 1e-4,\n",
    "            \"subsample\": 0.5,\n",
    "            \"max_depth\": 50,\n",
    "            \"max_leaves\": 1000,\n",
    "            \"max_bin\":63,\n",
    "        }\n",
    "        scaling_config = XGBScalingConfig(\n",
    "            num_workers=num_workers, num_cpu_per_worker=num_cpu_per_worker, \n",
    "            use_gpu=False\n",
    "        )\n",
    "        estimator = XGBEstimator(\n",
    "            n_estimators=100,\n",
    "            params=params,\n",
    "            scaling_config=scaling_config,\n",
    "        )\n",
    "        data_connector = DataConnector.from_dataframe(\n",
    "            cpu_train_df, ingestor_class=RayDataIngester\n",
    "        )\n",
    "        # return data_connector\n",
    "        xgb_model = estimator.fit(\n",
    "            data_connector, input_cols=input_cols, label_col=label_col\n",
    "        )\n",
    "        return xgb_model\n",
    "    assert xgb_train(-1, -1) is not None\n",
    "    assert xgb_train(3, 4) is not None\n",
    "    assert xgb_train(6, 2) is not None\n",
    "    return 1\n",
    "\n",
    "# Function invocation returns a job handle (snowflake.ml.jobs.MLJob)\n",
    "job = xgb(table_name, feature_list, \"TARGET_1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb1ad66-851f-4fdb-887c-4dcaaf5bf2c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(job.id)\n",
    "print(job.status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1909e552-d289-4bf4-8840-926d99295acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "job.wait()\n",
    "job.show_logs()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fec6364",
   "metadata": {},
   "source": [
    "### Approach 2: Train with file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "563b185e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from snowflake.ml.jobs import submit_file\n",
    "\n",
    "job = submit_file(\n",
    "    \"../src/main.py\",\n",
    "    \"E2E_CPU_POOL\",\n",
    "    stage_name=\"multi_node_payload_stage\",\n",
    "    num_instances=3  # Specify multiple instances\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e79e4424",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(job.id)\n",
    "print(job.status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce63881",
   "metadata": {},
   "outputs": [],
   "source": [
    "job.wait()\n",
    "job.show_logs()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "headless_sample",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
