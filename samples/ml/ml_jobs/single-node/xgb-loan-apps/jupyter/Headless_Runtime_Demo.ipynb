{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a6496bf-b928-4069-adfa-083299a21a13",
   "metadata": {},
   "source": [
    "### Set up Snowpark Session\n",
    "\n",
    "See [Configure Connections](https://docs.snowflake.com/developer-guide/snowflake-cli/connecting/configure-connections#define-connections)\n",
    "for information on how to define default Snowflake connection(s) in a config.toml\n",
    "file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f9880be-f3c2-4100-a35c-0e72595f3bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from snowflake.snowpark import Session, Row\n",
    "\n",
    "# Requires valid ~/.snowflake/config.toml file\n",
    "session = Session.builder.getOrCreate()\n",
    "print(session)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa9a253-38c5-4641-a01a-eaf899d24199",
   "metadata": {},
   "source": [
    "#### Set up Snowflake resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "46e5fad4-748b-4390-b4fa-748a10547835",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_name = \"HEADLESS_DEMO\"\n",
    "session.sql(f\"CREATE SCHEMA IF NOT EXISTS {schema_name}\").collect()\n",
    "session.use_schema(schema_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7fb47093-9b23-492d-a135-5722729a0c7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(status='DEMO_POOL_CPU already exists, statement succeeded.')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create compute pool\n",
    "def create_compute_pool(name: str, instance_family: str, min_nodes: int = 1, max_nodes: int = 10) -> list[Row]:\n",
    "    query = f\"\"\"\n",
    "        CREATE COMPUTE POOL IF NOT EXISTS {name}\n",
    "            MIN_NODES = {min_nodes}\n",
    "            MAX_NODES = {max_nodes}\n",
    "            INSTANCE_FAMILY = {instance_family}\n",
    "    \"\"\"\n",
    "    return session.sql(query).collect()\n",
    "\n",
    "compute_pool = \"DEMO_POOL_CPU\"\n",
    "create_compute_pool(compute_pool, \"CPU_X64_S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "84804c16-a359-4e5b-9037-207cc9675b75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(status='LOAN_APPLICATIONS already exists, statement succeeded.')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate synthetic data\n",
    "def generate_data(table_name: str, num_rows: int, replace: bool = False) -> list[Row]:\n",
    "    query = f\"\"\"\n",
    "        CREATE{\" OR REPLACE\" if replace else \"\"} TABLE{\"\" if replace else \" IF NOT EXISTS\"} {table_name} AS\n",
    "        SELECT \n",
    "            ROW_NUMBER() OVER (ORDER BY RANDOM()) as application_id,\n",
    "            ROUND(NORMAL(40, 10, RANDOM())) as age,\n",
    "            ROUND(NORMAL(65000, 20000, RANDOM())) as income,\n",
    "            ROUND(NORMAL(680, 50, RANDOM())) as credit_score,\n",
    "            ROUND(NORMAL(5, 2, RANDOM())) as employment_length,\n",
    "            ROUND(NORMAL(25000, 8000, RANDOM())) as loan_amount,\n",
    "            ROUND(NORMAL(35, 10, RANDOM()), 2) as debt_to_income,\n",
    "            ROUND(NORMAL(5, 2, RANDOM())) as number_of_credit_lines,\n",
    "            GREATEST(0, ROUND(NORMAL(1, 1, RANDOM()))) as previous_defaults,\n",
    "            ARRAY_CONSTRUCT(\n",
    "                'home_improvement', 'debt_consolidation', 'business', 'education',\n",
    "                'major_purchase', 'medical', 'vehicle', 'other'\n",
    "            )[UNIFORM(1, 8, RANDOM())] as loan_purpose,\n",
    "            RANDOM() < 0.15 as is_default,\n",
    "            TIMEADD(\"MINUTE\", UNIFORM(-525600, 0, RANDOM()), CURRENT_TIMESTAMP()) as created_at\n",
    "        FROM TABLE(GENERATOR(rowcount => {num_rows}))\n",
    "        ORDER BY created_at;\n",
    "    \"\"\"\n",
    "    return session.sql(query).collect()\n",
    "\n",
    "table_name = \"loan_applications\"\n",
    "generate_data(table_name, 1e5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6783667-50b2-4f41-a72c-3969e431f858",
   "metadata": {},
   "source": [
    "### Prepare Model Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f6ba7a8b-a944-4149-a46e-48196350b70e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pickle\n",
    "from time import perf_counter\n",
    "from typing import Literal, Optional\n",
    "\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import accuracy_score, classification_report, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "\n",
    "\n",
    "from snowflake.ml.data.data_connector import DataConnector\n",
    "from snowflake.ml.registry import Registry as ModelRegistry\n",
    "from snowflake.ml.utils.connection_params import SnowflakeLoginOptions\n",
    "from snowflake.snowpark import Session\n",
    "\n",
    "\n",
    "def create_data_connector(session, table_name: str) -> DataConnector:\n",
    "    \"\"\"Load data from Snowflake table\"\"\"\n",
    "    # Example query - modify according to your schema\n",
    "    query = f\"\"\"\n",
    "    SELECT\n",
    "        age,\n",
    "        income,\n",
    "        credit_score,\n",
    "        employment_length,\n",
    "        loan_amount,\n",
    "        debt_to_income,\n",
    "        number_of_credit_lines,\n",
    "        previous_defaults,\n",
    "        loan_purpose,\n",
    "        is_default\n",
    "    FROM {table_name}\n",
    "    \"\"\"\n",
    "    sp_df = session.sql(query)\n",
    "    return DataConnector.from_dataframe(sp_df)\n",
    "\n",
    "\n",
    "def build_pipeline(**model_params) -> Pipeline:\n",
    "    \"\"\"Create pipeline with preprocessors and model\"\"\"\n",
    "    # Define column types\n",
    "    categorical_cols = [\"LOAN_PURPOSE\"]\n",
    "    numerical_cols = [\n",
    "        \"AGE\",\n",
    "        \"INCOME\",\n",
    "        \"CREDIT_SCORE\",\n",
    "        \"EMPLOYMENT_LENGTH\",\n",
    "        \"LOAN_AMOUNT\",\n",
    "        \"DEBT_TO_INCOME\",\n",
    "        \"NUMBER_OF_CREDIT_LINES\",\n",
    "        \"PREVIOUS_DEFAULTS\",\n",
    "    ]\n",
    "\n",
    "    # Numerical preprocessing pipeline\n",
    "    numeric_transformer = Pipeline(\n",
    "        steps=[\n",
    "            (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "            (\"scaler\", StandardScaler()),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Categorical preprocessing pipeline\n",
    "    categorical_transformer = Pipeline(\n",
    "        steps=[\n",
    "            (\"imputer\", SimpleImputer(strategy=\"constant\", fill_value=\"missing\")),\n",
    "            (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Combine transformers\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"num\", numeric_transformer, numerical_cols),\n",
    "            (\"cat\", categorical_transformer, categorical_cols),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Define model parameters\n",
    "    default_params = {\n",
    "        \"objective\": \"binary:logistic\",\n",
    "        \"eval_metric\": \"auc\",\n",
    "        \"max_depth\": 6,\n",
    "        \"learning_rate\": 0.1,\n",
    "        \"n_estimators\": 100,\n",
    "        \"subsample\": 0.8,\n",
    "        \"colsample_bytree\": 0.8,\n",
    "        \"random_state\": 42,\n",
    "    }\n",
    "    model = xgb.XGBClassifier(**(model_params or default_params))\n",
    "\n",
    "    return Pipeline([(\"preprocessor\", preprocessor), (\"classifier\", model)])\n",
    "\n",
    "\n",
    "def evaluate_model(model: Pipeline, X_test: pd.DataFrame, y_test: pd.DataFrame):\n",
    "    \"\"\"Evaluate model performance\"\"\"\n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    # Calculate metrics\n",
    "    metrics = {\n",
    "        \"accuracy\": accuracy_score(y_test, y_pred),\n",
    "        \"roc_auc\": roc_auc_score(y_test, y_pred_proba),\n",
    "        \"classification_report\": classification_report(y_test, y_pred),\n",
    "    }\n",
    "\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def save_to_registry(\n",
    "    session: Session,\n",
    "    model: Pipeline,\n",
    "    model_name: str,\n",
    "    metrics: dict,\n",
    "    sample_input_data: pd.DataFrame,\n",
    "):\n",
    "    \"\"\"Save model and artifacts to Snowflake Model Registry\"\"\"\n",
    "    # Initialize model registry\n",
    "    registry = ModelRegistry(session)\n",
    "\n",
    "    # Save to registry\n",
    "    registry.log_model(\n",
    "        model=model,\n",
    "        model_name=model_name,\n",
    "        metrics=metrics,\n",
    "        sample_input_data=sample_input_data[:5],\n",
    "        conda_dependencies=[\"xgboost\"],\n",
    "    )\n",
    "\n",
    "\n",
    "def train(session: Session, source_data: str, save_mode: Literal[\"local\", \"registry\"] = \"local\", output_dir: Optional[str] = None, **kwargs):\n",
    "    # Load data\n",
    "    dc = create_data_connector(session, table_name=source_data)\n",
    "    print(\"Loading data...\", end=\"\", flush=True)\n",
    "    start = perf_counter()\n",
    "    df = dc.to_pandas()\n",
    "    elapsed = perf_counter() - start\n",
    "    print(f\" done! Loaded {len(df)} rows, elapsed={elapsed:.3f}s\")\n",
    "\n",
    "    # Split data\n",
    "    X = df.drop(\"IS_DEFAULT\", axis=1)\n",
    "    y = df[\"IS_DEFAULT\"]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42\n",
    "    )\n",
    "\n",
    "    # Train model\n",
    "    model = build_pipeline()\n",
    "    print(\"Training model...\", end=\"\")\n",
    "    start = perf_counter()\n",
    "    model.fit(X_train, y_train)\n",
    "    elapsed = perf_counter() - start\n",
    "    print(f\" done! Elapsed={elapsed:.3f}s\")\n",
    "\n",
    "    # Evaluate model\n",
    "    print(\"Evaluating model...\", end=\"\")\n",
    "    start = perf_counter()\n",
    "    metrics = evaluate_model(\n",
    "        model,\n",
    "        X_test,\n",
    "        y_test,\n",
    "    )\n",
    "    elapsed = perf_counter() - start\n",
    "    print(f\" done! Elapsed={elapsed:.3f}s\")\n",
    "\n",
    "    # Print evaluation results\n",
    "    print(\"\\nModel Performance Metrics:\")\n",
    "    print(f\"Accuracy: {metrics['accuracy']:.4f}\")\n",
    "    print(f\"ROC AUC: {metrics['roc_auc']:.4f}\")\n",
    "    # Uncomment below for full classification report\n",
    "    # print(\"\\nClassification Report:\")\n",
    "    # print(metrics[\"classification_report\"])\n",
    "\n",
    "    start = perf_counter()\n",
    "    if save_mode == \"local\":\n",
    "        # Save model locally\n",
    "        print(\"Saving model to disk...\", end=\"\")\n",
    "        output_dir = output_dir or '.'\n",
    "        model_subdir = os.environ.get(\"SNOWFLAKE_SERVICE_NAME\", \"output\")\n",
    "        model_dir = os.path.join(output_dir, model_subdir) if not output_dir.endswith(model_subdir) else output_dir\n",
    "        os.makedirs(model_dir, exist_ok=True)\n",
    "        with open(os.path.join(model_dir, \"model.pkl\"), \"wb\") as f:\n",
    "            pickle.dump(model, f)\n",
    "        with open(os.path.join(model_dir, \"metrics.json\"), \"w\") as f:\n",
    "            json.dump(metrics, f, indent=2)\n",
    "    elif save_mode == \"registry\":\n",
    "        # Save model to registry\n",
    "        print(\"Logging model to Model Registry...\", end=\"\")\n",
    "        save_to_registry(\n",
    "            session,\n",
    "            model=model,\n",
    "            model_name=\"loan_default_predictor\",\n",
    "            metrics=metrics,\n",
    "            sample_input_data=X_train,\n",
    "        )\n",
    "    elapsed = perf_counter() - start\n",
    "    print(f\" done! Elapsed={elapsed:.3f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "897e6b37-0cbc-4878-8e06-a6dae60b6249",
   "metadata": {},
   "source": [
    "### Run training locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ef1c3106-7e4a-4c7c-82e1-0fd1f6de417f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DataConnector.from_dataframe() is in private preview since 1.6.0. Do not use it in production. \n",
      "DataConnector.from_sql() is in private preview since 1.7.3. Do not use it in production. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data... done! Loaded 100000 rows, elapsed=0.651s\n",
      " done! Elapsed=0.374s\n",
      "Evaluating model... done! Elapsed=0.042s\n",
      "\n",
      "Model Performance Metrics:\n",
      "Accuracy: 0.4940\n",
      "ROC AUC: 0.4957\n",
      "Saving model to disk... done! Elapsed=0.002s\n"
     ]
    }
   ],
   "source": [
    "train(session, table_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e84a9f41-1113-47f5-9041-a8cf1f904a9f",
   "metadata": {},
   "source": [
    "### Train with GPU on remote SPCS instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c2940e5a-4f85-46d9-9b49-aed6ac79fb53",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "remote() is in private preview since 1.7.4. Do not use it in production. \n"
     ]
    }
   ],
   "source": [
    "from snowflake.ml.jobs import remote\n",
    "\n",
    "@remote(compute_pool, stage_name=\"payload_stage\")\n",
    "def train_remote(source_data: str, save_mode: str = \"local\", output_dir: str = None):\n",
    "    # Retrieve session from SPCS service context\n",
    "    from snowflake.ml.utils import connection_params\n",
    "    session = Session.builder.configs(connection_params.SnowflakeLoginOptions()).create()\n",
    "\n",
    "    # Run training script\n",
    "    train(session, source_data, save_mode, output_dir)\n",
    "\n",
    "train_job = train_remote(table_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3eb1ad66-851f-4fdb-887c-4dcaaf5bf2c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ML_JOB_3D5AA40C_52DF_47AA_A996_C6D5649F1047\n",
      "PENDING\n"
     ]
    }
   ],
   "source": [
    "print(train_job.id)\n",
    "print(train_job.status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1909e552-d289-4bf4-8840-926d99295acb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MLJob.wait() is in private preview since 1.7.4. Do not use it in production. \n",
      "MLJob.show_logs() is in private preview since 1.7.4. Do not use it in production. \n",
      "MLJob.get_logs() is in private preview since 1.7.4. Do not use it in production. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "'micromamba' is running as a subprocess and can't modify the parent shell.\n",
      "Thus you must initialize your shell before using activate and deactivate.\n",
      "\n",
      "To initialize the current bash shell, run:\n",
      "    $ eval \"$(micromamba shell hook --shell bash)\"\n",
      "and then activate or deactivate with:\n",
      "    $ micromamba activate\n",
      "To automatically initialize all future (bash) shells, run:\n",
      "    $ micromamba shell init --shell bash --root-prefix=~/micromamba\n",
      "If your shell was already initialized, reinitialize your shell with:\n",
      "    $ micromamba shell reinit --shell bash\n",
      "Otherwise, this may be an issue. In the meantime you can run commands. See:\n",
      "    $ micromamba run --help\n",
      "\n",
      "Supported shells are {bash, zsh, csh, xonsh, cmd.exe, powershell, fish}.\n",
      "Creating log directories...\n",
      "+ set -e\n",
      "+ echo 'Creating log directories...'\n",
      "+ mkdir -p /var/log/managedservices/user/mlrs\n",
      "+ mkdir -p /var/log/managedservices/system/mlrs\n",
      "+ mkdir -p /var/log/managedservices/system/ray\n",
      "+ echo '*/1 * * * * root /etc/ray_copy_cron.sh'\n",
      "+ echo ''\n",
      "+ chmod 744 /etc/cron.d/ray_copy_cron\n",
      "+ service cron start\n",
      " * Starting periodic command scheduler cron\n",
      "   ...done.\n",
      "+ mkdir -p /tmp/prometheus-multi-dir\n",
      "+ '[' -n /opt/app ']'\n",
      "+ cd /opt/app\n",
      "+ export PYTHONPATH=/opt/env/site-packages/\n",
      "+ PYTHONPATH=/opt/env/site-packages/\n",
      "+ MLRS_REQUIREMENTS_FILE=requirements.txt\n",
      "+ '[' -f requirements.txt ']'\n",
      "+ MLRS_CONDA_ENV_FILE=environment.yml\n",
      "+ '[' -f environment.yml ']'\n",
      "++ df --output=size --block-size=1 /dev/shm\n",
      "++ tail -n 1\n",
      "+ shm_size=4294967296\n",
      "++ ifconfig eth0\n",
      "++ sed -En -e 's/.*inet ([0-9.]+).*/\u0001/p'\n",
      "+ eth0Ip=$'\\001'\n",
      "+ log_dir=/tmp/ray\n",
      "+ '[' -z $'\\001' ']'\n",
      "+ common_params=(\"--node-ip-address=$eth0Ip\" \"--object-manager-port=${RAY_OBJECT_MANAGER_PORT:-12011}\" \"--node-manager-port=${RAY_NODE_MANAGER_PORT:-12012}\" \"--runtime-env-agent-port=${RAY_RUNTIME_ENV_AGENT_PORT:-12013}\" \"--dashboard-agent-grpc-port=${RAY_DASHBOARD_AGENT_GRPC_PORT:-12014}\" \"--dashboard-agent-listen-port=${RAY_DASHBOARD_AGENT_LISTEN_PORT:-12015}\" \"--min-worker-port=${RAY_MIN_WORKER_PORT:-12031}\" \"--max-worker-port=${RAY_MAX_WORKER_PORT:-13000}\" \"--metrics-export-port=11502\" \"--temp-dir=$log_dir\" \"--disable-usage-stats\")\n",
      "+ head_params=(\"--head\" \"--port=${RAY_HEAD_GCS_PORT:-12001}\" \"--ray-client-server-port=${RAY_HEAD_CLIENT_SERVER_PORT:-10001}\" \"--dashboard-host=${NODE_IP_ADDRESS}\" \"--dashboard-grpc-port=${RAY_HEAD_DASHBOARD_GRPC_PORT:-12002}\" \"--dashboard-port=${DASHBOARD_PORT}\" \"--resources={\\\"node_tag:head\\\":1}\")\n",
      "+ echo Running command: python /opt/app/func.py loan_applications\n",
      "+ python /opt/app/func.py loan_applications\n",
      "Running command: python /opt/app/func.py loan_applications\n",
      "+ ray start $'--node-ip-address=\\001' --object-manager-port=12011 --node-manager-port=12012 --runtime-env-agent-port=12013 --dashboard-agent-grpc-port=12014 --dashboard-agent-listen-port=12015 --min-worker-port=12031 --max-worker-port=13000 --metrics-export-port=11502 --temp-dir=/tmp/ray --disable-usage-stats --head --port=12001 --ray-client-server-port=10001 --dashboard-host=0.0.0.0 --dashboard-grpc-port=12002 --dashboard-port=12003 '--resources={\"node_tag:head\":1}'\n",
      "+ python -m web.ml_runtime_grpc_server\n",
      "SnowflakeLoginOptions() is in private preview since 0.2.0. Do not use it in production. \n",
      "DataConnector.from_dataframe() is in private preview since 1.6.0. Do not use it in production. \n",
      "[2025-01-13 23:25:19,059 E 44 44] gcs_rpc_client.h:179: Failed to connect to GCS at address \u0001:12001 within 5 seconds.\n",
      "2025-01-13 23:25:19,317\tINFO worker.py:1777 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "2025-01-13 23:25:21,147\tINFO streaming_executor.py:108 -- Starting execution of Dataset. Full logs are in /tmp/ray/session_2025-01-13_23-25-18_052965_46/logs/ray-data\n",
      "2025-01-13 23:25:21,147\tINFO streaming_executor.py:109 -- Execution plan of Dataset: InputDataBuffer[Input] -> TaskPoolMapOperator[ReadResultSetDataSource]\n",
      "Running 0: 0.00 row [00:00, ? row/s]\n",
      "Running. Resources: 3/3 CPU, 0/0 GPU, 768.0MB/1.9GB object_store_memory (pending: 0 CPU, 0 GPU): : 0.00 row [00:01, ? row/s]\n",
      "- ReadResultSetDataSource: 3 active, 197 queued ðŸš§, [cpu: 3.0, objects: 768.0MB]: : 0.00 row [00:01, ? row/s]\u001b[A\n",
      "                                                                                                                            \n",
      "\u001b[A\u001b[36m(ReadResultSetDataSource pid=236)\u001b[0m SnowflakeLoginOptions() is in private preview since 0.2.0. Do not use it in production. \n",
      "Running. Resources: 3/3 CPU, 0/0 GPU, 768.0MB/1.9GB object_store_memory (pending: 0 CPU, 0 GPU): : 0.00 row [00:01, ? row/s]\n",
      "Running. Resources: 2/3 CPU, 0/0 GPU, 256.0MB/1.9GB object_store_memory (pending: 0 CPU, 0 GPU): : 0.00 row [00:02, ? row/s]\n",
      "- ReadResultSetDataSource: 3 active, 196 queued ðŸš§, [cpu: 3.0, objects: 384.0MB]: : 0.00 row [00:02, ? row/s]\u001b[A\n",
      "- ReadResultSetDataSource: 3 active, 196 queued ðŸš§, [cpu: 3.0, objects: 384.0MB]: : 0.00 row [00:02, ? row/s]\u001b[A\n",
      "                                                                                                                            \n",
      "âœ”ï¸  Dataset execution finished in 2.16 seconds: : 0.00 row [00:02, ? row/s]                                  \n",
      "\n",
      "- ReadResultSetDataSource: 3 active, 196 queued ðŸš§, [cpu: 3.0, objects: 384.0MB]: : 0.00 row [00:02, ? row/s]\u001b[A\n",
      "- ReadResultSetDataSource: 3 active, 196 queued ðŸš§, [cpu: 3.0, objects: 384.0MB]: : 0.00 row [00:02, ? row/s]\u001b[A\n",
      "- ReadResultSetDataSource: 3 active, 196 queued ðŸš§, [cpu: 3.0, objects: 384.0MB]: : 0.00 row [00:02, ? row/s]\n",
      "2025-01-13 23:25:23,316\tINFO streaming_executor.py:108 -- Starting execution of Dataset. Full logs are in /tmp/ray/session_2025-01-13_23-25-18_052965_46/logs/ray-data\n",
      "2025-01-13 23:25:23,316\tINFO streaming_executor.py:109 -- Execution plan of Dataset: InputDataBuffer[Input] -> TaskPoolMapOperator[ReadResultSetDataSource]\n",
      "Running 0: 0.00 row [00:00, ? row/s]\n",
      "Running. Resources: 1/3 CPU, 0/0 GPU, 269.3KB/1.9GB object_store_memory (pending: 0 CPU, 0 GPU):   7%|â–‹         | 98.7k/1.32M [00:01<00:13, 93.1k row/s]\n",
      "- ReadResultSetDataSource: 3 active, 180 queued ðŸš§, [cpu: 3.0, objects: 808.0KB]: : 0.00 row [00:01, ? row/s]\u001b[A\n",
      "- ReadResultSetDataSource: 3 active, 180 queued ðŸš§, [cpu: 3.0, objects: 808.0KB]:   0%|          | 0.00/1.04M [00:01<?, ? row/s]\u001b[A\n",
      "- ReadResultSetDataSource: 3 active, 180 queued ðŸš§, [cpu: 3.0, objects: 808.0KB]:  10%|â–‰         | 98.7k/1.04M [00:01<00:09, 95.6k row/s]\u001b[A\n",
      "                                                                                                                                                        \n",
      "âœ”ï¸  Dataset execution finished in 1.80 seconds: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100k/100k [00:01<00:00, 55.4k row/s]                                    \n",
      "\n",
      "- ReadResultSetDataSource: 3 active, 180 queued ðŸš§, [cpu: 3.0, objects: 808.0KB]:  10%|â–‰         | 98.7k/1.04M [00:01<00:09, 95.6k row/s]\u001b[A\n",
      "- ReadResultSetDataSource: 0 active, 0 queued, [cpu: 0.0, objects: 0.0B]:  10%|â–‰         | 98.7k/1.04M [00:01<00:09, 95.6k row/s]        \u001b[A\n",
      "- ReadResultSetDataSource: 0 active, 0 queued, [cpu: 0.0, objects: 0.0B]:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 98.7k/100k [00:01<00:00, 95.6k row/s] \u001b[A\n",
      "- ReadResultSetDataSource: 0 active, 0 queued, [cpu: 0.0, objects: 0.0B]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100k/100k [00:01<00:00, 95.6k row/s] \u001b[A\n",
      "- ReadResultSetDataSource: 0 active, 0 queued, [cpu: 0.0, objects: 0.0B]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100k/100k [00:01<00:00, 59.7k row/s]\n",
      "Loading data... done! Loaded 100000 rows, elapsed=7.322s\n",
      "Training model... done! Elapsed=3.136s\n",
      "Evaluating model... done! Elapsed=0.127s\n",
      "\n",
      "Model Performance Metrics:\n",
      "Accuracy: 0.5017\n",
      "ROC AUC: 0.5017\n",
      "Saving model to disk... done! Elapsed=1.272s\n",
      "\u001b[36m(ReadResultSetDataSource pid=235)\u001b[0m SnowflakeLoginOptions() is in private preview since 0.2.0. Do not use it in production. \u001b[32m [repeated 2x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_job.wait()\n",
    "train_job.show_logs()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe67e0b-e73f-4c90-8a22-f7b7fce0bfe2",
   "metadata": {},
   "source": [
    "### Run concurrent training jobs on SPCS\n",
    "\n",
    "Suppose we want to train multiple models on different datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5c1be7c9-1512-48f5-adc0-630102e4e41e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating datasets\n",
      "Generated datasets: ['loan_applications_0', 'loan_applications_1', 'loan_applications_2', 'loan_applications_3', 'loan_applications_4', 'loan_applications_5', 'loan_applications_6', 'loan_applications_7', 'loan_applications_8', 'loan_applications_9']\n",
      "Starting training jobs\n",
      "Started 10 training jobs\n"
     ]
    }
   ],
   "source": [
    "datasets = []\n",
    "print(\"Generating datasets\")\n",
    "for i in range(10):\n",
    "    dataset = f\"loan_applications_{i}\"\n",
    "    generate_data(dataset, 1e6)\n",
    "    datasets.append(dataset)\n",
    "print(f\"Generated datasets: {datasets}\")\n",
    "    \n",
    "print(\"Starting training jobs\")\n",
    "train_jobs = []\n",
    "for ds in datasets:\n",
    "    train_jobs.append(train_remote(ds))\n",
    "print(f\"Started {len(train_jobs)} training jobs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "75619ff2-70fb-4237-b183-25d6627b902a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "list_jobs() is in private preview since 1.7.4. Do not use it in production. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------------------------------------------------\n",
      "|\"id\"                                         |\"owner\"   |\"status\"  |\"created_on\"                      |\"compute_pool\"  |\n",
      "-------------------------------------------------------------------------------------------------------------------------\n",
      "|ML_JOB_23596270_9B94_4DEC_B28B_AB494C3B6CFD  |ENGINEER  |DONE      |2025-01-13 15:25:38.436000-08:00  |DEMO_POOL_CPU   |\n",
      "|ML_JOB_3D5AA40C_52DF_47AA_A996_C6D5649F1047  |ENGINEER  |DONE      |2025-01-13 15:25:06.870000-08:00  |DEMO_POOL_CPU   |\n",
      "|ML_JOB_2065D772_04F5_43FA_90DA_7E48ACCF2235  |ENGINEER  |DONE      |2025-01-13 15:03:39.172000-08:00  |DEMO_POOL_CPU   |\n",
      "|ML_JOB_33AD7502_F646_4B13_B1D1_10D252E4E5FA  |ENGINEER  |DONE      |2025-01-13 15:03:25.193000-08:00  |DEMO_POOL_CPU   |\n",
      "|ML_JOB_288CF902_3174_428B_9838_382EA62E9996  |ENGINEER  |DONE      |2025-01-13 15:03:19.679000-08:00  |DEMO_POOL_CPU   |\n",
      "|ML_JOB_3AA62A51_9F74_4788_B18B_9F739C7C5E54  |ENGINEER  |DONE      |2025-01-13 15:03:10.811000-08:00  |DEMO_POOL_CPU   |\n",
      "|ML_JOB_306F69E1_B12C_43A5_B375_A7C812AE8B7E  |ENGINEER  |DONE      |2025-01-06 10:10:19.958000-08:00  |DEMO_POOL_CPU   |\n",
      "|ML_JOB_11A6B15A_7616_4684_8F20_A134A2F4EF4E  |ENGINEER  |DONE      |2025-01-06 10:10:04.404000-08:00  |DEMO_POOL_CPU   |\n",
      "|ML_JOB_2CD110A7_4759_4B15_BAEB_A07CB4999213  |ENGINEER  |DONE      |2025-01-06 10:07:54.345000-08:00  |DEMO_POOL_CPU   |\n",
      "|ML_JOB_34052E7E_2049_4ACA_8810_F485611BB83E  |ENGINEER  |DONE      |2025-01-06 09:57:23.184000-08:00  |DEMO_POOL_CPU   |\n",
      "-------------------------------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from snowflake.ml.jobs import list_jobs\n",
    "\n",
    "list_jobs().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e6f178-e80d-4101-82f1-a4287aac4021",
   "metadata": {},
   "outputs": [],
   "source": [
    "session.sql(f\"DROP SCHEMA {schema_name}\").collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
