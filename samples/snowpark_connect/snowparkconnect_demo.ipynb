{
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  },
  "lastEditStatus": {
   "notebookId": "cizhwcq3q6vyjca7iwml",
   "authorId": "197899322529",
   "authorName": "VINOD",
   "authorEmail": "vino.duraisamy@snowflake.com",
   "sessionId": "ba770c0a-bc0e-4f85-a472-87a4b36ae6e5",
   "lastEditTime": 1756402705775
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa0bb148-3018-4471-870c-73e6033f91ec",
   "metadata": {
    "name": "introduction",
    "collapsed": false
   },
   "source": "# SNOWPARK CONNECT FOR APACHE SPARK *TM*\n\nIn this demo we will see how you can : \n- Connect to the Snowpark Connect server:\n- Execute simple PySpark code\n- Create nested table structures and write to Snowflake\n- Generate synthetic data for support cases and write to Snowfalke\n- Create a spark UDF, register it and invoke it directly\n- Create a Snowflake Python function and invoke it with SQL Passthrough from Spark\n \n"
  },
  {
   "cell_type": "markdown",
   "id": "953912a5-e524-4a9d-b81b-acc64cd8aa3b",
   "metadata": {
    "name": "connectionmd",
    "collapsed": false
   },
   "source": "Establish spark connection and start the snowpark connect server"
  },
  {
   "cell_type": "code",
   "id": "092e2bd8-0891-403c-8d12-189b986071c9",
   "metadata": {
    "language": "python",
    "name": "establish_connection"
   },
   "outputs": [],
   "source": "from snowflake import snowpark_connect\nfrom snowflake.snowpark.context import get_active_session\n\nsession = get_active_session()\nprint(session)\n\nspark = snowpark_connect.server.init_spark_session()",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "50bc29da-af06-4528-8efc-a5a33bfd84f0",
   "metadata": {
    "name": "querydata",
    "collapsed": false
   },
   "source": "Query structured data from Snowflake table inside Spark"
  },
  {
   "cell_type": "code",
   "id": "8d50cbf4-0c8d-4950-86cb-114990437ac9",
   "metadata": {
    "language": "python",
    "name": "connection_test"
   },
   "source": "df = spark.sql(\"show tables\").limit(10)\ndf.show()",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "f7efec50-96c1-4d5a-8c97-4aa8fdcfd47c",
   "metadata": {
    "name": "simple_examplemd",
    "collapsed": false
   },
   "source": "We will show an extremely simple one as below. If you are interested to use [Pyspark examples](https://docs.google.com/document/d/1F9mmoSP4DuObNREvbQ5lClrExrBThbB_Ww6_7NsI0es/edit?tab=t.ltuj1iuzoic1#heading=h.wl5zn2ai10cn) or start exploring various guides from sites such as [Kaggle](https://www.kaggle.com/code/kkhandekar/apache-spark-beginner-tutorial)"
  },
  {
   "cell_type": "code",
   "id": "da394518-2335-4753-ab0d-1ab2277822a8",
   "metadata": {
    "language": "python",
    "name": "simpleexample"
   },
   "outputs": [],
   "source": "from pyspark.sql import Row\n\ndf = spark.createDataFrame([\n                Row(a=1, b=2.),\n                Row(a=2, b=3.),\n                Row(a=4, b=5.),])\ndf.show()\n",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "30c0f6f7-d767-4232-90a0-4b1c94c5eeb9",
   "metadata": {
    "name": "PySpark_syntheticdata",
    "collapsed": false
   },
   "source": "#### Generate synthetic support case data using PySpark\nThe below code defines a schema and then populates it with various data types, including random strings, dates, and boolean values."
  },
  {
   "cell_type": "code",
   "id": "97c6174b-8204-48b1-82b3-e79b4fc33c5d",
   "metadata": {
    "language": "python",
    "name": "generate_synthetic_supportcases"
   },
   "outputs": [],
   "source": "from pyspark.sql import SparkSession\nfrom pyspark.sql.types import StringType, TimestampType, BooleanType\nfrom pyspark.sql.functions import lit, rand, expr, date_add, to_timestamp\nimport random\nfrom datetime import datetime, timedelta\n\n\n\nschema = \"\"\"\n    CASE_ID STRING,\n    CATEGORY STRING,\n    CASE_TITLE STRING,\n    CASE_DESCRIPTION STRING,\n    LAST_UPDATE TIMESTAMP,\n    STATUS STRING,\n    DATE_CREATED TIMESTAMP,\n    DATE_CLOSED TIMESTAMP,\n    REQUIRED_ESCALATION BOOLEAN\n\"\"\"\n\n# --- Data Generation Parameters ---\nnum_records = 1000 \n\ncategories = [\"Technical Issue\", \"Billing Inquiry\", \"Feature Request\", \"Account Management\", \"Bug Report\", \"General Question\"]\nstatuses = [\"Open\", \"In Progress\", \"Resolved\", \"Closed\", \"Pending Customer\"]\ncase_titles_templates = [\n    \"Problem with {}\",\n    \"Issue accessing {}\",\n    \"Request for new feature: {}\",\n    \"Account update required for {}\",\n    \"Bug in {} module\",\n    \"Question about {} functionality\"\n]\ndescription_templates = [\n    \"User reported that {} is not working as expected. Needs investigation.\",\n    \"Customer is unable to {} after recent update.\",\n    \"Details: User needs assistance with {}. Please provide guidance.\",\n    \"Troubleshooting steps taken: {}. Still experiencing the issue.\",\n    \"New feature request: {}. Describe desired functionality and benefits.\",\n    \"Error observed: {}. Stack trace attached if available.\"\n]\n\n# Function to generate a random datetime within a range\ndef random_date(start_date, end_date):\n    # Ensure start_date is not after end_date. If they are the same, return start_date.\n    if start_date >= end_date:\n        return start_date # Or handle as an error, but returning start_date is safer for small ranges\n\n    time_between_dates = end_date - start_date\n    days_between_dates = time_between_dates.days\n\n    # Ensure days_between_dates is at least 0 before calling randrange\n    # randrange(0) is valid and returns 0, so if days_between_dates is 0, this works.\n    if days_between_dates < 0: # This should ideally not happen with the check above, but as a safeguard\n        days_between_dates = 0\n\n    random_number_of_days = random.randrange(days_between_dates + 1) # +1 to include the end_date day\n    \n    random_date_offset = start_date + timedelta(days=random_number_of_days)\n    \n    \n    if days_between_dates == 0:\n        # If dates are the same, pick a time between start_date.time() and end_date.time()\n        total_seconds_in_range = int((end_date - start_date).total_seconds())\n        if total_seconds_in_range <= 0:\n            return start_date # Should already be handled by the start_date >= end_date check\n        random_seconds_offset = random.randrange(total_seconds_in_range + 1)\n        return start_date + timedelta(seconds=random_seconds_offset)\n    else:\n        # If dates span multiple days, pick a full day offset and then a random time within that day\n        random_seconds_offset = random.randrange(86400) # seconds in a full day\n        return random_date_offset + timedelta(seconds=random_seconds_offset)\n\n\n# Generate data row by row\ndata = []\ncurrent_time = datetime.now()\nstart_creation_date = current_time - timedelta(days=365) # Cases created within the last year\n\nfor i in range(num_records):\n    case_id = f\"CASE-{100000 + i}\"\n    category = random.choice(categories)\n    \n    placeholder = f\"Module {random.randint(1, 10)}\" if \"Module\" in str(random.choice(case_titles_templates)).format('{}') else \\\n                  f\"Service {chr(65 + random.randint(0, 5))}\" if \"Service\" in str(random.choice(case_titles_templates)).format('{}') else \\\n                  f\"Feature {random.randint(1, 20)}\" if \"Feature\" in str(random.choice(case_titles_templates)).format('{}') else \\\n                  f\"User Account {random.randint(100, 999)}\"\n                  \n    case_title = random.choice(case_titles_templates).format(placeholder)\n    case_description = random.choice(description_templates).format(placeholder)\n\n    # Date Created: Ensure it's not in the future\n    date_created = random_date(start_creation_date, current_time)\n    \n    date_closed = None\n    status = random.choice(statuses)\n    \n    if status in [\"Resolved\", \"Closed\"]:\n        \n        date_closed_earliest = date_created + timedelta(minutes=1)\n        date_closed_latest = date_created + timedelta(days=30)\n\n    \n        effective_date_closed_end = min(date_closed_latest, current_time)\n\n        if date_closed_earliest < effective_date_closed_end:\n            date_closed = random_date(date_closed_earliest, effective_date_closed_end)\n        else:\n            \n            date_closed = effective_date_closed_end \n\n    if date_closed:\n        last_update = random_date(date_created, date_closed)\n    else:\n        last_update = random_date(date_created, current_time)\n    \n    required_escalation = random.choice([True, False, False, False]) \n\n    data.append((case_id, category, case_title, case_description, last_update, status, date_created, date_closed, required_escalation))\n\n# Create DataFrame\ndf = spark.createDataFrame(data).toDF(*[field.strip().split()[0] for field in schema.strip().split(\",\")])\n\n# Show a sample of the generated data\nprint(f\"\\nGenerated {num_records} records.\")\ndf.show(5, truncate=False)\n\n\n\ndf.write.mode(\"overwrite\").saveAsTable(\"Support_Cases\")",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "daf4a713-a37f-4c00-9c6c-3fda4c68cb6b",
   "metadata": {
    "name": "spark_udf",
    "collapsed": false
   },
   "source": "### Spark UDF\nThe below code snippet reads data from a Snowflake table, applies a custom spark User-Defined Function (UDF) to categorize text, and then writes the transformed data back to a new Snowflake table."
  },
  {
   "cell_type": "code",
   "id": "928397ca-fed9-42b5-97dd-1e7d6896a930",
   "metadata": {
    "language": "python",
    "name": "sparkudf"
   },
   "outputs": [],
   "source": "import os\n\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql import Row\n\nfrom pyspark.sql.functions import col, unix_timestamp, when, regexp_replace,lower\nfrom pyspark.sql.types import StringType\n\nfrom pyspark.sql.functions import udf\n\ndef udf_parse_case_description(input_desc):\n    try:\n        if input_desc is None: # Handle None inputs gracefully\n            return \"unknown\"\n            \n        desc = input_desc.lower()\n\n        # Billing/Payment\n        if any(keyword in desc for keyword in [\"refund\", \"charged twice\", \"duplicate charges\", \"invoice\", \"payment\", \"billing error\", \"subscription\", \"credit card\", \"transaction\"]):\n            return \"billing_payment\"\n        \n        # Shipping/Delivery\n        elif any(keyword in desc for keyword in [\"not delivered\", \"missing package\", \"stuck in transit\", \"tracking\", \"shipment\", \"delivery\", \"late\", \"lost\"]):\n            return \"shipping_delivery\"\n            \n        # Technical Issue\n        elif any(keyword in desc for keyword in [\"not working\", \"error\", \"bug\", \"malfunction\", \"crashed\", \"login\", \"password reset\", \"connectivity\", \"performance\", \"glitch\", \"broken\"]):\n            return \"technical_issue\"\n\n        # Account Management\n        elif any(keyword in desc for keyword in [\"account access\", \"update profile\", \"change email\", \"close account\", \"password\", \"username\", \"profile update\"]):\n            return \"account_management\"\n\n        # Product Inquiry/Feature Request\n        elif any(keyword in desc for keyword in [\"availability\", \"warranty\", \"discount\", \"feature request\", \"compatibility\", \"specs\", \"how to\", \"information about\"]):\n            return \"product_inquiry_feature_request\"\n            \n        # High Priority/Escalation\n        elif any(keyword in desc for keyword in [\"urgent\", \"critical\", \"escalate\", \"immediate attention\", \"severe\", \"blocking\"]):\n            return \"high_priority_escalation\"\n        \n        # General Inquiry - Fallback\n        else:\n            return \"general_inquiry\"\n\n    except Exception as e:\n        \n        return \"error\"\n\n# Register Spark UDF\nparse_case_udf = udf(udf_parse_case_description, StringType())\n#spark.udf.register(\"udf_parse_case_description\", udf_parse_case_description, StringType())\n\ntable_name = \"SUPPORT_CASES\" \ndf = spark.read.table(\"SUPPORT_CASES\")\ndf1 = df.withColumn(\"INTENT\", parse_case_udf(col(\"CASE_DESCRIPTION\")))\n\n#df1 = spark.sql(f\"SELECT *, udf_parse_case_description(CASE_DESCRIPTION) AS INTENT FROM {table_name}\")\n\ndf1.write.mode(\"overwrite\").saveAsTable(\"Transformed_Cases\")\n\n",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "74d08087-292d-4591-951b-35f3b6aa842c",
   "metadata": {
    "name": "query_Transformedcases",
    "collapsed": false
   },
   "source": "We can query the Transformed_Cases table with familiar Spark SQL here "
  },
  {
   "cell_type": "code",
   "id": "fc2d52b8-35d3-4ddd-8fc5-d4e7ab185207",
   "metadata": {
    "language": "python",
    "name": "transformed_Cases"
   },
   "outputs": [],
   "source": "df= spark.sql(\"select * from TRANSFORMED_CASES\");\ndf.show()",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "a7271d22-eda8-43bf-9fb7-10d2f1ccf770",
   "metadata": {
    "name": "snowpark_udf_parse_review",
    "collapsed": false
   },
   "source": "Create a simple Python UDF in Snowflake to categorize text data stored in tables. "
  },
  {
   "cell_type": "code",
   "id": "01477123-f2e7-4972-9914-ac6b47737d59",
   "metadata": {
    "language": "python",
    "name": "sp_udf_parse_review"
   },
   "outputs": [],
   "source": "session.sql(\"\"\"CREATE OR REPLACE FUNCTION udf_parse_review(input STRING)\nRETURNS STRING\nLANGUAGE PYTHON\nRUNTIME_VERSION = '3.9'\nHANDLER = 'parse'\nAS\n$$\ndef parse(input_desc):\n    try:\n        if input_desc is None: # Handle None inputs gracefully\n            return \"unknown\"\n            \n        desc = input_desc.lower()\n\n        # Billing/Payment\n        if any(keyword in desc for keyword in [\"refund\", \"charged twice\", \"duplicate charges\", \"invoice\", \"payment\", \"billing error\", \"subscription\", \"credit card\", \"transaction\"]):\n            return \"billing_payment\"\n        \n        # Shipping/Delivery\n        elif any(keyword in desc for keyword in [\"not delivered\", \"missing package\", \"stuck in transit\", \"tracking\", \"shipment\", \"delivery\", \"late\", \"lost\"]):\n            return \"shipping_delivery\"\n            \n        # Technical Issue\n        elif any(keyword in desc for keyword in [\"not working\", \"error\", \"bug\", \"malfunction\", \"crashed\", \"login\", \"password reset\", \"connectivity\", \"performance\", \"glitch\", \"broken\"]):\n            return \"technical_issue\"\n\n        # Account Management\n        elif any(keyword in desc for keyword in [\"account access\", \"update profile\", \"change email\", \"close account\", \"password\", \"username\", \"profile update\"]):\n            return \"account_management\"\n\n        # Product Inquiry/Feature Request\n        elif any(keyword in desc for keyword in [\"availability\", \"warranty\", \"discount\", \"feature request\", \"compatibility\", \"specs\", \"how to\", \"information about\"]):\n            return \"product_inquiry_feature_request\"\n            \n        # High Priority/Escalation\n        elif any(keyword in desc for keyword in [\"urgent\", \"critical\", \"escalate\", \"immediate attention\", \"severe\", \"blocking\"]):\n            return \"high_priority_escalation\"\n        \n        # General Inquiry - Fallback\n        else:\n            return \"general_inquiry\"\n\n    except Exception as e:\n        \n        return \"error\"\n$$;\n\"\"\").collect()",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "d676f471-5efd-41e8-b4a7-7d0de377d2b9",
   "metadata": {
    "name": "passthrough",
    "collapsed": false
   },
   "source": "### Enable SQL Passthrough for Snowpark Connect \nConfigure to allow direct passthrough of SQL queries to Snowflake. When snowpark.connect.sql.passthrough is set to True, spark.sql() calls will send the SQL statement directly to the Snowflake backend for execution without PySpark's local processing. This is particularly useful for leveraging Snowflake's native capabilities in spark code. \n"
  },
  {
   "cell_type": "code",
   "id": "f6e0f686-4251-494f-baa9-95c4f1277d94",
   "metadata": {
    "language": "python",
    "name": "sql_passthrough"
   },
   "outputs": [],
   "source": "spark.conf.set(\"snowpark.connect.sql.passthrough\", True)\n\nspark.sql(\"\"\"\n    CREATE OR REPLACE VIEW REVIEW_INTENT_VIEW AS\n    SELECT \n        CASE_DESCRIPTION,\n        udf_parse_review(CASE_DESCRIPTION) AS INTENT\n    FROM SUPPORT_CASES\n\"\"\")",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "1d10131f-7274-4e98-a4d1-160237e76e8e",
   "metadata": {
    "name": "read_data_from_view",
    "collapsed": false
   },
   "source": "Query the newly created view"
  },
  {
   "cell_type": "code",
   "id": "04c76095-6fe3-4bb7-9713-17b0b9087270",
   "metadata": {
    "language": "python",
    "name": "query_view"
   },
   "outputs": [],
   "source": "spark.read.table('REVIEW_INTENT_VIEW').show(100)",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "bcc999cb-2ef5-4af1-a7c1-a27aa856084c",
   "metadata": {
    "name": "end",
    "collapsed": false
   },
   "source": "## End of Notebook\n"
  }
 ]
}
