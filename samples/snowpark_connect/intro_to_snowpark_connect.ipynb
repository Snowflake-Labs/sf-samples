{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b412c899-c4ac-4c23-85e2-c8870f23e86e",
   "metadata": {
    "name": "cell1"
   },
   "source": [
    "# Getting Started with Snowpark Connect for Apache Spark\n",
    "\n",
    "Snowpark Connect allows you to run the **PySpark DataFrame API** on **Snowflake infrastructure**. \n",
    "\n",
    "**In this guide, you will learn:**\n",
    "- How Snowpark Connect executes PySpark on Snowflake infrastructure\n",
    "- How to build production pipelines with Snowpark Connect\n",
    "    - Data ingestion patterns (tables, stages, cloud storage)\n",
    "    - Transformations, joins, and aggregations\n",
    "    - Writing data with partitioning and compression\n",
    "    - Integrating with telemetry and best practices\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "962083b7-d5ad-4b9d-8ebb-303be234fc12",
   "metadata": {
    "name": "cell2"
   },
   "source": [
    "## Snowpark Connect - Key Concepts\n",
    "\n",
    "**Execution Model:**\n",
    "- Your DataFrame operations are translated to Snowflake SQL\n",
    "- Computation happens in Snowflake warehouses\n",
    "- Results stream back via Apache Arrow format\n",
    "- No Spark cluster, driver, or executors\n",
    "\n",
    "**Query Pushdown:**\n",
    "- ‚úÖ **Fully Optimized:** DataFrame operations, SQL functions, aggregations push down to Snowflake\n",
    "- ‚ö†Ô∏è **Performance Impact:** Python UDFs run client-side (fetch data ‚Üí process ‚Üí send back)\n",
    "- üí° **Better Alternative:** Use built-in SQL functions instead of UDFs\n",
    "\n",
    "**CLI: Snowpark Submit**\n",
    "- Run Spark workloads as batch jobs using Snowpark Submit CLI\n",
    "- To install: ```pip install snowpark-submit```\n",
    "- To submit a job, run: \n",
    "\n",
    "``` \n",
    "snowpark-submit \\\n",
    "  --snowflake-workload-name MY_JOB \\\n",
    "  --snowflake-connection-name MY_CONNECTION \\\n",
    "  --compute-pool MY_COMPUTE_POOL \\\n",
    "  app.py\n",
    "```\n",
    "[CLI examples](https://docs.snowflake.com/en/developer-guide/snowpark-connect/snowpark-submit-examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b4a6a6-c040-48ad-8003-542017678ff0",
   "metadata": {
    "collapsed": false,
    "name": "cell21"
   },
   "source": [
    "## Demo Pipeline: Tasty Bytes Menu Analytics\n",
    "\n",
    "In this section, we will cover key aspects of using Snowpark Connect through a production-ready pipeline that analyzes menu profitability for Tasty Bytes, a fictitious global food truck network. \n",
    "\n",
    "### Pipeline structure\n",
    " 1. Setup & Configuration\n",
    " 2. Telemetry Initialization\n",
    " 3. Data Ingestion\n",
    " 4. Data Validation\n",
    " 5. Transformations (profit analysis)\n",
    " 6. Data Quality Checks\n",
    " 7. Write Output\n",
    " 8. Cleanup & Summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d7a338-1754-47b1-9ff1-68f89dce022e",
   "metadata": {
    "language": "python",
    "name": "cell3"
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# STEP 0: INITIALIZE SESSION\n",
    "# =============================================================================\n",
    "\n",
    "# import packages - select \"snowpark_connect\" from the packages dropdown\n",
    "from snowflake import snowpark_connect\n",
    "from snowflake.snowpark.context import get_active_session\n",
    "\n",
    "# initialize session\n",
    "session = get_active_session()\n",
    "spark = snowpark_connect.server.init_spark_session()\n",
    "\n",
    "# print session info\n",
    "print(session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "772662cb-f0da-43ae-b6dc-be8fea7cc014",
   "metadata": {
    "language": "python",
    "name": "cell12"
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# STEP 1: SETUP & CONFIGURATION\n",
    "# =============================================================================\n",
    "\n",
    "import os\n",
    "import uuid\n",
    "import logging\n",
    "from datetime import datetime\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, List\n",
    "\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import (\n",
    "    col, lit, when, coalesce, trim, upper,\n",
    "    sum, avg, count, min, max, countDistinct,\n",
    "    current_timestamp, round\n",
    ")\n",
    "\n",
    "# --- Configuration for Tasty Bytes Menu Pipeline ---\n",
    "@dataclass\n",
    "class PipelineConfig:\n",
    "    \"\"\"Centralized configuration for the menu analytics pipeline.\"\"\"\n",
    "    \n",
    "    # Pipeline metadata\n",
    "    pipeline_name: str = \"tastybytes_menu_analytics\"\n",
    "    pipeline_version: str = \"1.0.0\"\n",
    "    environment: str = os.getenv(\"ENVIRONMENT\", \"dev\")\n",
    "    \n",
    "    # Snowflake context\n",
    "    role: str = \"SNOWFLAKE_LEARNING_ROLE\"\n",
    "    warehouse: str = \"SNOWFLAKE_LEARNING_WH\"\n",
    "    database: str = \"SNOWFLAKE_LEARNING_DB\"\n",
    "    \n",
    "    # Source configuration\n",
    "    stage_url: str = \"s3://sfquickstarts/tastybytes/\"\n",
    "    stage_path: str = \"@blob_stage/raw_pos/menu/\"\n",
    "    source_table: str = \"MENU_RAW\"\n",
    "    \n",
    "    # Output configuration\n",
    "    output_table: str = \"MENU_BRAND_SUMMARY\"\n",
    "    output_mode: str = \"overwrite\"\n",
    "    \n",
    "    # Quality thresholds\n",
    "    min_expected_rows: int = 50\n",
    "    max_expected_rows: int = 10_000\n",
    "    max_null_percentage: float = 0.05\n",
    "    min_profit_margin: float = 0.0  # No negative margins allowed\n",
    "\n",
    "config = PipelineConfig()\n",
    "\n",
    "# --- Initialize sessions ---\n",
    "# Note: session and spark should already be initialized\n",
    "print(f\"üöÄ Pipeline: {config.pipeline_name} v{config.pipeline_version}\")\n",
    "print(f\"üìç Environment: {config.environment}\")\n",
    "\n",
    "# ============================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b76689f-67d6-4271-b476-481b8513e74b",
   "metadata": {
    "language": "python",
    "name": "cell13"
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# STEP 2: TELEMETRY INITIALIZATION\n",
    "# =============================================================================\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s | %(levelname)s | %(message)s\",\n",
    "    datefmt=\"%Y-%m-%d %H:%M:%S\"\n",
    ")\n",
    "logger = logging.getLogger(config.pipeline_name)\n",
    "\n",
    "@dataclass\n",
    "class PipelineMetrics:\n",
    "    \"\"\"Track metrics throughout pipeline execution.\"\"\"\n",
    "    run_id: str\n",
    "    start_time: datetime\n",
    "    end_time: Optional[datetime] = None\n",
    "    status: str = \"running\"\n",
    "    \n",
    "    # Stage metrics\n",
    "    rows_ingested: int = 0\n",
    "    rows_after_validation: int = 0\n",
    "    rows_after_transform: int = 0\n",
    "    rows_written: int = 0\n",
    "    \n",
    "    # Business metrics\n",
    "    total_menu_items: int = 0\n",
    "    total_brands: int = 0\n",
    "    avg_profit_margin: float = 0.0\n",
    "    \n",
    "    # Quality metrics\n",
    "    validation_passed: bool = False\n",
    "    quality_score: float = 0.0\n",
    "    warnings: List[str] = None\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        self.warnings = self.warnings or []\n",
    "    \n",
    "    def add_warning(self, message: str):\n",
    "        self.warnings.append(message)\n",
    "        logger.warning(f\"‚ö†Ô∏è  {message}\")\n",
    "    \n",
    "    def duration_seconds(self) -> float:\n",
    "        end = self.end_time or datetime.now()\n",
    "        return (end - self.start_time).total_seconds()\n",
    "\n",
    "metrics = PipelineMetrics(\n",
    "    run_id=str(uuid.uuid4())[:8],\n",
    "    start_time=datetime.now()\n",
    ")\n",
    "\n",
    "logger.info(f\"{'='*60}\")\n",
    "logger.info(f\"üöÄ PIPELINE START | Run ID: {metrics.run_id}\")\n",
    "logger.info(f\"{'='*60}\")\n",
    "\n",
    "# ============================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "806fc92d-4be6-4c81-b70e-c17bf2979829",
   "metadata": {
    "language": "python",
    "name": "cell14"
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# STEP 3: DATA INGESTION\n",
    "# =============================================================================\n",
    "\n",
    "def setup_snowflake_context(session, config: PipelineConfig) -> str:\n",
    "    \"\"\"Set up Snowflake role, warehouse, database, and schema.\"\"\"\n",
    "    logger.info(\"Setting up Snowflake context...\")\n",
    "    \n",
    "    session.sql(f\"USE ROLE {config.role}\").collect()\n",
    "    session.sql(f\"USE WAREHOUSE {config.warehouse}\").collect()\n",
    "    session.sql(f\"USE DATABASE {config.database}\").collect()\n",
    "    \n",
    "    # Create user-specific schema\n",
    "    current_user = session.sql(\"SELECT current_user()\").collect()[0][0]\n",
    "    schema_name = f\"{current_user}_MENU_ANALYTICS\"\n",
    "    session.sql(f\"CREATE SCHEMA IF NOT EXISTS {schema_name}\").collect()\n",
    "    session.sql(f\"USE SCHEMA {schema_name}\").collect()\n",
    "    \n",
    "    logger.info(f\"  Context: {config.database}.{schema_name}\")\n",
    "    return schema_name\n",
    "\n",
    "def create_stage_and_table(session, config: PipelineConfig):\n",
    "    \"\"\"Create external stage and target table.\"\"\"\n",
    "    logger.info(\"Creating stage and table...\")\n",
    "    \n",
    "    # Create stage for S3 access\n",
    "    session.sql(f\"\"\"\n",
    "        CREATE OR REPLACE STAGE blob_stage\n",
    "        URL = '{config.stage_url}'\n",
    "        FILE_FORMAT = (TYPE = CSV)\n",
    "    \"\"\").collect()\n",
    "    \n",
    "    # Create raw table with proper schema\n",
    "    session.sql(f\"\"\"\n",
    "        CREATE OR REPLACE TABLE {config.source_table} (\n",
    "            MENU_ID NUMBER(19,0),\n",
    "            MENU_TYPE_ID NUMBER(38,0),\n",
    "            MENU_TYPE VARCHAR,\n",
    "            TRUCK_BRAND_NAME VARCHAR,\n",
    "            MENU_ITEM_ID NUMBER(38,0),\n",
    "            MENU_ITEM_NAME VARCHAR,\n",
    "            ITEM_CATEGORY VARCHAR,\n",
    "            ITEM_SUBCATEGORY VARCHAR,\n",
    "            COST_OF_GOODS_USD NUMBER(38,4),\n",
    "            SALE_PRICE_USD NUMBER(38,4),\n",
    "            MENU_ITEM_HEALTH_METRICS_OBJ VARIANT\n",
    "        )\n",
    "    \"\"\").collect()\n",
    "    \n",
    "    logger.info(\"  ‚úì Stage and table created\")\n",
    "\n",
    "def load_data_from_stage(session, config: PipelineConfig) -> int:\n",
    "    \"\"\"Load CSV data using COPY INTO.\"\"\"\n",
    "    logger.info(f\"Loading data from: {config.stage_path}\")\n",
    "    \n",
    "    result = session.sql(f\"\"\"\n",
    "        COPY INTO {config.source_table}\n",
    "        FROM {config.stage_path}\n",
    "    \"\"\").collect()\n",
    "    \n",
    "    # Get loaded row count\n",
    "    count_result = session.sql(f\"SELECT COUNT(*) FROM {config.source_table}\").collect()\n",
    "    row_count = count_result[0][0]\n",
    "    \n",
    "    logger.info(f\"  ‚úì Loaded {row_count:,} rows\")\n",
    "    return row_count\n",
    "\n",
    "# Execute ingestion\n",
    "logger.info(\"STEP 3: Data Ingestion\")\n",
    "schema_name = setup_snowflake_context(session, config)\n",
    "create_stage_and_table(session, config)\n",
    "metrics.rows_ingested = load_data_from_stage(session, config)\n",
    "\n",
    "# Read into Spark DataFrame for processing\n",
    "df_raw = spark.read.table(config.source_table)\n",
    "logger.info(f\"  ‚úì DataFrame created with {len(df_raw.columns)} columns\")\n",
    "\n",
    "# ============================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd9d3a9c-cdee-43ac-951a-efa5458386c9",
   "metadata": {
    "collapsed": false,
    "name": "cell5"
   },
   "source": [
    "## Other Data Ingestion Methods\n",
    "\n",
    "### Read from Snowflake Tables\n",
    "```python\n",
    "df = spark.read.table(\"MY_DATABASE.MY_SCHEMA.MY_TABLE\")\n",
    "```\n",
    "\n",
    "```python\n",
    "df = spark.sql(\"SELECT * FROM MY_TABLE WHERE status = 'active' LIMIT 1000\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Read from Snowflake Stages\n",
    "\n",
    "```python\n",
    "# parquet\n",
    "df = spark.read.parquet(\"@MY_STAGE/path/to/data.parquet\")\n",
    "```\n",
    "\n",
    "```python\n",
    "# CSV\n",
    "df = spark.read.format(\"csv\") \\\n",
    "    .option(\"header\", True) \\\n",
    "    .option(\"inferSchema\", False) \\\n",
    "    .option(\"delimiter\", \",\") \\\n",
    "    .option(\"quote\", '\"') \\\n",
    "    .option(\"escape\", \"\\\\\") \\\n",
    "    .option(\"nullValue\", \"\") \\\n",
    "    .option(\"dateFormat\", \"yyyy-MM-dd\") \\\n",
    "    .load(\"@MY_STAGE/path/to/data.csv\")\n",
    "```\n",
    "\n",
    "```python\n",
    "# JSON\n",
    "df = spark.read.format(\"json\") \\\n",
    "    .option(\"multiLine\", True) \\\n",
    "    .option(\"allowComments\", True) \\\n",
    "    .option(\"dateFormat\", \"yyyy-MM-dd\") \\\n",
    "    .load(\"@MY_STAGE/path/to/data.json\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Direct Cloud Storage Access (requires credentials)\n",
    "\n",
    "```python\n",
    "# S3\n",
    "df = spark.read.parquet(\"s3://my-bucket/path/to/data/\")\n",
    "```\n",
    "\n",
    "```python\n",
    "# GCP\n",
    "df = spark.read.parquet(\"gs://my-bucket/path/to/data/\")\n",
    "```\n",
    "\n",
    "```python\n",
    "# Azure\n",
    "df = spark.read.parquet(\"wasbs://container@account.blob.core.windows.net/path/to/data/\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Read Multiple Files\n",
    "\n",
    "```python\n",
    "# Wildcard pattern\n",
    "# Match all parquet files in a directory\n",
    "df = spark.read.parquet(\"@MY_STAGE/data/*.parquet\")\n",
    "\n",
    "# Match files with a naming pattern\n",
    "df = spark.read.csv(\"@MY_STAGE/logs/2024-*/events_*.csv\")\n",
    "\n",
    "# Recursive directory search\n",
    "df = spark.read.option(\"recursiveFileLookup\", True).parquet(\"@MY_STAGE/nested_data/\")\n",
    "```\n",
    "\n",
    "\n",
    "### Handling Large CSVs\n",
    "\n",
    "#### ‚ùå Slow Pattern\n",
    "Scans entire file just to guess column types.\n",
    "\n",
    "```python\n",
    "df = spark.read.format(\"csv\") \\\n",
    "    .option(\"header\", True) \\\n",
    "    .option(\"inferSchema\", True) \\\n",
    "    .load(\"@MY_STAGE/large_file.csv\")\n",
    "```\n",
    "\n",
    "#### ‚úÖ Fast Pattern (define schema explicitly)\n",
    "\n",
    "```python\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, TimestampType, BooleanType\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), False),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"amount\", DoubleType(), True),\n",
    "    StructField(\"created_at\", TimestampType(), True),\n",
    "    StructField(\"is_active\", BooleanType(), True)\n",
    "])\n",
    "\n",
    "df = spark.read.format(\"csv\") \\\n",
    "    .option(\"header\", True) \\\n",
    "    .schema(schema) \\\n",
    "    .load(\"@MY_STAGE/large_file.csv\")\n",
    "```\n",
    "\n",
    "#### ‚úÖ Compressed Files (auto-decompressed on read)\n",
    "\n",
    "```python\n",
    "df = spark.read.format(\"csv\") \\\n",
    "    .option(\"header\", True) \\\n",
    "    .schema(schema) \\\n",
    "    .load(\"@MY_STAGE/data.csv.gz\")\n",
    "```\n",
    "\n",
    "#### ‚úÖ Best Practice: Convert CSV to Parquet\n",
    "\n",
    "```python\n",
    "# Read CSV once\n",
    "df_csv = spark.read.format(\"csv\") \\\n",
    "    .option(\"header\", True) \\\n",
    "    .schema(schema) \\\n",
    "    .load(\"@MY_STAGE/raw.csv\")\n",
    "\n",
    "# Write as Parquet\n",
    "df_csv.write.mode(\"overwrite\").parquet(\"@MY_STAGE/optimized/data.parquet\")\n",
    "\n",
    "# Read from Parquet for all future queries\n",
    "df = spark.read.parquet(\"@MY_STAGE/optimized/data.parquet\")\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8104b04d-158d-48bf-848f-711aea9fece7",
   "metadata": {
    "language": "python",
    "name": "cell16"
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# STEP 4: DATA VALIDATION\n",
    "# =============================================================================\n",
    "\n",
    "class ValidationError(Exception):\n",
    "    \"\"\"Raised when critical validation fails.\"\"\"\n",
    "    pass\n",
    "\n",
    "def validate_menu_data(\n",
    "    df: DataFrame,\n",
    "    config: PipelineConfig,\n",
    "    metrics: PipelineMetrics\n",
    ") -> DataFrame:\n",
    "    \"\"\"\n",
    "    Validate menu data before processing.\n",
    "    \n",
    "    Checks:\n",
    "    - Row count within expected range\n",
    "    - Required columns present\n",
    "    - No nulls in key columns\n",
    "    - No duplicate menu items\n",
    "    - Prices are positive\n",
    "    \"\"\"\n",
    "    logger.info(\"STEP 4: Data Validation\")\n",
    "    errors = []\n",
    "    row_count = metrics.rows_ingested\n",
    "    \n",
    "    # Check 1: Row count bounds\n",
    "    if row_count < config.min_expected_rows:\n",
    "        errors.append(f\"Row count {row_count:,} below minimum {config.min_expected_rows:,}\")\n",
    "    if row_count > config.max_expected_rows:\n",
    "        errors.append(f\"Row count {row_count:,} exceeds maximum {config.max_expected_rows:,}\")\n",
    "    else:\n",
    "        logger.info(f\"  ‚úì Row count ({row_count:,}) within bounds\")\n",
    "    \n",
    "    # Check 2: Required columns\n",
    "    required = [\"MENU_ITEM_ID\", \"MENU_ITEM_NAME\", \"TRUCK_BRAND_NAME\", \n",
    "                \"COST_OF_GOODS_USD\", \"SALE_PRICE_USD\"]\n",
    "    missing = [c for c in required if c not in df.columns]\n",
    "    if missing:\n",
    "        errors.append(f\"Missing required columns: {missing}\")\n",
    "    else:\n",
    "        logger.info(f\"  ‚úì All {len(required)} required columns present\")\n",
    "    \n",
    "    # Check 3: Null check on key columns\n",
    "    key_columns = [\"MENU_ITEM_ID\", \"MENU_ITEM_NAME\", \"COST_OF_GOODS_USD\", \"SALE_PRICE_USD\"]\n",
    "    for col_name in key_columns:\n",
    "        null_count = df.filter(col(col_name).isNull()).count()\n",
    "        null_pct = null_count / row_count if row_count > 0 else 0\n",
    "        if null_pct > config.max_null_percentage:\n",
    "            errors.append(f\"Column '{col_name}' has {null_pct:.1%} nulls\")\n",
    "        elif null_pct > 0:\n",
    "            metrics.add_warning(f\"Column '{col_name}' has {null_pct:.1%} nulls\")\n",
    "    \n",
    "    if not any(\"nulls\" in str(e) for e in errors):\n",
    "        logger.info(\"  ‚úì Null check passed\")\n",
    "    \n",
    "    # Check 4: No duplicate menu items\n",
    "    unique_items = df.select(\"MENU_ITEM_ID\").distinct().count()\n",
    "    if unique_items < row_count:\n",
    "        duplicates = row_count - unique_items\n",
    "        metrics.add_warning(f\"Found {duplicates:,} duplicate menu items\")\n",
    "    else:\n",
    "        logger.info(\"  ‚úì No duplicate menu items\")\n",
    "    \n",
    "    # Check 5: Positive prices\n",
    "    negative_prices = df.filter(\n",
    "        (col(\"COST_OF_GOODS_USD\") < 0) | (col(\"SALE_PRICE_USD\") < 0)\n",
    "    ).count()\n",
    "    if negative_prices > 0:\n",
    "        errors.append(f\"Found {negative_prices:,} items with negative prices\")\n",
    "    else:\n",
    "        logger.info(\"  ‚úì All prices are positive\")\n",
    "    \n",
    "    # Handle validation results\n",
    "    if errors:\n",
    "        for error in errors:\n",
    "            logger.error(f\"  ‚úó VALIDATION FAILED: {error}\")\n",
    "        raise ValidationError(f\"Validation failed with {len(errors)} error(s)\")\n",
    "    \n",
    "    metrics.validation_passed = True\n",
    "    metrics.rows_after_validation = row_count\n",
    "    logger.info(\"  ‚úÖ Validation PASSED\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Execute validation\n",
    "df_validated = validate_menu_data(df_raw, config, metrics)\n",
    "\n",
    "# ============================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a77faaf8-8272-4e17-8bbc-39d4f2bc447c",
   "metadata": {
    "name": "cell7"
   },
   "source": [
    "## Data Transformations\n",
    "### Feature Support Matrix:\n",
    "\n",
    "Understanding what PySpark features are supported helps you write efficient code.\n",
    "\n",
    "#### ‚úÖ Fully Supported DataFrame Operations:\n",
    "- `select`, `filter`, `where`\n",
    "- `groupBy`, `agg` (all aggregation functions)\n",
    "- `join` (inner, left, right, outer, broadcast)\n",
    "- `orderBy`, `sort`\n",
    "- `distinct`, `dropDuplicates`\n",
    "- Window functions (`row_number`, `rank`, `lag`, `lead`, etc.)\n",
    "- Built-in functions (95%+ coverage)\n",
    "- `cache`, `persist` (creates temp tables in Snowflake)\n",
    "\n",
    "#### ‚ö†Ô∏è Limited Support:\n",
    "- `repartition` (logical operation only)\n",
    "- `coalesce` (similar to repartition)\n",
    "- Python UDFs (work but slow - avoid if possible)\n",
    "- Pandas UDFs (work but slow - avoid if possible)\n",
    "- MLlib (partial - transformers work, estimators limited)\n",
    "\n",
    "#### ‚ùå NOT Supported:\n",
    "- RDD API completely\n",
    "- `.rdd`, `.foreach()`, `.foreachPartition()`\n",
    "- Structured Streaming\n",
    "- GraphX\n",
    "- Custom data sources\n",
    "- `.checkpoint()`\n",
    "\n",
    "\n",
    "### Data Types Support\n",
    "\n",
    "**‚úÖ Supported:**\n",
    "- String, Integer, Long, Float, Double, Decimal\n",
    "- Boolean, Date, Timestamp\n",
    "- Array, Map, Struct\n",
    "- Binary\n",
    "\n",
    "**‚ùå Not Supported:**\n",
    "- DayTimeIntervalType\n",
    "- YearMonthIntervalType\n",
    "- UserDefinedTypes\n",
    "\n",
    "#### Supported File Formats:\n",
    "- ‚úÖ Parquet, CSV, JSON, Avro, ORC\n",
    "- ‚ùå Delta Lake, Hudi not supported\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd7aedfa-fbdd-4cdc-a8a4-51e0a989f098",
   "metadata": {
    "language": "python",
    "name": "cell17"
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# STEP 5: TRANSFORMATIONS\n",
    "# =============================================================================\n",
    "\n",
    "def transform_menu_data(\n",
    "    df: DataFrame,\n",
    "    metrics: PipelineMetrics\n",
    ") -> DataFrame:\n",
    "    \"\"\"\n",
    "    Apply business transformations to menu data.\n",
    "    \n",
    "    Stages:\n",
    "    5A. Data Cleaning\n",
    "    5B. Profit Calculations\n",
    "    5C. Categorization\n",
    "    5D. Aggregation by Brand\n",
    "    \"\"\"\n",
    "    logger.info(\"STEP 5: Transformations\")\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # STAGE 5A: Data Cleaning\n",
    "    # -------------------------------------------------------------------------\n",
    "    logger.info(\"  5A: Data Cleaning\")\n",
    "    \n",
    "    df_clean = df \\\n",
    "        .withColumn(\"TRUCK_BRAND_NAME\", trim(upper(col(\"TRUCK_BRAND_NAME\")))) \\\n",
    "        .withColumn(\"ITEM_CATEGORY\", trim(upper(col(\"ITEM_CATEGORY\")))) \\\n",
    "        .withColumn(\"MENU_TYPE\", trim(upper(col(\"MENU_TYPE\")))) \\\n",
    "        .filter(col(\"COST_OF_GOODS_USD\").isNotNull()) \\\n",
    "        .filter(col(\"SALE_PRICE_USD\").isNotNull())\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # STAGE 5B: Profit Calculations\n",
    "    # -------------------------------------------------------------------------\n",
    "    logger.info(\"  5B: Profit Calculations\")\n",
    "    \n",
    "    df_with_profit = df_clean \\\n",
    "        .withColumn(\n",
    "            \"PROFIT_USD\",\n",
    "            round(col(\"SALE_PRICE_USD\") - col(\"COST_OF_GOODS_USD\"), 2)\n",
    "        ) \\\n",
    "        .withColumn(\n",
    "            \"PROFIT_MARGIN_PCT\",\n",
    "            round(\n",
    "                (col(\"SALE_PRICE_USD\") - col(\"COST_OF_GOODS_USD\")) / \n",
    "                col(\"SALE_PRICE_USD\") * 100, \n",
    "                2\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # STAGE 5C: Categorization\n",
    "    # -------------------------------------------------------------------------\n",
    "    logger.info(\"  5C: Profit Categorization\")\n",
    "    \n",
    "    df_categorized = df_with_profit \\\n",
    "        .withColumn(\n",
    "            \"PROFIT_TIER\",\n",
    "            when(col(\"PROFIT_MARGIN_PCT\") >= 70, \"Premium\")\n",
    "            .when(col(\"PROFIT_MARGIN_PCT\") >= 50, \"High\")\n",
    "            .when(col(\"PROFIT_MARGIN_PCT\") >= 30, \"Medium\")\n",
    "            .otherwise(\"Low\")\n",
    "        ) \\\n",
    "        .withColumn(\n",
    "            \"PRICE_TIER\",\n",
    "            when(col(\"SALE_PRICE_USD\") >= 10, \"Premium\")\n",
    "            .when(col(\"SALE_PRICE_USD\") >= 5, \"Mid-Range\")\n",
    "            .otherwise(\"Value\")\n",
    "        )\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # STAGE 5D: Aggregation by Brand\n",
    "    # -------------------------------------------------------------------------\n",
    "    logger.info(\"  5D: Aggregation by Brand\")\n",
    "    \n",
    "    df_brand_summary = df_categorized.groupBy(\n",
    "        \"TRUCK_BRAND_NAME\",\n",
    "        \"MENU_TYPE\"\n",
    "    ).agg(\n",
    "        count(\"*\").alias(\"ITEM_COUNT\"),\n",
    "        round(avg(\"COST_OF_GOODS_USD\"), 2).alias(\"AVG_COST_USD\"),\n",
    "        round(avg(\"SALE_PRICE_USD\"), 2).alias(\"AVG_PRICE_USD\"),\n",
    "        round(avg(\"PROFIT_USD\"), 2).alias(\"AVG_PROFIT_USD\"),\n",
    "        round(avg(\"PROFIT_MARGIN_PCT\"), 2).alias(\"AVG_MARGIN_PCT\"),\n",
    "        round(min(\"PROFIT_USD\"), 2).alias(\"MIN_PROFIT_USD\"),\n",
    "        round(max(\"PROFIT_USD\"), 2).alias(\"MAX_PROFIT_USD\"),\n",
    "        round(sum(\"PROFIT_USD\"), 2).alias(\"TOTAL_POTENTIAL_PROFIT_USD\")\n",
    "    ).orderBy(col(\"AVG_MARGIN_PCT\").desc())\n",
    "    \n",
    "    # Add metadata\n",
    "    df_final = df_brand_summary \\\n",
    "        .withColumn(\"PIPELINE_RUN_ID\", lit(metrics.run_id)) \\\n",
    "        .withColumn(\"PROCESSED_AT\", current_timestamp())\n",
    "    \n",
    "    # Update metrics\n",
    "    metrics.rows_after_transform = df_final.count()\n",
    "    metrics.total_brands = df_final.select(\"TRUCK_BRAND_NAME\").distinct().count()\n",
    "    metrics.total_menu_items = df_categorized.count()\n",
    "    \n",
    "    # Calculate overall average margin\n",
    "    avg_margin = df_final.agg(avg(\"AVG_MARGIN_PCT\")).collect()[0][0]\n",
    "    metrics.avg_profit_margin = float(avg_margin) if avg_margin else 0.0\n",
    "    \n",
    "    logger.info(f\"  ‚úì Aggregated to {metrics.rows_after_transform:,} brand/menu-type combinations\")\n",
    "    logger.info(f\"  ‚úì {metrics.total_brands} unique brands analyzed\")\n",
    "    logger.info(f\"  ‚úì Average profit margin: {metrics.avg_profit_margin:.1f}%\")\n",
    "    \n",
    "    return df_final\n",
    "\n",
    "# Execute transformations\n",
    "df_transformed = transform_menu_data(df_validated, metrics)\n",
    "df_transformed.cache()\n",
    "\n",
    "# Preview results\n",
    "logger.info(\"\\nüìä Brand Summary Preview:\")\n",
    "df_transformed.select(\n",
    "    \"TRUCK_BRAND_NAME\", \"MENU_TYPE\", \"ITEM_COUNT\", \n",
    "    \"AVG_PRICE_USD\", \"AVG_PROFIT_USD\", \"AVG_MARGIN_PCT\"\n",
    ").show(10, truncate=False)\n",
    "\n",
    "# ============================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf83207d-9dec-41c5-8c62-8943f367feb0",
   "metadata": {
    "name": "cell8"
   },
   "source": [
    "## Data Transformation API Reference\n",
    "\n",
    "All transformations push down to Snowflake SQL - no data leaves the warehouse until you explicitly collect results.\n",
    "\n",
    "### Required Imports\n",
    "\n",
    "```python\n",
    "from pyspark.sql.functions import (\n",
    "    col, lit, when, coalesce,\n",
    "    sum, avg, count, min, max, countDistinct, collect_list,\n",
    "    year, month, dayofmonth, hour, minute, second, dayofweek, dayofyear, weekofyear, quarter,\n",
    "    to_date, to_timestamp, date_add, date_sub, months_between, datediff, date_trunc,\n",
    "    upper, lower, initcap, trim, ltrim, rtrim, lpad, rpad,\n",
    "    concat, substring, regexp_replace, regexp_extract,\n",
    "    row_number, rank, dense_rank, percent_rank, lag, lead, first, last,\n",
    "    broadcast\n",
    ")\n",
    "from pyspark.sql.window import Window\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Selecting and Filtering\n",
    "\n",
    "**Select columns:**\n",
    "\n",
    "```python\n",
    "df.select(\"id\", \"name\", \"amount\")\n",
    "df.select(col(\"id\"), col(\"name\").alias(\"customer_name\"))  # with rename\n",
    "```\n",
    "\n",
    "**Filter rows:**\n",
    "\n",
    "```python\n",
    "df.filter(col(\"status\") == \"active\")\n",
    "df.where(col(\"amount\") > 100)\n",
    "df.filter((col(\"status\") == \"active\") & (col(\"amount\") > 100))  # AND\n",
    "df.filter((col(\"status\") == \"active\") | (col(\"amount\") > 1000)) # OR\n",
    "```\n",
    "\n",
    "**Remove duplicates:**\n",
    "\n",
    "```python\n",
    "df.distinct()                                    # all columns\n",
    "df.dropDuplicates([\"customer_id\", \"order_date\"]) # specific columns\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Adding and Modifying Columns\n",
    "\n",
    "**Add calculated columns:**\n",
    "\n",
    "```python\n",
    "df.withColumn(\"total\", col(\"price\") * col(\"quantity\"))\n",
    "df.withColumn(\"discounted_price\", col(\"price\") * 0.9)\n",
    "df.withColumn(\"status_flag\", when(col(\"status\") == \"active\", 1).otherwise(0))\n",
    "```\n",
    "\n",
    "**Rename columns:**\n",
    "\n",
    "```python\n",
    "df.withColumnRenamed(\"old_name\", \"new_name\")\n",
    "df.toDF(\"col1\", \"col2\", \"col3\")  # rename all at once\n",
    "```\n",
    "\n",
    "**Cast data types:**\n",
    "\n",
    "```python\n",
    "df.withColumn(\"amount\", col(\"amount\").cast(\"double\"))\n",
    "df.withColumn(\"event_date\", col(\"timestamp_col\").cast(\"date\"))\n",
    "```\n",
    "\n",
    "**Handle null values:**\n",
    "\n",
    "```python\n",
    "df.fillna(0, subset=[\"amount\"])                         # single column\n",
    "df.fillna({\"amount\": 0, \"name\": \"Unknown\"})             # multiple columns\n",
    "df.withColumn(\"value\", coalesce(col(\"value\"), lit(0)))  # using coalesce\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Aggregations\n",
    "\n",
    "**Group and aggregate:**\n",
    "\n",
    "```python\n",
    "df.groupBy(\"category\").agg(\n",
    "    count(\"*\").alias(\"row_count\"),\n",
    "    countDistinct(\"customer_id\").alias(\"unique_customers\"),\n",
    "    sum(\"amount\").alias(\"total_amount\"),\n",
    "    avg(\"amount\").alias(\"avg_amount\"),\n",
    "    min(\"amount\").alias(\"min_amount\"),\n",
    "    max(\"amount\").alias(\"max_amount\"),\n",
    "    collect_list(\"product_name\").alias(\"products\")\n",
    ")\n",
    "```\n",
    "\n",
    "**Multiple grouping columns:**\n",
    "\n",
    "```python\n",
    "df.groupBy(\"category\", \"region\").agg(\n",
    "    sum(\"amount\").alias(\"total_sales\"),\n",
    "    avg(\"amount\").alias(\"average_sale\")\n",
    ")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Joins\n",
    "\n",
    "| Join Type | Syntax |\n",
    "|-----------|--------|\n",
    "| **Inner** | `df1.join(df2, \"id\", \"inner\")` |\n",
    "| **Left** | `df1.join(df2, \"customer_id\", \"left\")` |\n",
    "| **Right** | `df1.join(df2, \"customer_id\", \"right\")` |\n",
    "| **Outer** | `df1.join(df2, \"customer_id\", \"outer\")` |\n",
    "| **Cross** | `df1.crossJoin(df2)` |\n",
    "\n",
    "**Explicit join condition:**\n",
    "\n",
    "```python\n",
    "df1.join(df2, df1[\"id\"] == df2[\"id\"], \"inner\")\n",
    "```\n",
    "\n",
    "**Broadcast small tables** (performance optimization):\n",
    "\n",
    "```python\n",
    "df_large.join(broadcast(df_small), \"key_column\", \"left\")\n",
    "```\n",
    "\n",
    "**Handle column name conflicts:**\n",
    "\n",
    "```python\n",
    "df_joined = df1.alias(\"a\").join(df2.alias(\"b\"), col(\"a.id\") == col(\"b.id\"))\n",
    "df_result = df_joined.select(col(\"a.id\"), col(\"a.name\"), col(\"b.value\"))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Window Functions\n",
    "\n",
    "**Define window specification:**\n",
    "\n",
    "```python\n",
    "window_spec = Window.partitionBy(\"customer_id\").orderBy(\"order_date\")\n",
    "\n",
    "# With frame bounds\n",
    "window_frame = Window.partitionBy(\"customer_id\") \\\n",
    "    .orderBy(\"order_date\") \\\n",
    "    .rowsBetween(-2, 0)\n",
    "```\n",
    "\n",
    "**Ranking functions:**\n",
    "\n",
    "```python\n",
    "df.withColumn(\"row_num\", row_number().over(window_spec))\n",
    "df.withColumn(\"rank\", rank().over(window_spec))              # gaps for ties\n",
    "df.withColumn(\"dense_rank\", dense_rank().over(window_spec))  # no gaps\n",
    "df.withColumn(\"pct_rank\", percent_rank().over(window_spec))\n",
    "```\n",
    "\n",
    "**Analytic functions:**\n",
    "\n",
    "```python\n",
    "df.withColumn(\"prev_amount\", lag(\"amount\", 1).over(window_spec))\n",
    "df.withColumn(\"next_amount\", lead(\"amount\", 1).over(window_spec))\n",
    "df.withColumn(\"first_order\", first(\"amount\").over(window_spec))\n",
    "df.withColumn(\"last_order\", last(\"amount\").over(window_spec))\n",
    "```\n",
    "\n",
    "**Running calculations:**\n",
    "\n",
    "```python\n",
    "window_running = Window.partitionBy(\"customer_id\") \\\n",
    "    .orderBy(\"order_date\") \\\n",
    "    .rowsBetween(Window.unboundedPreceding, 0)\n",
    "\n",
    "df.withColumn(\"running_total\", sum(\"amount\").over(window_running))\n",
    "df.withColumn(\"running_avg\", avg(\"amount\").over(window_running))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 6. Date and Time Operations\n",
    "\n",
    "**Extract components:**\n",
    "\n",
    "| Function | Example |\n",
    "|----------|---------|\n",
    "| `year()` | `df.withColumn(\"year\", year(\"ts\"))` |\n",
    "| `month()` | `df.withColumn(\"month\", month(\"ts\"))` |\n",
    "| `dayofmonth()` | `df.withColumn(\"day\", dayofmonth(\"ts\"))` |\n",
    "| `hour()` | `df.withColumn(\"hour\", hour(\"ts\"))` |\n",
    "| `dayofweek()` | `df.withColumn(\"dow\", dayofweek(\"ts\"))` |\n",
    "| `quarter()` | `df.withColumn(\"qtr\", quarter(\"ts\"))` |\n",
    "\n",
    "**Parse strings to dates:**\n",
    "\n",
    "```python\n",
    "df.withColumn(\"date_col\", to_date(\"date_string\", \"yyyy-MM-dd\"))\n",
    "df.withColumn(\"ts_col\", to_timestamp(\"ts_string\", \"yyyy-MM-dd HH:mm:ss\"))\n",
    "```\n",
    "\n",
    "**Date arithmetic:**\n",
    "\n",
    "```python\n",
    "df.withColumn(\"next_week\", date_add(\"date_col\", 7))\n",
    "df.withColumn(\"last_week\", date_sub(\"date_col\", 7))\n",
    "df.withColumn(\"days_diff\", datediff(\"end_date\", \"start_date\"))\n",
    "df.withColumn(\"months_diff\", months_between(\"end_date\", \"start_date\"))\n",
    "df.withColumn(\"month_start\", date_trunc(\"month\", \"date_col\"))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 7. String Operations\n",
    "\n",
    "**Case transformation:**\n",
    "\n",
    "```python\n",
    "df.withColumn(\"upper_name\", upper(\"name\"))\n",
    "df.withColumn(\"lower_name\", lower(\"name\"))\n",
    "df.withColumn(\"title_name\", initcap(\"name\"))\n",
    "```\n",
    "\n",
    "**Pattern matching:**\n",
    "\n",
    "```python\n",
    "df.withColumn(\"has_email\", col(\"text\").contains(\"@\"))\n",
    "df.withColumn(\"cleaned\", regexp_replace(\"text\", \"[^a-zA-Z0-9]\", \"\"))\n",
    "df.withColumn(\"domain\", regexp_extract(\"email\", \"@(.+)$\", 1))\n",
    "```\n",
    "\n",
    "**Trim and pad:**\n",
    "\n",
    "```python\n",
    "df.withColumn(\"trimmed\", trim(\"text\"))\n",
    "df.withColumn(\"padded_id\", lpad(\"id\", 10, \"0\"))    # \"42\" ‚Üí \"0000000042\"\n",
    "df.withColumn(\"padded_right\", rpad(\"code\", 5, \"X\")) # \"AB\" ‚Üí \"ABXXX\"\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 8. Sorting and Limiting\n",
    "\n",
    "**Sort results:**\n",
    "\n",
    "```python\n",
    "df.orderBy(\"name\")                                            # ascending\n",
    "df.orderBy(col(\"amount\").desc())                              # descending\n",
    "df.orderBy(col(\"category\").asc(), col(\"amount\").desc())       # multiple\n",
    "df.orderBy(col(\"value\").asc_nulls_first())                    # nulls first\n",
    "df.orderBy(col(\"value\").desc_nulls_last())                    # nulls last\n",
    "```\n",
    "\n",
    "**Limit rows:**\n",
    "\n",
    "```python\n",
    "df.limit(10)                                    # first 10 rows\n",
    "df.orderBy(col(\"amount\").desc()).limit(10)     # top 10 by amount\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de21d75-6860-4821-8155-ac6c3b999c01",
   "metadata": {
    "language": "python",
    "name": "cell18"
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# STEP 6: DATA QUALITY CHECKS\n",
    "# =============================================================================\n",
    "\n",
    "def quality_checks(\n",
    "    df: DataFrame,\n",
    "    config: PipelineConfig,\n",
    "    metrics: PipelineMetrics\n",
    ") -> float:\n",
    "    \"\"\"Validate output data quality before writing.\"\"\"\n",
    "    logger.info(\"STEP 6: Data Quality Checks\")\n",
    "    checks_passed = 0\n",
    "    total_checks = 4\n",
    "    \n",
    "    row_count = metrics.rows_after_transform\n",
    "    \n",
    "    # Check 1: Non-zero output\n",
    "    if row_count > 0:\n",
    "        checks_passed += 1\n",
    "        logger.info(f\"  ‚úì Output has {row_count:,} rows\")\n",
    "    else:\n",
    "        logger.error(\"  ‚úó Output is empty!\")\n",
    "    \n",
    "    # Check 2: No duplicate keys\n",
    "    key_cols = [\"TRUCK_BRAND_NAME\", \"MENU_TYPE\"]\n",
    "    distinct_keys = df.select(key_cols).distinct().count()\n",
    "    if distinct_keys == row_count:\n",
    "        checks_passed += 1\n",
    "        logger.info(\"  ‚úì No duplicate brand/menu-type combinations\")\n",
    "    else:\n",
    "        logger.error(f\"  ‚úó Found {row_count - distinct_keys:,} duplicate keys\")\n",
    "    \n",
    "    # Check 3: All margins are reasonable (not negative)\n",
    "    negative_margins = df.filter(col(\"AVG_MARGIN_PCT\") < config.min_profit_margin).count()\n",
    "    if negative_margins == 0:\n",
    "        checks_passed += 1\n",
    "        logger.info(\"  ‚úì All profit margins are positive\")\n",
    "    else:\n",
    "        metrics.add_warning(f\"Found {negative_margins:,} brands with negative margins\")\n",
    "    \n",
    "    # Check 4: Data completeness\n",
    "    null_brands = df.filter(col(\"TRUCK_BRAND_NAME\").isNull()).count()\n",
    "    if null_brands == 0:\n",
    "        checks_passed += 1\n",
    "        logger.info(\"  ‚úì All records have brand names\")\n",
    "    else:\n",
    "        logger.error(f\"  ‚úó Found {null_brands:,} records without brand names\")\n",
    "    \n",
    "    quality_score = checks_passed / total_checks\n",
    "    logger.info(f\"\\n  üìà Quality Score: {quality_score:.0%} ({checks_passed}/{total_checks} checks)\")\n",
    "    \n",
    "    return quality_score\n",
    "\n",
    "# Execute quality checks\n",
    "metrics.quality_score = quality_checks(df_transformed, config, metrics)\n",
    "\n",
    "# ============================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44038781-51a9-40b8-a03c-b8e3ffcca11b",
   "metadata": {
    "language": "python",
    "name": "cell19"
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# STEP 7: WRITE OUTPUT\n",
    "# =============================================================================\n",
    "\n",
    "def write_output(\n",
    "    df: DataFrame,\n",
    "    config: PipelineConfig,\n",
    "    metrics: PipelineMetrics\n",
    ") -> int:\n",
    "    \"\"\"Write transformed data to Snowflake table.\"\"\"\n",
    "    logger.info(\"STEP 7: Write Output\")\n",
    "    logger.info(f\"  Destination: {config.output_table}\")\n",
    "    logger.info(f\"  Mode: {config.output_mode}\")\n",
    "    \n",
    "    # Write to Snowflake\n",
    "    df.write.mode(config.output_mode).saveAsTable(config.output_table)\n",
    "    \n",
    "    # Verify\n",
    "    written_count = spark.read.table(config.output_table).count()\n",
    "    logger.info(f\"  ‚úì Rows written: {written_count:,}\")\n",
    "    \n",
    "    return written_count\n",
    "\n",
    "# Execute write\n",
    "metrics.rows_written = write_output(df_transformed, config, metrics)\n",
    "\n",
    "# =============================================================================\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc3fd5b-f351-43fe-87a7-dcdda75314e3",
   "metadata": {
    "name": "cell6"
   },
   "source": [
    "## Other Ways to Write Data\n",
    "\n",
    "Write transformed data back to Snowflake tables, stages, or cloud storage.\n",
    "\n",
    "---\n",
    "\n",
    "### Write to Snowflake Tables\n",
    "\n",
    "```python\n",
    "# Overwrite replaces all existing data\n",
    "df.write.mode(\"overwrite\").saveAsTable(\"MY_TABLE\")\n",
    "\n",
    "# Append adds new rows to existing data (use for incremental loads)\n",
    "df.write.mode(\"append\").saveAsTable(\"MY_TABLE\")\n",
    "\n",
    "# Ignore skips write if destination exists (use for idempotent operations)\n",
    "df.write.mode(\"ignore\").saveAsTable(\"MY_TABLE\")\n",
    "\n",
    "# Error fails if destination exists\n",
    "df.write.mode(\"error\").saveAsTable(\"MY_TABLE\")\n",
    "```\n",
    "\n",
    "### Write to Snowflake Stages\n",
    "\n",
    "```python\n",
    "# Parquet (recommended)\n",
    "df.write.mode(\"overwrite\").parquet(\"@MY_STAGE/output/data.parquet\")\n",
    "\n",
    "# CSV\n",
    "df.write.mode(\"overwrite\") \\\n",
    "    .option(\"header\", True) \\\n",
    "    .option(\"compression\", \"gzip\") \\\n",
    "    .option(\"delimiter\", \",\") \\\n",
    "    .option(\"quote\", '\"') \\\n",
    "    .csv(\"@MY_STAGE/output/data.csv\")\n",
    "\n",
    "# JSON\n",
    "df.write.mode(\"overwrite\") \\\n",
    "    .option(\"compression\", \"gzip\") \\\n",
    "    .json(\"@MY_STAGE/output/data.json\")\n",
    "```\n",
    "\n",
    "\n",
    "### Write to Cloud Storage\n",
    "\n",
    "```python\n",
    "# S3\n",
    "df.write.mode(\"overwrite\").parquet(\"s3://my-bucket/output/data/\")\n",
    "```\n",
    "\n",
    "```python\n",
    "# GCP\n",
    "df.write.mode(\"overwrite\").parquet(\"gs://my-bucket/output/data/\")\n",
    "```\n",
    "\n",
    "```python\n",
    "# Azure\n",
    "df.write.mode(\"overwrite\").parquet(\"wasbs://container@account.blob.core.windows.net/output/data/\")\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cca66b21",
   "metadata": {},
   "source": [
    "## Performance and Compression\n",
    "\n",
    "\n",
    "### Partitioning for Performance\n",
    "\n",
    "\n",
    "\n",
    "```python\n",
    "# Multi-column partitioning\n",
    "df.write.mode(\"overwrite\") \\\n",
    "    .partitionBy(\"year\", \"month\") \\\n",
    "    .parquet(\"@MY_STAGE/partitioned_data/\")\n",
    "\n",
    "\n",
    "# Single column partitioning\n",
    "df.write.mode(\"overwrite\") \\\n",
    "    .partitionBy(\"event_date\") \\\n",
    "    .parquet(\"@MY_STAGE/daily_data/\")\n",
    "```\n",
    "\n",
    "**Benefits:**\n",
    "- Faster queries that filter on partition columns\n",
    "- Enables parallel reads of different partitions\n",
    "- Easier data management (delete old partitions)\n",
    "\n",
    "**Considerations:**\n",
    "- Too many partitions = too many small files\n",
    "- Choose columns with moderate cardinality\n",
    "- Date-based partitioning is very common\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Compression Options\n",
    "\n",
    "| Compression | Speed | Ratio | Use Case |\n",
    "|-------------|-------|-------|----------|\n",
    "| **Snappy** | Fast | Moderate | Default for Parquet, frequently accessed data |\n",
    "| **Gzip** | Slow | High | Archival, infrequent access |\n",
    "| **LZ4** | Very Fast | Moderate | High-throughput workloads |\n",
    "| **Zstd** | Medium | High | Good balance of speed and compression |\n",
    "| **None** | N/A | None | Rarely recommended |\n",
    "\n",
    "```python\n",
    "# Snappy (default)\n",
    "df.write.option(\"compression\", \"snappy\").parquet(\"@MY_STAGE/data/\")\n",
    "\n",
    "# Gzip\n",
    "df.write.option(\"compression\", \"gzip\").parquet(\"@MY_STAGE/data/\")\n",
    "\n",
    "# LZ4\n",
    "df.write.option(\"compression\", \"lz4\").parquet(\"@MY_STAGE/data/\")\n",
    "\n",
    "# Zstd\n",
    "df.write.option(\"compression\", \"zstd\").parquet(\"@MY_STAGE/data/\")\n",
    "\n",
    "# None\n",
    "df.write.option(\"compression\", \"none\").parquet(\"@MY_STAGE/data/\")\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c2c73d-8d04-431e-a077-f6e46b2967f7",
   "metadata": {
    "language": "python",
    "name": "cell20"
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# STEP 8: CLEANUP & SUMMARY\n",
    "# =============================================================================\n",
    "\n",
    "def cleanup_and_summarize(df_cached: DataFrame, metrics: PipelineMetrics):\n",
    "    \"\"\"Release resources and log final summary.\"\"\"\n",
    "    logger.info(\"STEP 8: Cleanup & Summary\")\n",
    "    \n",
    "    # Release cache\n",
    "    try:\n",
    "        df_cached.unpersist()\n",
    "        logger.info(\"  ‚úì Cache released\")\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"  Could not unpersist: {e}\")\n",
    "    \n",
    "    # Finalize metrics\n",
    "    metrics.end_time = datetime.now()\n",
    "    metrics.status = \"SUCCESS\" if metrics.quality_score >= 0.75 else \"COMPLETED_WITH_WARNINGS\"\n",
    "    \n",
    "    # Print summary\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"üéâ PIPELINE SUMMARY\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"  Run ID:            {metrics.run_id}\")\n",
    "    print(f\"  Status:            {metrics.status}\")\n",
    "    print(f\"  Duration:          {metrics.duration_seconds():.1f} seconds\")\n",
    "    print(f\"  {'‚îÄ'*56}\")\n",
    "    print(f\"  Menu Items:        {metrics.total_menu_items:,}\")\n",
    "    print(f\"  Brands Analyzed:   {metrics.total_brands}\")\n",
    "    print(f\"  Avg Profit Margin: {metrics.avg_profit_margin:.1f}%\")\n",
    "    print(f\"  {'‚îÄ'*56}\")\n",
    "    print(f\"  Rows In:           {metrics.rows_ingested:,}\")\n",
    "    print(f\"  Rows Out:          {metrics.rows_written:,}\")\n",
    "    print(f\"  Quality Score:     {metrics.quality_score:.0%}\")\n",
    "    \n",
    "    if metrics.warnings:\n",
    "        print(f\"  {'‚îÄ'*56}\")\n",
    "        print(f\"  ‚ö†Ô∏è  Warnings ({len(metrics.warnings)}):\")\n",
    "        for w in metrics.warnings:\n",
    "            print(f\"      ‚Ä¢ {w}\")\n",
    "    \n",
    "    print(f\"{'='*60}\\n\")\n",
    "\n",
    "# Execute cleanup\n",
    "cleanup_and_summarize(df_transformed, metrics)\n",
    "\n",
    "# =============================================================================\n",
    "# BEST PRACTICES\n",
    "# =============================================================================\n",
    "#\n",
    "# ‚úÖ PERFORMANCE:\n",
    "#    - Use COPY INTO for bulk loading (faster than spark.read for CSV)\n",
    "#    - Cache DataFrame for multiple operations\n",
    "#    - Avoid UDFs\n",
    "#\n",
    "# ‚úÖ RELIABILITY:\n",
    "#    - Quality gates before writing output\n",
    "#    - Type-safe configuration with dataclasses\n",
    "#    - Proper error handling with custom exceptions\n",
    "#\n",
    "# ‚úÖ OBSERVABILITY:\n",
    "#    - Structured logging at each stage\n",
    "#    - Unique run ID for tracing\n",
    "#    - Row counts and business metrics tracked\n",
    "#\n",
    "# =============================================================================\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6dd6008-0a4e-48be-a4d1-dbb899c2cf31",
   "metadata": {
    "name": "cell10"
   },
   "source": [
    "## Best Practices\n",
    "\n",
    "Follow these best practices to get optimal performance from Snowpark Connect.\n",
    "\n",
    "1. **Use SQL Functions Over UDFs:** Python UDFs require data to be transferred to the client, processed, and sent back - this is 10-100x slower than native operations!\n",
    "2. **Broadcast Joins for Small Tables:** When joining a large table with a small dimension table, use `broadcast()` to optimize the join.\n",
    "3. **Cache Frequently Accessed DataFrames:** Caching creates temporary tables in Snowflake for faster repeated access. Remember to `unpersist()` when done!\n",
    "4. **Minimize Data Movement:** Process data in Snowflake and only transfer final results. Avoid `collect()` on large datasets!\n",
    "5. **Partition Awareness:** Filter on partitioned columns to enable partition pruning and reduce data scanned."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f2c24f-5071-47ab-a371-5a553e540c79",
   "metadata": {
    "name": "cell11"
   },
   "source": [
    "## Additional Resources\n",
    "\n",
    "**Official Documentation:**\n",
    "- [Snowflake Documentation](https://docs.snowflake.com/)\n",
    "- [PySpark API Reference](https://spark.apache.org/docs/latest/api/python/)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "lastEditStatus": {
   "authorEmail": "vino.duraisamy@snowflake.com",
   "authorId": "5740887325800",
   "authorName": "VINOD",
   "lastEditTime": 1764953239763,
   "notebookId": "ruwftjbxrzkd3lbmjqih",
   "sessionId": "c05c7547-88d0-4b71-abd9-316d5b62b724"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
