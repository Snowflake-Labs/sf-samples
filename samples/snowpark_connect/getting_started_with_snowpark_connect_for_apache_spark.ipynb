{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa0bb148-3018-4471-870c-73e6033f91ec",
   "metadata": {
    "collapsed": false,
    "name": "introduction"
   },
   "source": [
    "# Getting Started with Snowpark Connect for Apache Spark™\n",
    "\n",
    "Snowpark Connect enables you to run PySpark workloads directly against Snowflake, combining the familiarity of Spark with the power of Snowflake's cloud data platform.\n",
    "\n",
    "### What You'll Learn\n",
    "- **Setup**: Initialize a Spark session with Snowpark Connect\n",
    "- **Data Generation**: Create synthetic data using PySpark and write to Snowflake\n",
    "- **Spark UDFs**: Apply custom transformations using Spark User-Defined Functions\n",
    "- **Snowflake UDFs & SQL Passthrough**: Create Snowflake Python functions and execute native SQL from Spark\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "953912a5-e524-4a9d-b81b-acc64cd8aa3b",
   "metadata": {
    "collapsed": false,
    "name": "connectionmd"
   },
   "source": [
    "## 1. Setup and Connection\n",
    "\n",
    "Initialize the Snowpark Connect server and verify the connection with a simple query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "092e2bd8-0891-403c-8d12-189b986071c9",
   "metadata": {
    "language": "python",
    "name": "establish_connection"
   },
   "outputs": [],
   "source": [
    "from snowflake import snowpark_connect\n",
    "from snowflake.snowpark.context import get_active_session\n",
    "\n",
    "session = get_active_session()\n",
    "print(session)\n",
    "\n",
    "spark = snowpark_connect.server.init_spark_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50bc29da-af06-4528-8efc-a5a33bfd84f0",
   "metadata": {
    "collapsed": false,
    "name": "querydata"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d50cbf4-0c8d-4950-86cb-114990437ac9",
   "metadata": {
    "language": "python",
    "name": "connection_test"
   },
   "outputs": [],
   "source": [
    "df = spark.sql(\"show tables\").limit(10)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7efec50-96c1-4d5a-8c97-4aa8fdcfd47c",
   "metadata": {
    "collapsed": false,
    "name": "simple_examplemd"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da394518-2335-4753-ab0d-1ab2277822a8",
   "metadata": {
    "language": "python",
    "name": "simpleexample"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "df = spark.createDataFrame([\n",
    "                Row(a=1, b=2.),\n",
    "                Row(a=2, b=3.),\n",
    "                Row(a=4, b=5.),])\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c0f6f7-d767-4232-90a0-4b1c94c5eeb9",
   "metadata": {
    "collapsed": false,
    "name": "PySpark_syntheticdata"
   },
   "source": [
    "## 2. Generate Synthetic Data\n",
    "\n",
    "Create synthetic support case data using PySpark and write it to a Snowflake table. This demonstrates data generation, schema definition, and writing DataFrames to Snowflake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c6174b-8204-48b1-82b3-e79b4fc33c5d",
   "metadata": {
    "language": "python",
    "name": "generate_synthetic_supportcases"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StringType, TimestampType, BooleanType\n",
    "from pyspark.sql.functions import lit, rand, expr, date_add, to_timestamp\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "\n",
    "\n",
    "schema = \"\"\"\n",
    "    CASE_ID STRING,\n",
    "    CATEGORY STRING,\n",
    "    CASE_TITLE STRING,\n",
    "    CASE_DESCRIPTION STRING,\n",
    "    LAST_UPDATE TIMESTAMP,\n",
    "    STATUS STRING,\n",
    "    DATE_CREATED TIMESTAMP,\n",
    "    DATE_CLOSED TIMESTAMP,\n",
    "    REQUIRED_ESCALATION BOOLEAN\n",
    "\"\"\"\n",
    "\n",
    "# --- Data Generation Parameters ---\n",
    "num_records = 1000 \n",
    "\n",
    "categories = [\"Technical Issue\", \"Billing Inquiry\", \"Feature Request\", \"Account Management\", \"Bug Report\", \"General Question\"]\n",
    "statuses = [\"Open\", \"In Progress\", \"Resolved\", \"Closed\", \"Pending Customer\"]\n",
    "case_titles_templates = [\n",
    "    \"Problem with {}\",\n",
    "    \"Issue accessing {}\",\n",
    "    \"Request for new feature: {}\",\n",
    "    \"Account update required for {}\",\n",
    "    \"Bug in {} module\",\n",
    "    \"Question about {} functionality\"\n",
    "]\n",
    "description_templates = [\n",
    "    \"User reported that {} is not working as expected. Needs investigation.\",\n",
    "    \"Customer is unable to {} after recent update.\",\n",
    "    \"Details: User needs assistance with {}. Please provide guidance.\",\n",
    "    \"Troubleshooting steps taken: {}. Still experiencing the issue.\",\n",
    "    \"New feature request: {}. Describe desired functionality and benefits.\",\n",
    "    \"Error observed: {}. Stack trace attached if available.\"\n",
    "]\n",
    "\n",
    "# Function to generate a random datetime within a range\n",
    "def random_date(start_date, end_date):\n",
    "    # Ensure start_date is not after end_date. If they are the same, return start_date.\n",
    "    if start_date >= end_date:\n",
    "        return start_date # Or handle as an error, but returning start_date is safer for small ranges\n",
    "\n",
    "    time_between_dates = end_date - start_date\n",
    "    days_between_dates = time_between_dates.days\n",
    "\n",
    "    # Ensure days_between_dates is at least 0 before calling randrange\n",
    "    # randrange(0) is valid and returns 0, so if days_between_dates is 0, this works.\n",
    "    if days_between_dates < 0: # This should ideally not happen with the check above, but as a safeguard\n",
    "        days_between_dates = 0\n",
    "\n",
    "    random_number_of_days = random.randrange(days_between_dates + 1) # +1 to include the end_date day\n",
    "    \n",
    "    random_date_offset = start_date + timedelta(days=random_number_of_days)\n",
    "    \n",
    "    \n",
    "    if days_between_dates == 0:\n",
    "        # If dates are the same, pick a time between start_date.time() and end_date.time()\n",
    "        total_seconds_in_range = int((end_date - start_date).total_seconds())\n",
    "        if total_seconds_in_range <= 0:\n",
    "            return start_date # Should already be handled by the start_date >= end_date check\n",
    "        random_seconds_offset = random.randrange(total_seconds_in_range + 1)\n",
    "        return start_date + timedelta(seconds=random_seconds_offset)\n",
    "    else:\n",
    "        # If dates span multiple days, pick a full day offset and then a random time within that day\n",
    "        random_seconds_offset = random.randrange(86400) # seconds in a full day\n",
    "        return random_date_offset + timedelta(seconds=random_seconds_offset)\n",
    "\n",
    "\n",
    "# Generate data row by row\n",
    "data = []\n",
    "current_time = datetime.now()\n",
    "start_creation_date = current_time - timedelta(days=365) # Cases created within the last year\n",
    "\n",
    "for i in range(num_records):\n",
    "    case_id = f\"CASE-{100000 + i}\"\n",
    "    category = random.choice(categories)\n",
    "    \n",
    "    placeholder = f\"Module {random.randint(1, 10)}\" if \"Module\" in str(random.choice(case_titles_templates)).format('{}') else \\\n",
    "                  f\"Service {chr(65 + random.randint(0, 5))}\" if \"Service\" in str(random.choice(case_titles_templates)).format('{}') else \\\n",
    "                  f\"Feature {random.randint(1, 20)}\" if \"Feature\" in str(random.choice(case_titles_templates)).format('{}') else \\\n",
    "                  f\"User Account {random.randint(100, 999)}\"\n",
    "                  \n",
    "    case_title = random.choice(case_titles_templates).format(placeholder)\n",
    "    case_description = random.choice(description_templates).format(placeholder)\n",
    "\n",
    "    # Date Created: Ensure it's not in the future\n",
    "    date_created = random_date(start_creation_date, current_time)\n",
    "    \n",
    "    date_closed = None\n",
    "    status = random.choice(statuses)\n",
    "    \n",
    "    if status in [\"Resolved\", \"Closed\"]:\n",
    "        \n",
    "        date_closed_earliest = date_created + timedelta(minutes=1)\n",
    "        date_closed_latest = date_created + timedelta(days=30)\n",
    "\n",
    "    \n",
    "        effective_date_closed_end = min(date_closed_latest, current_time)\n",
    "\n",
    "        if date_closed_earliest < effective_date_closed_end:\n",
    "            date_closed = random_date(date_closed_earliest, effective_date_closed_end)\n",
    "        else:\n",
    "            \n",
    "            date_closed = effective_date_closed_end \n",
    "\n",
    "    if date_closed:\n",
    "        last_update = random_date(date_created, date_closed)\n",
    "    else:\n",
    "        last_update = random_date(date_created, current_time)\n",
    "    \n",
    "    required_escalation = random.choice([True, False, False, False]) \n",
    "\n",
    "    data.append((case_id, category, case_title, case_description, last_update, status, date_created, date_closed, required_escalation))\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data).toDF(*[field.strip().split()[0] for field in schema.strip().split(\",\")])\n",
    "\n",
    "# Show a sample of the generated data\n",
    "print(f\"\\nGenerated {num_records} records.\")\n",
    "df.show(5, truncate=False)\n",
    "\n",
    "\n",
    "\n",
    "df.write.mode(\"overwrite\").saveAsTable(\"Support_Cases\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf4a713-a37f-4c00-9c6c-3fda4c68cb6b",
   "metadata": {
    "collapsed": false,
    "name": "spark_udf"
   },
   "source": [
    "## 3. Spark User-Defined Functions (UDFs)\n",
    "\n",
    "Create a Spark UDF to categorize support cases based on their description, apply it to the data, and write the transformed results back to Snowflake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "928397ca-fed9-42b5-97dd-1e7d6896a930",
   "metadata": {
    "language": "python",
    "name": "sparkudf"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "\n",
    "from pyspark.sql.functions import col, unix_timestamp, when, regexp_replace,lower\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "def udf_parse_case_description(input_desc):\n",
    "    try:\n",
    "        if input_desc is None: # Handle None inputs gracefully\n",
    "            return \"unknown\"\n",
    "            \n",
    "        desc = input_desc.lower()\n",
    "\n",
    "        # Billing/Payment\n",
    "        if any(keyword in desc for keyword in [\"refund\", \"charged twice\", \"duplicate charges\", \"invoice\", \"payment\", \"billing error\", \"subscription\", \"credit card\", \"transaction\"]):\n",
    "            return \"billing_payment\"\n",
    "        \n",
    "        # Shipping/Delivery\n",
    "        elif any(keyword in desc for keyword in [\"not delivered\", \"missing package\", \"stuck in transit\", \"tracking\", \"shipment\", \"delivery\", \"late\", \"lost\"]):\n",
    "            return \"shipping_delivery\"\n",
    "            \n",
    "        # Technical Issue\n",
    "        elif any(keyword in desc for keyword in [\"not working\", \"error\", \"bug\", \"malfunction\", \"crashed\", \"login\", \"password reset\", \"connectivity\", \"performance\", \"glitch\", \"broken\"]):\n",
    "            return \"technical_issue\"\n",
    "\n",
    "        # Account Management\n",
    "        elif any(keyword in desc for keyword in [\"account access\", \"update profile\", \"change email\", \"close account\", \"password\", \"username\", \"profile update\"]):\n",
    "            return \"account_management\"\n",
    "\n",
    "        # Product Inquiry/Feature Request\n",
    "        elif any(keyword in desc for keyword in [\"availability\", \"warranty\", \"discount\", \"feature request\", \"compatibility\", \"specs\", \"how to\", \"information about\"]):\n",
    "            return \"product_inquiry_feature_request\"\n",
    "            \n",
    "        # High Priority/Escalation\n",
    "        elif any(keyword in desc for keyword in [\"urgent\", \"critical\", \"escalate\", \"immediate attention\", \"severe\", \"blocking\"]):\n",
    "            return \"high_priority_escalation\"\n",
    "        \n",
    "        # General Inquiry - Fallback\n",
    "        else:\n",
    "            return \"general_inquiry\"\n",
    "\n",
    "    except Exception as e:\n",
    "        \n",
    "        return \"error\"\n",
    "\n",
    "# Register Spark UDF\n",
    "parse_case_udf = udf(udf_parse_case_description, StringType())\n",
    "#spark.udf.register(\"udf_parse_case_description\", udf_parse_case_description, StringType())\n",
    "\n",
    "table_name = \"SUPPORT_CASES\" \n",
    "df = spark.read.table(\"SUPPORT_CASES\")\n",
    "df1 = df.withColumn(\"INTENT\", parse_case_udf(col(\"CASE_DESCRIPTION\")))\n",
    "\n",
    "#df1 = spark.sql(f\"SELECT *, udf_parse_case_description(CASE_DESCRIPTION) AS INTENT FROM {table_name}\")\n",
    "\n",
    "df1.write.mode(\"overwrite\").saveAsTable(\"Transformed_Cases\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d08087-292d-4591-951b-35f3b6aa842c",
   "metadata": {
    "collapsed": false,
    "name": "query_Transformedcases"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc2d52b8-35d3-4ddd-8fc5-d4e7ab185207",
   "metadata": {
    "language": "python",
    "name": "transformed_Cases"
   },
   "outputs": [],
   "source": [
    "df= spark.sql(\"select * from TRANSFORMED_CASES\");\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7271d22-eda8-43bf-9fb7-10d2f1ccf770",
   "metadata": {
    "collapsed": false,
    "name": "snowpark_udf_parse_review"
   },
   "source": [
    "## 4. Snowflake Python UDFs and SQL Passthrough\n",
    "\n",
    "Create a Python UDF directly in Snowflake and use SQL passthrough to execute Snowflake-native SQL from Spark. This enables leveraging Snowflake's compute for UDF execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01477123-f2e7-4972-9914-ac6b47737d59",
   "metadata": {
    "language": "python",
    "name": "sp_udf_parse_review"
   },
   "outputs": [],
   "source": [
    "session.sql(\"\"\"CREATE OR REPLACE FUNCTION udf_parse_review(input STRING)\n",
    "RETURNS STRING\n",
    "LANGUAGE PYTHON\n",
    "RUNTIME_VERSION = '3.9'\n",
    "HANDLER = 'parse'\n",
    "AS\n",
    "$$\n",
    "def parse(input_desc):\n",
    "    try:\n",
    "        if input_desc is None: # Handle None inputs gracefully\n",
    "            return \"unknown\"\n",
    "            \n",
    "        desc = input_desc.lower()\n",
    "\n",
    "        # Billing/Payment\n",
    "        if any(keyword in desc for keyword in [\"refund\", \"charged twice\", \"duplicate charges\", \"invoice\", \"payment\", \"billing error\", \"subscription\", \"credit card\", \"transaction\"]):\n",
    "            return \"billing_payment\"\n",
    "        \n",
    "        # Shipping/Delivery\n",
    "        elif any(keyword in desc for keyword in [\"not delivered\", \"missing package\", \"stuck in transit\", \"tracking\", \"shipment\", \"delivery\", \"late\", \"lost\"]):\n",
    "            return \"shipping_delivery\"\n",
    "            \n",
    "        # Technical Issue\n",
    "        elif any(keyword in desc for keyword in [\"not working\", \"error\", \"bug\", \"malfunction\", \"crashed\", \"login\", \"password reset\", \"connectivity\", \"performance\", \"glitch\", \"broken\"]):\n",
    "            return \"technical_issue\"\n",
    "\n",
    "        # Account Management\n",
    "        elif any(keyword in desc for keyword in [\"account access\", \"update profile\", \"change email\", \"close account\", \"password\", \"username\", \"profile update\"]):\n",
    "            return \"account_management\"\n",
    "\n",
    "        # Product Inquiry/Feature Request\n",
    "        elif any(keyword in desc for keyword in [\"availability\", \"warranty\", \"discount\", \"feature request\", \"compatibility\", \"specs\", \"how to\", \"information about\"]):\n",
    "            return \"product_inquiry_feature_request\"\n",
    "            \n",
    "        # High Priority/Escalation\n",
    "        elif any(keyword in desc for keyword in [\"urgent\", \"critical\", \"escalate\", \"immediate attention\", \"severe\", \"blocking\"]):\n",
    "            return \"high_priority_escalation\"\n",
    "        \n",
    "        # General Inquiry - Fallback\n",
    "        else:\n",
    "            return \"general_inquiry\"\n",
    "\n",
    "    except Exception as e:\n",
    "        \n",
    "        return \"error\"\n",
    "$$;\n",
    "\"\"\").collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d676f471-5efd-41e8-b4a7-7d0de377d2b9",
   "metadata": {
    "collapsed": false,
    "name": "passthrough"
   },
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e0f686-4251-494f-baa9-95c4f1277d94",
   "metadata": {
    "language": "python",
    "name": "sql_passthrough"
   },
   "outputs": [],
   "source": [
    "spark.conf.set(\"snowpark.connect.sql.passthrough\", True)\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "    CREATE OR REPLACE VIEW REVIEW_INTENT_VIEW AS\n",
    "    SELECT \n",
    "        CASE_DESCRIPTION,\n",
    "        udf_parse_review(CASE_DESCRIPTION) AS INTENT\n",
    "    FROM SUPPORT_CASES\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d10131f-7274-4e98-a4d1-160237e76e8e",
   "metadata": {
    "collapsed": false,
    "name": "read_data_from_view"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c76095-6fe3-4bb7-9713-17b0b9087270",
   "metadata": {
    "language": "python",
    "name": "query_view"
   },
   "outputs": [],
   "source": [
    "spark.read.table('REVIEW_INTENT_VIEW').show(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc999cb-2ef5-4af1-a7c1-a27aa856084c",
   "metadata": {
    "collapsed": false,
    "name": "end"
   },
   "source": [
    "## Summary\n",
    "\n",
    "You've successfully learned how to:\n",
    "- ✅ Connect to Snowpark Connect and run PySpark code against Snowflake\n",
    "- ✅ Generate synthetic data and write it to Snowflake tables  \n",
    "- ✅ Create and apply Spark UDFs for data transformation\n",
    "- ✅ Create Snowflake Python UDFs and invoke them via SQL passthrough\n",
    "\n",
    "### Next Steps\n",
    "- Explore the other notebooks in this repository for more advanced examples\n",
    "- Try [Kaggle's PySpark tutorials](https://www.kaggle.com/code/kkhandekar/apache-spark-beginner-tutorial) for additional Spark patterns\n",
    "- Check out the [Snowpark Connect documentation](https://docs.snowflake.com) for more features\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  },
  "lastEditStatus": {
   "authorEmail": "vino.duraisamy@snowflake.com",
   "authorId": "197899322529",
   "authorName": "VINOD",
   "lastEditTime": 1756402705775,
   "notebookId": "cizhwcq3q6vyjca7iwml",
   "sessionId": "ba770c0a-bc0e-4f85-a472-87a4b36ae6e5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
